boxes: Đây là tensor chứa các thông tin về bounding boxes của các đối tượng được phát hiện trong hình ảnh. Mỗi dòng của tensor này tương ứng với một bounding box và bao gồm các thông tin sau:

Cột 1 và 2: Tọa độ x và y của góc trên bên trái của bounding box.
Cột 3 và 4: Tọa độ x và y của góc dưới bên phải của bounding box.
Cột 5: Độ tự tin (confidence) của việc phát hiện.
Cột 6: ID của lớp (class ID) của đối tượng được phát hiện.
cls: Đây là tensor chứa ID của lớp của các đối tượng được phát hiện.

conf: Đây là tensor chứa giá trị độ tự tin (confidence) của các đối tượng được phát hiện.

data: Tương tự như boxes, đây là tensor chứa thông tin về bounding boxes của các đối tượng, nhưng được tổ chức theo cấu trúc tensor khác.

id: Đây là một thuộc tính (attribute) không có giá trị cụ thể trong kết quả.

is_track: Đây là một thuộc tính (attribute) đánh dấu xem liệu các bounding boxes có phải là theo dõi các đối tượng hay không.

orig_shape: Đây là kích thước gốc của hình ảnh trước khi được xử lý bởi mô hình.

shape: Đây là kích thước của hình ảnh sau khi được xử lý bởi mô hình.

xywh: Đây là tensor chứa thông tin về tọa độ và kích thước của bounding boxes dưới dạng (x_center, y_center, width, height).

xywhn: Đây là tensor chứa thông tin tương tự xywh, nhưng được chuẩn hóa trong khoảng từ 0 đến 1.

xyxy: Đây là tensor chứa thông tin về tọa độ của bounding boxes dưới dạng (x_min, y_min, x_max, y_max).

xyxyn: Đây là tensor chứa thông tin tương tự xyxy, nhưng được chuẩn hóa trong khoảng từ 0 đến 1.



Thông tin về mask trong ultralytics.engine.results.Masks bao gồm:

data và masks: Đây là hai tensor có cùng kích thước, đại diện cho mask của các vật thể trong hình ảnh. Mỗi tensor có kích thước (1, H, W), trong đó H là chiều cao của mask và W là chiều rộng của mask. Mask được biểu diễn dưới dạng ma trận các giá trị số thực trong khoảng từ 0 đến 1, trong đó 0 thường đại diện cho vùng không có vật thể và 1 đại diện cho vùng chứa vật thể.

orig_shape: Kích thước của hình ảnh gốc trước khi được thay đổi kích thước để phù hợp với mô hình.

segments: Một danh sách các điểm trên biên của mask, mỗi điểm được biểu diễn bằng tọa độ (x, y) trên ma trận mask.

shape: Kích thước của tensor mask, có dạng (1, H, W), trong đó H là chiều cao của mask và W là chiều rộng của mask.

xy và xyn: xy là danh sách các điểm trên biên của mask, mỗi điểm được biểu diễn bằng tọa độ (x, y) trong định dạng tensor. xyn là danh sách tọa độ của các điểm trên biên của mask được chuẩn hóa trong khoảng từ 0 đến 1, thường được sử dụng để vẽ mask trên hình ảnh.

Với thông tin này, bạn có thể truy cập và xử lý mask của các vật thể được phát hiện trong hình ảnh.


/////////////////////////////////

OpenCV cung cấp một số phương pháp để xác định góc quay của một vật trong một ảnh. Dưới đây là một số cách thường được sử dụng:

Sử dụng Hough Line Transform (Biến đổi Hough cho đường thẳng): Đây là một trong những phương pháp phổ biến để xác định góc quay của vật thể trên ảnh. Bạn có thể sử dụng hàm cv2.HoughLines() để tìm các đường thẳng trong ảnh sau đó tính toán góc giữa đường thẳng và trục tọa độ.

Sử dụng Feature Matching (Khớp đặc trưng): Bạn có thể sử dụng thuật toán khớp đặc trưng như SIFT (Scale-Invariant Feature Transform) hoặc SURF (Speeded-Up Robust Features) để tìm các điểm đặc trưng trên vật thể và sau đó tính toán góc quay từ các điểm này.

=>> (không dùng được) Sử dụng Contour Detection (Phát hiện đường viền): Nếu vật thể có hình dạng đặc trưng, bạn có thể sử dụng hàm cv2.findContours() để tìm đường viền của vật thể và sau đó tính toán góc quay dựa trên hướng của đường viền.

Sử dụng Homography: Nếu bạn có một vật thể với biến đổi homography (một loại biến đổi affine hoặc perspective), bạn có thể sử dụng hàm cv2.findHomography() để xác định ma trận homography và sau đó tính toán góc quay từ ma trận này.

Dưới đây là một ví dụ sử dụng phương pháp Hough Line Transform để xác định góc quay của vật thể trong ảnh:





////////////////////////////////////


Để thêm tâm của bounding box vào ảnh, bạn có thể sử dụng thư viện OpenCV (hoặc một thư viện xử lý ảnh tương tự). Dưới đây là cách bạn có thể thực hiện điều này:

Trích xuất tọa độ tâm của bounding boxes từ tensor xywh (hoặc xyxy nếu bạn muốn).

Vẽ một điểm hoặc một đường trỏ tới tâm của bounding box trên ảnh gốc.

Dưới đây là một ví dụ về cách thực hiện điều này:

import cv2
import numpy as np

# Load the image
image = cv2.imread('/content/drive/MyDrive/NhanDienvat/NDVat/train/images/20230919_104657_jpg.rf.f419369aed5b63d2614e2c2014ef43e5.jpg')

# Extract the 'xywh' tensor from the results (assuming a single image prediction)
xywh = results.pred[0].xywh

# Iterate through bounding boxes and draw a dot at the center
for bbox in xywh:
    x_center, y_center, width, height = bbox[:4]
    x_center = int(x_center)
    y_center = int(y_center)
    cv2.circle(image, (x_center, y_center), 5, (0, 255, 0), -1)  # Draw a green circle at the center

# Save the image with center points
cv2.imwrite('/content/center_points.jpg', image)

# Display the image (optional)
cv2.imshow('Image with Center Points', image)
cv2.waitKey(0)
cv2.destroyAllWindows()

//////Để tính toán góc xoay của một đường thẳng nằm trong một hình ảnh, bạn có thể sử dụng một số phương pháp xử lý hình ảnh và toán học. Dưới đây là một cách tiêu biểu để thực hiện điều này:

Phát hiện đường thẳng: Đầu tiên, bạn cần phát hiện đường thẳng trong hình ảnh. OpenCV cung cấp một số phương pháp phát hiện đường thẳng, chẳng hạn như cv2.HoughLines hoặc cv2.HoughLinesP dựa trên biến đổi Hough.

Tính toán góc xoay: Sau khi bạn đã có danh sách các đường thẳng được phát hiện, bạn có thể tính toán góc xoay của mỗi đường thẳng. Để làm điều này, bạn có thể sử dụng hàm toán học như atan2 (tính toán góc dựa trên các tọa độ của đầu mút của đường thẳng) hoặc arctan (tính toán góc dựa trên hệ số góc của đường thẳng).

Dưới đây là một ví dụ cụ thể sử dụng OpenCV để tính toán góc xoay của đường thẳng:

python
Copy code
import cv2
import numpy as np

# Đọc hình ảnh
image = cv2.imread('image_with_line.jpg', cv2.IMREAD_GRAYSCALE)

# Phát hiện đường thẳng bằng biến đổi Hough
lines = cv2.HoughLines(image, 1, np.pi / 180, threshold=100)

# Tính toán góc xoay của từng đường thẳng và chuyển đổi thành độ
for rho, theta in lines[:, 0]:
    angle = theta * 180 / np.pi
    print(f'Góc xoay: {angle} độ')

# Hiển thị hình ảnh với đường thẳng đã phát hiện
for rho, theta in lines[:, 0]:
    a = np.cos(theta)
    b = np.sin(theta)
    x0 = a * rho
    y0 = b * rho
    x1 = int(x0 + 1000 * (-b))
    y1 = int(y0 + 1000 * (a))
    x2 = int(x0 - 1000 * (-b))
    y2 = int(y0 - 1000 * (a))
    cv2.line(image, (x1, y1), (x2, y2), (0, 0, 255), 2)

cv2.imshow('Image with Lines', image)
cv2.waitKey(0)
cv2.destroyAllWindows()


///////////////////////////////////////////////xac dinh huong orientation.py

# from __future__ import print_function
# from __future__ import division
import cv2 as cv
import numpy as np
import argparse
from math import atan2, cos, sin, sqrt, pi
def drawAxis(img, p_, q_, colour, scale,degree):
 p = list(p_)
 q = list(q_)
 print("diemP: ",p)
 print("diemQ: ",q)
 text1 = f"({p[0]}, {p[1]})"
 
 cv.putText(img, text1, (p[0]+30,p[1]+30), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)
 angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians
 print("angle: ",degree)
 text2 = f"({degree})"
 cv.putText(img, text2, (p[0]+150,p[1]-30), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)

 hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))
 # Here we lengthen the arrow by a factor of scale
 q[0] = p[0] - scale * hypotenuse * cos(angle)
 q[1] = p[1] - scale * hypotenuse * sin(angle)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 # create the arrow hooks
 p[0] = q[0] + 9 * cos(angle + pi / 4)
 p[1] = q[1] + 9 * sin(angle + pi / 4)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 p[0] = q[0] + 9 * cos(angle - pi / 4)
 p[1] = q[1] + 9 * sin(angle - pi / 4)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 
def getOrientation(pts, img):
 
 sz = len(pts)
 data_pts = np.empty((sz, 2), dtype=np.float64)
 for i in range(data_pts.shape[0]):
  data_pts[i,0] = pts[i,0,0]
  data_pts[i,1] = pts[i,0,1]
 # Perform PCA analysis
 mean = np.empty((0))
 mean, eigenvectors, eigenvalues = cv.PCACompute2(data_pts, mean)
 # Store the center of the object
 cntr = (int(mean[0,0]), int(mean[0,1]))
 
 
 cv.circle(img, cntr, 3, (255, 0, 255), 2)
 p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])
 p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])

 angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians
 degree = angle * (180 / np.pi)

 drawAxis(img, cntr, p1, (0, 255, 0), 1,degree)
 drawAxis(img, cntr, p2, (255, 255, 0), 5,degree)

 
 return degree
parser = argparse.ArgumentParser(description='Code for Introduction to Principal Component Analysis (PCA) tutorial.\
 This program demonstrates how to use OpenCV PCA to extract the orientation of an object.')
parser.add_argument('--input', help='Path to input image.', default='pca_test1.jpg')
args = parser.parse_args()
src = cv.imread(cv.samples.findFile(args.input))


# Xác định các tọa độ của hình ảnh bạn muốn cắt
# x, y, width, height = 900, 900, 1200, 1000  # Ví dụ: cắt từ (100, 100) đến (300, 300)

# Thực hiện phép cắt
# src = image[y:y+height, x:x+width]

# Check if image is loaded successfully
if src is None:
 print('Could not open or find the image: ', args.input)
 exit(0)
# cv.imshow('src', src)
# Convert image to grayscale
gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)
# Convert image to binary
_, bw = cv.threshold(gray, 50, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
contours, _ = cv.findContours(bw, cv.RETR_LIST, cv.CHAIN_APPROX_NONE)
for i, c in enumerate(contours):
 # Calculate the area of each contour
 area = cv.contourArea(c)
 # Ignore contours that are too small or too large
 if area < 1e2 or 1e5 < area:
  continue
 # Draw each contour only for visualisation purposes
 cv.drawContours(src, contours, i, (0, 0, 255), 2)
 # Find the orientation of each shape
 getOrientation(c, src)
 cv.imwrite('orientation_image.jpg',src)
# cv.imshow('output', src)
# cv.waitKey()


///////////////////////////////////////////////xac dinh huong orientation.py

# from __future__ import print_function
# from __future__ import division
import cv2 as cv
import numpy as np
import argparse
from math import atan2, cos, sin, sqrt, pi
def drawAxis(img, p_, q_, colour, scale,degree):
 p = list(p_)
 q = list(q_)
 print("diemP: ",p)
 print("diemQ: ",q)
 text1 = f"({p[0]}, {p[1]})"
 
 cv.putText(img, text1, (p[0]+30,p[1]+30), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)
 angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians
 print("angle: ",degree)
 text2 = f"({degree})"
 cv.putText(img, text2, (p[0]+150,p[1]-30), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)

 hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))
 # Here we lengthen the arrow by a factor of scale
 q[0] = p[0] - scale * hypotenuse * cos(angle)
 q[1] = p[1] - scale * hypotenuse * sin(angle)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 # create the arrow hooks
 p[0] = q[0] + 9 * cos(angle + pi / 4)
 p[1] = q[1] + 9 * sin(angle + pi / 4)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 p[0] = q[0] + 9 * cos(angle - pi / 4)
 p[1] = q[1] + 9 * sin(angle - pi / 4)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 
def getOrientation(pts, img):
 
 sz = len(pts)
 data_pts = np.empty((sz, 2), dtype=np.float64)
 for i in range(data_pts.shape[0]):
  data_pts[i,0] = pts[i,0,0]
  data_pts[i,1] = pts[i,0,1]
 # Perform PCA analysis
 mean = np.empty((0))
 mean, eigenvectors, eigenvalues = cv.PCACompute2(data_pts, mean)
 # Store the center of the object
 cntr = (int(mean[0,0]), int(mean[0,1]))
 
 
 cv.circle(img, cntr, 3, (255, 0, 255), 2)
 p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])
 p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])

 angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians
 degree = angle * (180 / np.pi)

 drawAxis(img, cntr, p1, (0, 255, 0), 1,degree)
 drawAxis(img, cntr, p2, (255, 255, 0), 5,degree)

 
 return degree
parser = argparse.ArgumentParser(description='Code for Introduction to Principal Component Analysis (PCA) tutorial.\
 This program demonstrates how to use OpenCV PCA to extract the orientation of an object.')
parser.add_argument('--input', help='Path to input image.', default='pca_test1.jpg')
args = parser.parse_args()
src = cv.imread(cv.samples.findFile(args.input))


# Xác định các tọa độ của hình ảnh bạn muốn cắt
# x, y, width, height = 900, 900, 1200, 1000  # Ví dụ: cắt từ (100, 100) đến (300, 300)

# Thực hiện phép cắt
# src = image[y:y+height, x:x+width]

# Check if image is loaded successfully
if src is None:
 print('Could not open or find the image: ', args.input)
 exit(0)
# cv.imshow('src', src)
# Convert image to grayscale
gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)
# Convert image to binary
_, bw = cv.threshold(gray, 50, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
contours, _ = cv.findContours(bw, cv.RETR_LIST, cv.CHAIN_APPROX_NONE)
for i, c in enumerate(contours):
 # Calculate the area of each contour
 area = cv.contourArea(c)
 # Ignore contours that are too small or too large
 if area < 1e2 or 1e5 < area:
  continue
 # Draw each contour only for visualisation purposes
 cv.drawContours(src, contours, i, (0, 0, 255), 2)
 # Find the orientation of each shape
 getOrientation(c, src)
 cv.imwrite('orientation_image.jpg',src)
# cv.imshow('output', src)
# cv.waitKey()


//////////////////////////////////////////////



# ///////////////////////////////////////////////////////////

from ultralytics import YOLO
import cv2
import numpy as np

import argparse
from math import atan2, cos, sin, sqrt, pi
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

link = r'/content/nhandienvat/detectByYolov8/dataset_specialItems/train/images/20230922_154950_jpg.rf.a55265100fc54aa788b672e6a11b49f2.jpg'

image = cv2.imread(link)

def drawAxis(img, p_, q_, colour, scale,degree):
 p = list(p_)
 q = list(q_)
 print("diemP: ",p)
 print("diemQ: ",q)
 text1 = f"({p[0]}, {p[1]})"
 
#  cv.putText(img, text1, (p[0]+30,p[1]+30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)
 angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians
 print("angle: ",degree)
 text2 = f"{round(degree,2)} degree"
 cv2.putText(img, text2, (p[0]+10,p[1]-30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)

 hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))
 # Here we lengthen the arrow by a factor of scale
 q[0] = p[0] - scale * hypotenuse * cos(angle)
 q[1] = p[1] - scale * hypotenuse * sin(angle)
 cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv2.LINE_AA)
 # create the arrow hooks
 p[0] = q[0] + 9 * cos(angle + pi / 4)
 p[1] = q[1] + 9 * sin(angle + pi / 4)
 cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv2.LINE_AA)
 p[0] = q[0] + 9 * cos(angle - pi / 4)
 p[1] = q[1] + 9 * sin(angle - pi / 4)
 cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv2.LINE_AA)
 
def getOrientation(pts, img):
 sz = len(pts)
 data_pts = np.empty((sz, 2), dtype=np.float64)
 for i in range(data_pts.shape[0]):
  data_pts[i,0] = pts[i,0,0]
  data_pts[i,1] = pts[i,0,1]
 # Perform PCA analysis
 mean = np.empty((0))
 mean, eigenvectors, eigenvalues = cv2.PCACompute2(data_pts, mean)
 # Store the center of the object
 cntr = (int(mean[0,0]), int(mean[0,1]))
 
 
 cv2.circle(img, cntr, 2, (255, 0, 255), 1)
 p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])
 p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])
#  cv.circle(img, (eigenvectors[0,1], eigenvectors[0,0]), 2, (0, 0, 255), 1)
 angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians
 degree = angle * (180 / np.pi)

 drawAxis(img, cntr, p1, (0, 255, 0), 2,degree)
 drawAxis(img, cntr, p2, (255, 255, 0), 5,degree)
 return degree


# beta = -50  # Giảm độ sáng

# # Sử dụng hàm cv2.convertScaleAbs để giảm độ sáng của ảnh
# src = cv2.convertScaleAbs(image, alpha=1, beta=beta)


# # Check if image is loaded successfully
# if src is None:
# #  print('Could not open or find the image: ', args.input)
#  exit(0)
# # cv2.imshow('src', src)
# # Convert image to grayscale
# gray_picture = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)

# gray_inverted = cv2.bitwise_not(gray_picture)
# # Convert image to binary
# _, bw = cv2.threshold(gray_picture, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
# contours, _ = cv2.findContours(bw, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
# for i, c in enumerate(contours):
#  # Calculate the area of each contour
#  area = cv2.contourArea(c)
#  # Ignore contours that are too small or too large
#  if area < 1e2*3 or 1e5 < area:
#   continue
#  # Draw each contour only for visualisation purposes
#  cv2.drawContours(src, contours, i, (0, 0, 255), 2)
#  # Find the orientation of each shape
#  getOrientation(c, src)
#  cv2.imwrite('orientation_image.jpg',src)
# //////////////
# cv2.imshow('output', src)
# Assuming you have a grayscale image in the 'gray_picture' variable
# Normalize the grayscale values to be between 0 and 1

# plt.imshow(bw, cmap="gray")
# plt.title("Bitwise Gray")
# plt.axis("off")
# plt.show()

# plt.imshow(gray_inverted, cmap="gray")
# plt.title("gray_inverted Gray")
# plt.axis("off")
# plt.show()

# ///////////////////////////////////

# Load a model
model = YOLO('yolov8n-seg.pt')  # load an official model
model = YOLO(r'/content/nhandienvat/runs/segment/train/weights/best.pt')  # load a custom model

# Predict with the model
results = model(r"/content/nhandienvat/detectByYolov8/dataset_specialItems/train/images/20230922_154950_jpg.rf.a55265100fc54aa788b672e6a11b49f2.jpg", save = True)  # predict on an image


myList = []

mylistmask = []

checkitem = []
for r in results:
    # mylistmask = r.masks.xy #ssử dụng nếu dùng phương pháp tìm tọa độ tâm theo các điểm masks
    # print("mask: ",r.masks[0].xy[0])
    # print("shape: ",r.masks.shape)
    # print("masks: ",r.masks)
    print("boxes: ",r.boxes)  # print the Boxes object containing the detection bounding boxes
    myList = r.boxes.xywh.tolist()
    checkitem = r.boxes.cls.tolist()
    # print(int(checkitem[2]))
print("myList: ",myList)
print("length: ",len(myList))


# print(list)
count = 0

while count < len(checkitem):
  print("count: ",count)
  if(checkitem[count] == 3.0):
    print("3")
    bBoxitem = myList[count]
    mask = np.zeros_like(image, dtype=np.uint8)
#     # Định nghĩa màu xanh mà bạn muốn sử dụng (ví dụ: màu xanh lá cây)
#     green_color = ( 255,255, 0)  # Xanh lá cây: (B, G, R)

# # Gán giá trị màu xanh cho mặt nạ
#     mask[:, :] = green_color

    a = (bBoxitem[0] - bBoxitem[2]/2, bBoxitem[1] - bBoxitem[3]/2)
    b = (bBoxitem[0] + bBoxitem[2]/2, bBoxitem[1] - bBoxitem[3]/2)
    c = (bBoxitem[0] + bBoxitem[2]/2, bBoxitem[1] + bBoxitem[3]/2)
    d = (bBoxitem[0] - bBoxitem[2]/2, bBoxitem[1] + bBoxitem[3]/2)

    points = np.array([a, b,c ,d], dtype=np.int32)
    cv2.fillPoly(mask, [points], (255, 255, 255))
    src = cv2.bitwise_and(image, mask)
    beta = -50  # Giảm độ sáng
        # Sử dụng hàm cv2.convertScaleAbs để giảm độ sáng của ảnh
    result = cv2.convertScaleAbs(src, alpha=1, beta=beta)


    # Check if image is loaded successfully
    if result is None:
    #  print('Could not open or find the image: ', args.input)
        exit(0)
    # cv2.imshow('src', src)
    # Convert image to grayscale
    gray_picture = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)

    gray_inverted = cv2.bitwise_not(gray_picture)
    # Convert image to binary
    _, bw = cv2.threshold(gray_picture, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    # cv2.imwrite('bibi.jpg', bw)
    
    contours, _ = cv2.findContours(bw, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    for i, c in enumerate(contours):
    # Calculate the area of each contour
      area = cv2.contourArea(c)
    # Ignore contours that are too small or too large
      if area < 1e2*3 or 1e5 < area:
        continue
    # Draw each contour only for visualisation purposes
        cv2.drawContours(result, contours, i, (0, 0, 255), 2)
    # Find the orientation of each shape
      kq =  getOrientation(c, result)
      print("kq: ", kq)
  if checkitem[count] == 4.0:

    list = myList[count]
    point = (list[0],list[1])
    print("point :",point)
    color = (0, 255, 0)
    # Kích thước của điểm
    thickness = -1  # Đặt -1 để vẽ một điểm đầy đủ
    # Vẽ điểm trên hình ảnh
    cv2.circle(image, (int(list[0]),int(list[1])), 3, (0, 0, 255), thickness) #chấm điểm vào ảnh ở tọa độ tâm của bounding box của lỗ tâm
    # in tọa độ của tâm
    text = f"({int(list[0])}, {int(list[1])})"
    cv2.putText(image, text, (int(list[0])+30,int(list[1])+30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
    cv2.line(image, (int(list[0]),int(list[1])), (int(list[0]) + 50, int(list[1])), (0, 0, 255), 2)  # Vẽ trục X (màu đỏ)
    cv2.line(image, (int(list[0]),int(list[1])), (int(list[0]), int(list[1]) + 50), (0, 255, 0), 2)  # Vẽ trục Y (màu xanh lá)
  count = count + 1
# print("list masks: ",mylistmask.tolist()[0])



# ///////////////////Cách lấy tâm từ các điểm masks/////////////
# count = 0
# while count < len(mylistmask):
#   if checkitem[count] == 4.0:
#     print("count: ", count)
#     print(mylistmask[count].tolist())
#     listitem = mylistmask[count].tolist()
#     count2 = 0
#     totalx= 0
#     totaly = 0
#     x = 0
#     y = 0
    
#     while count2 < len(listitem):
#       print(listitem[count2])
#       coodinate = listitem[count2]
#       totalx = totalx + coodinate[0]
#       totaly = totaly + coodinate[1]
#       # cv2.circle(image, (int(coodinate[0]),int(coodinate[1])), 3, color, thickness)
#       cv2.drawMarker(image, (int(coodinate[0]),int(coodinate[1])), color, markerType=cv2.MARKER_STAR, markerSize=2)
#       count2 = count2 + 1
#     x = totalx / len(listitem)
#     y = totaly / len(listitem)
#     cv2.drawMarker(image, (int(x),int(y)), color, markerType=cv2.MARKER_STAR, markerSize=2)
#   count = count + 1


cv2.imwrite('output_image2.jpg', image)
cv2.imwrite('output_image3.jpg', result)


#Kết luận rằng lấy tâm của bounding box của lỗ tâm chuẩn hơn cách lấy tâm từ các điểm masks của lỗ tâm 



////////////////////////////code//////////////////////////////////
proposal_box_yolo.py:

import numpy as np
from Utils.proposal_angle import apply_min_area

class YOLOSegmentation:
    def __init__(self, model):
        self.model = model

    def predict(self, img, conf_score, img_size):
        pred_img = self.model.predict(source=img, show=False, save=True, conf=conf_score, imgsz=img_size)
        
        # get bounding boxes
        bboxes = np.array(pred_img[0].boxes.xyxy, dtype="int")
        # get masks
        masks = np.array(pred_img[0].masks.xy, dtype=object)
        # get class of bboxes
        class_ids = np.array(pred_img[0].boxes.cls, dtype="int")
        # get score of bboxes
        scores = np.array(pred_img[0].boxes.conf, dtype="float").round(2)
        
        return bboxes, class_ids, masks, scores
    
    @staticmethod
    def filter_boxes(bboxes, class_ids, masks, scores):
        obj_idx = class_ids == 0
        obj = bboxes[obj_idx, :], masks[obj_idx], scores[obj_idx]
        fail_obj = bboxes[~obj_idx, :], masks[~obj_idx], scores[~obj_idx]
        return obj, fail_obj
    
    @staticmethod
    def compute_angle(masks_obj):
        angles_pred = list(map(lambda x: apply_min_area(x), masks_obj))   
        return angles_pred
    
    @staticmethod
    def convert_boxes(boxes): # xyxy to xywh
        boxes[:, 2], boxes[:, 3] = boxes[:, 2] - boxes[:, 0], boxes[:, 3] - boxes[:, 1]
        return boxes
    
def proposal_box_yolo(img, model, conf_score, img_size):
    ys = YOLOSegmentation(model)
    
    bboxes, class_ids, masks, scores = ys.predict(img, conf_score, img_size)
    obj, _ = ys.filter_boxes(bboxes, class_ids, masks, scores)
    angles_pred = ys.compute_angle(obj[1])
    new_bboxes = ys.convert_boxes(obj[0])
    
    return list(zip(new_bboxes, angles_pred))

///////////////////////////////////////////////
proposal_box_improve.py:

import cv2
import numpy as np
import matplotlib.pyplot as plt
from Utils import *
import json

def proposal_roi(image, temp, model, conf, enhance_algorithms=None):
    if len(image.shape) == 3:
        img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    if len(temp.shape) == 3:
        temp = cv2.cvtColor(temp, cv2.COLOR_BGR2GRAY)
    area_temp = temp.shape[0] * temp.shape[1]

    labels_img = instance_segment(image, model=model, conf=conf)
    
    # for key, value in enhance_algorithms.items():
    #     func = eval(key)
    #     labels_img = func(labels_img, value)
    #     # plt.imshow(labels_img)
    #     # plt.show()

    # labels_img = remove_wrong_contours(labels_img, area_temp, selection_area=[0.1, 1.5])

    # _, labels_img = cv2.connectedComponents(labels_img)

    labels_img = labels_img.reshape(labels_img.shape[0], labels_img.shape[1], 1)
    obj_ids = np.unique(labels_img)
    num_objs = len(obj_ids)

    boxes = []
    for i in range(1, num_objs):
        binary_mask = np.all(labels_img==obj_ids[i], axis=2)
        binary_mask = binary_mask.astype(np.uint8)

        points = cv2.findNonZero(binary_mask)
        del binary_mask
        
        box = cv2.boundingRect(points)
        angle = apply_min_area(points)
        del points

        boxes.append([box, angle])

    # for box, angle in boxes:
    #     cv2.rectangle(image, (box[0], box[1]), (box[0]+box[2], box[1]+box[3]), (255, 0, 0), 6)

    # plt.imshow(image)
    # plt.show()

    del labels_img
    return boxes

///////////////////////////////////////////////////
match_template.py:

import cv2
import numpy as np
import matplotlib.pyplot as plt

from Utils import *
import logging

logger = logging.getLogger(__name__)

def match_template(img, template, method, rot, scale, matched_thresh):
    if len(img.shape) == 3:
        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        img_gray = img
        
    if len(template.shape) == 3:
        template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)
    else:
        template_gray = template

    h, w = template_gray.shape

    if rot == 0:
        mask = np.full((h, w, 1), (255), dtype=np.uint8)
        rotated_template = template_gray
        new_w, new_h = w, h
    else:
        rotated_template, mask, new_w, new_h = rotate_template(template_gray, rot)

    method = eval(method)

    if (img_gray.shape[0] < rotated_template.shape[0]) or (img_gray.shape[1] < rotated_template.shape[1]):
        logger.warning(f'img_gray shape: {img_gray.shape}, rotated_template shape: {rotated_template.shape}')
        return
    
    # start = time()
    matched_points = cv2.matchTemplate(img_gray, rotated_template, method, None, mask)
    # end = time()
    # print(f'time: {end-start}')

    _, max_val, _, max_loc = cv2.minMaxLoc(matched_points)

    if max_val >= matched_thresh and max_val <= 1.0:
        return [*max_loc, rot, scale, max_val, new_w, new_h]
//////////////////////////////////////

__init__.py:

import numpy as np
import cv2
import json
from copy import deepcopy

from .match_template import *
from .proposal_box_improve import *
from .proposal_box_yolo import *

/////////////////////////////
Folder Utils:

__init__.py:

from .export_csv import *
from .image_representation import *
from .non_max_suppression import *
from .image_processing_algorithms import *
from .rotate_template import *
from .scale_template import *
from .convert_to_realistic_position import *
from .socket_client import *
from .instance_segmentation import *
from .proposal_angle import *
from .find_center import *
from .calibration import *
from .load_image import *

//////////////////////////////////////
calibration.py:

import numpy as np
import os

class Calibration:
    def __init__(self):
        ...
    
    @staticmethod
    def calibrate_homography(input_points, output_points, predict_points=ModuleNotFoundError):
        # Add homogeneous coordinate 1 to each input point
        input_points_homogeneous = np.hstack((input_points, np.ones((input_points.shape[0], 1))))
        # Solve for the transformation matrix
        transformation_matrix, _ = np.linalg.lstsq(input_points_homogeneous, output_points, rcond=None)[:2]
        # Convert the transformation matrix to a 3x3 matrix
        transformation_matrix = np.vstack((transformation_matrix.T, [0, 0, 1]))
        
        if predict_points is not None:
            new_input_points = np.array(predict_points)
            new_input_points_homogeneous = np.hstack((new_input_points, np.ones((new_input_points.shape[0], 1))))
            output_points = np.dot(transformation_matrix, new_input_points_homogeneous.T).T
            return transformation_matrix, output_points
        
        return transformation_matrix
    
    @staticmethod
    def save_transformation_matrix(transformation_matrix, calib_path):
        if not os.path.exists(calib_path):
            os.makedirs(calib_path)
            
        transformation_matrix_path = os.path.join(calib_path, "transformation_matrix.npy")
        np.save(transformation_matrix_path, transformation_matrix)

    @staticmethod
    def predict(center_obj, transformation_matrix):
        center_points = np.array(center_obj)
        center_points = np.hstack((center_points, np.ones((center_points.shape[0], 1))))
        robot_points = np.dot(transformation_matrix, center_points.T).T
        
        center_point_robot = robot_points[:, :2]
        return center_point_robot
    
    
if __name__ == '__main__':
    transformation_matrix_path = 'Calib/transformation_matrix.npy'
    transformation_matrix = np.load(transformation_matrix_path)
    calib = Calibration()
    
    center_obj = np.load('Calib/input_points.npy')
    
    center_point_robot = calib.predict(center_obj, transformation_matrix)
    print(center_point_robot)

/////////////////
convert_to_realistic_position.py:

import numpy as np
from Utils.calibration import *

def convert_position(points, transformation_matrix=None):
    calib = Calibration()
    
    box = np.vstack(np.array(points[:, 0]))
    center = np.vstack(np.array(points[:, 1]))
    possible_grasp_ratio = np.vstack(np.array(points[:, 2])).flatten()
    
    center_point_robot = calib.predict(center, transformation_matrix)
    center_x, center_y = center_point_robot[:, 0], center_point_robot[:, 1]
    
    angle = -box[:, 2] - 180
    angle = np.where(angle > 180, angle-360, angle)
    angle = np.where(angle < -180, angle+360, angle)
    
    score = box[:, 4] * 100
    
    center_z = np.full((center_x.shape[0]), 97)
    
    realistic_points = np.array(list(zip(center_x, center_y, center_z, angle, possible_grasp_ratio, score)), dtype=np.float32)
    
    return realistic_points

//////////////////////////

export_csv.py:
import pandas as pd
import numpy as np
import os

def export_csv(points, output_folder):
    centerx_series = pd.Series(points[:, 0], name='x')
    centery_series = pd.Series(points[:, 1], name='y')
    angle_series = pd.Series(points[:, 3], name='angle')
    possible_grasp_ratio_series = pd.Series(points[:, 4], name='possible')
    score_series = pd.Series(points[:, 5], name='score')
    index_series = pd.Series(np.arange(len(points[:, 3])), name='index')

    result = pd.concat([index_series, centerx_series, centery_series, angle_series, possible_grasp_ratio_series, score_series], axis=1)

    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    result.to_csv(os.path.join(output_folder, 'result.csv'), index=False)

//////////////////////////////

find_center.py:

import cv2
import numpy as np
import matplotlib.pyplot as plt
from Utils.image_processing_algorithms import *
import logging

logger = logging.getLogger(__name__)

def compute_distance(point1, point2):
    x1, y1 = point1
    x2, y2 = point2
    return np.sqrt((x2-x1)**2 + (y2-y1)**2)

def find_center(img_gray, bbox, intensity_of_template_gray):
    (x1, y1, w, h) = bbox
    x2, y2 = x1 + w, y1 + h
    center_b_x, center_b_y = (x2-x1)/2, (y2-y1)/2
    
    roi_gray = img_gray[y1:y2, x1:x2]
    padded_roi_gray = img_gray[y1-100:y2+100, x1-100:x2+100]
    
    roi_gray, padded_roi_gray = list(map(lambda x: contrast_stretching(x, {"low_clip": 10, "high_clip": 90}), [roi_gray, padded_roi_gray]))
    _, roi_gray = cv2.threshold(roi_gray, 100, 255, cv2.THRESH_BINARY_INV)
    _, padded_roi_gray = cv2.threshold(padded_roi_gray, 100, 255, cv2.THRESH_BINARY_INV)
    
    intensity_of_roi_gray = np.sum(padded_roi_gray == 0)
    possible_grasp_ratio = (intensity_of_template_gray / intensity_of_roi_gray) * 100
    
    # find canny edges
    edges = cv2.Canny(roi_gray, 100, 200)
    edges = cv2.dilate(edges, np.ones((2, 2), np.uint8))
    
    # find contours from edges
    contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
    
    distances_circle = []
    for contour in contours:
        if cv2.minAreaRect(contour)[1][0] * cv2.minAreaRect(contour)[1][1] > roi_gray.shape[0] * roi_gray.shape[1] / 8:
            continue

        try:
            if (cv2.minAreaRect(contour)[1][0] / cv2.minAreaRect(contour)[1][1] > 1.1) or (cv2.minAreaRect(contour)[1][0] / cv2.minAreaRect(contour)[1][1] < 0.9):
                continue
        except Exception as e:
            # logger.exception(f'Filter contour: {e}\n')
            continue

        if len(contour) < 100: #fine tune
            continue
        
        # create circle from contour
        (center_x_c, center_y_c), radious = cv2.minEnclosingCircle(contour)
        
        distance_c = compute_distance((center_b_x, center_b_y), (center_x_c, center_y_c))
        if distance_c > 25:
            continue
        
        distances_circle.append({"center":(center_x_c, center_y_c), "radious":radious, "contour": contour})
    
    distances_circle = sorted(distances_circle, key=lambda x:x["radious"])
    
    try:
        centroid = np.mean(distances_circle[0]["contour"], axis=0)
        centroid_x = centroid[0][0]
        centroid_y = centroid[0][1]
        
        center_c_x, center_c_y = distances_circle[0]["center"]  # center of circle    
        true_center_x, true_center_y = (0.5*centroid_x + 0.5*center_c_x), (0.5*centroid_y + 0.5*center_c_y)
        
    except Exception as e:
        logger.error(f'No contour found\n')
        true_center_x, true_center_y = w/2, h/2

    center_obj = (true_center_x+x1, true_center_y+y1)

    return center_obj, possible_grasp_ratio


////////////////////////////

image_processing_algorithms.py:

import cv2
import numpy as np
from rembg import remove
import inspect
import matplotlib.pyplot as plt

def with_params(func):
    def wrapper(img, params):
        return func(img, **{k: params[k] for k in inspect.signature(func).parameters.keys() if k in params})
    return wrapper

@with_params
def rembg_add_weights(img, alpha=0.75, beta=2.5):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    enhanced_img = remove(img)
    enhanced_img = cv2.cvtColor(enhanced_img, cv2.COLOR_BGRA2GRAY)
    enhanced_img = cv2.addWeighted(img, alpha, enhanced_img, beta, 0)
    return enhanced_img

@with_params
def adaptive_threshold(img, iteration=1):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    hist = cv2.calcHist([img], [0], None, [256], [0, 256])
    score, _ = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_TRIANGLE)
    score = int(score)
    # peak1 = np.argmax(hist.T[0][:score])
    peak2 = np.argmax(hist.T[0][score:]) + score
    
    if peak2 == 255:
        _, mask = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_TRIANGLE)

        kernel = np.ones((3, 3), np.uint8)
        mask = cv2.erode(mask, kernel, iterations=1)
        mask = cv2.dilate(mask, kernel, iterations=1)
    
    else:
        _, mask = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_TRIANGLE)

        kernel = np.ones((3, 3), np.uint8)
        mask = cv2.erode(mask, kernel, iterations=1)
        mask = cv2.dilate(mask, kernel, iterations=1)

    return mask

def remove_wrong_contours(img, area_temp, selection_area=[0.25, 1.5]):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    _, labels, stats_img, _ = cv2.connectedComponentsWithStats(img)
    areas_img = stats_img[:, 4]

    binary = np.zeros_like(img)

    mask = (areas_img[1:] >= area_temp*selection_area[0]) & (areas_img[1:] <= area_temp*selection_area[1])
    indices = np.where(mask)[0] + 1
    binary[np.isin(labels, indices)] = 255
    return binary

@with_params
def gamma_correction(img, gamma=0.8):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    inv_gamma = 1.0 / gamma
    table = np.array([((i / 255.0) ** inv_gamma) * 255
        for i in np.arange(0, 256)]).astype("uint8")
    return cv2.LUT(img, table)

@with_params
def pixel_duplicate(img, ratio=0.75):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    img_float = np.array(img, np.float16)
    enhanced_img = img + ratio*img_float
    enhanced_img = np.clip(enhanced_img, 0, 255)
    enhanced_img = np.array(enhanced_img, np.uint8)
    return enhanced_img

@with_params
def remove_shadow(img, blur=21, thresh=220, dilate=8):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    dilated_img = cv2.dilate(img, np.ones((dilate, dilate), np.uint8)) 
    bg_img = cv2.GaussianBlur(dilated_img, (blur, blur), -1)
    diff_img = 255 - cv2.absdiff(img, bg_img)
    norm_img = diff_img.copy()
    cv2.normalize(diff_img, norm_img, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)
    _, thr_img = cv2.threshold(norm_img, thresh, 255, cv2.THRESH_BINARY_INV)
    cv2.normalize(thr_img, thr_img, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)
    return thr_img

@with_params
def sharpen(img):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    img = img.astype(np.float32)
    kernel_sharpen = np.array([[-1, -1, -1],
                               [-1, 9, -1],
                               [-1, -1, -1]])
    
    img = cv2.filter2D(img, -1, kernel_sharpen)
    img = np.clip(img, 0, 255)
    return img.astype(np.uint8)

@with_params
def filter_clahe(img, cliplimit=3, titleGridSize=8):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    clahe = cv2.createCLAHE(clipLimit=cliplimit, tileGridSize=(titleGridSize, titleGridSize))
    img = clahe.apply(img)
    return img

@with_params
def laplacian_detect(img, ksize=3):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    dst = cv2.Laplacian(img, cv2.CV_16S, ksize=ksize)
    abs_dst = cv2.convertScaleAbs(dst)
    return abs_dst

@with_params
def gradient(img, ksize=3):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    grad_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=ksize)
    grad_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=ksize)
    grad_mag = np.sqrt(grad_x ** 2 + grad_y ** 2)
    grad_mag_norm = cv2.normalize(grad_mag, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)
    return grad_mag_norm

@with_params
def canny_detect(img, thresh1=100, thresh2=200):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    return cv2.Canny(img, thresh1, thresh2)

@with_params
def contrast_stretching(img, low_clip=5.0, high_clip=97.0):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    low_val, high_val = np.percentile(img, (low_clip, high_clip))
    out_img = np.uint8(np.clip((img - low_val) * 255.0 / (high_val - low_val), 0, 255))
    return out_img

def multiScaleRetinex(img, sigma_list=[15, 80, 256]):
    log_img = np.log10(img)
    kernel_sizes = [int(3 * sigma) | 1 for sigma in sigma_list]
    blurred_imgs = [cv2.GaussianBlur(log_img, (kernel_size, kernel_size), sigma) for sigma, kernel_size in zip(sigma_list, kernel_sizes)]
    retinex = np.sum([log_img - blurred_img for blurred_img in blurred_imgs], axis=0)
    retinex = retinex / len(sigma_list)
    return retinex

def colorRestoration(img, alpha=125, beta=50):
    img_sum = np.sum(img, axis=-1, keepdims=True)
    img_sum[img_sum == 0] = 1
    color_restoration = beta * (np.log10(alpha * img) - np.log10(img_sum))
    return color_restoration

def simplestColorBalance(img, low_clip=5.0, high_clip=97.0):
    low_val, high_val = np.percentile(img, (low_clip, high_clip))
    out_img = np.uint8(np.clip((img - low_val) * 255.0 / (high_val - low_val), 0, 255))
    return out_img

@with_params
def MSRCP(img, sigma_list=[15, 80, 256], G=5, b=25, alpha=125, beta=50, low_clip=5.0, high_clip=97.0, pyramid=2):
    assert len(img.shape) == 3, "The image has to be a color image"
    for _ in range(pyramid):
        img = cv2.pyrDown(img)

    img = np.float32(img) + 1.0
    img_retinex = multiScaleRetinex(img, sigma_list)
    img_color_restoration = colorRestoration(img, alpha, beta)
    img_msrcp = G * (img_retinex * img_color_restoration + b)
    img_msrcp = (img_msrcp - np.amin(img_msrcp)) / (np.amax(img_msrcp) - np.amin(img_msrcp)) * 255
    img_msrcp = np.uint8(img_msrcp)
    img_msrcp = simplestColorBalance(img_msrcp, low_clip, high_clip)
    img_msrcp = cv2.cvtColor(img_msrcp, cv2.COLOR_BGR2GRAY) if len(img_msrcp.shape) == 3 else img_msrcp

    for _ in range(pyramid):
        img_msrcp = cv2.pyrUp(img_msrcp)

    return img_msrcp

@with_params
def apply_representation(img, color='lab', normalize=False, channel=0):
    assert len(img.shape) == 3, "The image has to be a color image"
    assert channel <= 2, "The channel has to be smaller and equal 2"

    if color == 'lab':
        new_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        new_img = new_img[:, :, channel]
    elif color == 'hsv':
        new_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        new_img = new_img[:, :, channel]
    elif color == 'gray':
        new_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    
    if bool(normalize):
        cv2.normalize(new_img, new_img, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)
        
    return new_img

////////////////////////////////
image_representation.py


import numpy as np
import matplotlib.pyplot as plt
from Utils.image_processing_algorithms import *

def image_representation(img, target, representation_algorithms):
    if target == 'template':
        algorithms = representation_algorithms['template']
    elif target == 'target_image':
        algorithms = representation_algorithms['target_image']

    for key, value in algorithms.items():
        func = eval(key)
        new_img = func(img, value)

    return new_img

////////////////////

instance_segmentation.py:


from ultralytics import YOLO
import cv2
import numpy as np
import matplotlib.pyplot as plt

def instance_segment(image, model, conf=0.7):
    result = model.predict(source=image, show=False, save=False, conf=conf)
    contours = result[0].masks.xy

    mask = np.zeros(shape=(image.shape[:2]))
    for idx, cnt in enumerate(contours):
        cnt = cnt.astype(np.int32)
        cv2.drawContours(mask, [cnt], 0, idx+1, cv2.FILLED)

    return mask

if __name__ == '__main__':
    model = YOLO('Weights/last.pt')
    img = cv2.imread('Dataset/Src5-0.bmp')
    mask = instance_segment(img, model, conf=0.25)
    plt.imshow(mask, cmap='gray')
    plt.show()

//////////////

load_image.py


from turbojpeg import TurboJPEG
from time import time

class TurboJpegLoader:
    def __init__(self):
        super(TurboJpegLoader, self).__init__()
        
        # create TurboJPEG object for image reading
        self.jpeg_reader = TurboJPEG('DLL/libturbojpeg.dll') 
 
    def load(self, path):
        start = time()
        # open the input file as bytes
        file = open(path, "rb")  
        full_time = time() - start
 
        start = time()
        # decode raw image
        image = self.jpeg_reader.decode(file.read(), 1)  
        full_time += time() - start
        file.close()
        
        return image, full_time
    
if __name__ == "__main__":
    path = 'Dataset/input_image.jpg'
    loader = TurboJpegLoader()
    
    s = time()
    img, t = loader.load(path)
    e = time()
    print(f'time: {e-s}')

//////////////////////

non_max_suppression.py:

import numpy as np

def non_max_suppression_fast(points, overlapThresh):
    box = np.array(points[:, 0])
    box = np.vstack(box)
    if box.dtype.kind == "i":
        box = box.astype("float")
        
    pick = []

    x1 = box[:, 0]
    y1 = box[:, 1]
    x2 = box[:, 0] + box[:, 5]
    y2 = box[:, 1] + box[:, 6]

    score = box[:, 4]

    area = (x2 - x1 + 1) * (y2 - y1 + 1)
    idxs = np.argsort(score)

    while len(idxs) > 0:
        last = len(idxs) - 1
        i = idxs[last]
        pick.append(i)
        xx1 = np.maximum(x1[i], x1[idxs[:last]])
        yy1 = np.maximum(y1[i], y1[idxs[:last]])
        xx2 = np.minimum(x2[i], x2[idxs[:last]])
        yy2 = np.minimum(y2[i], y2[idxs[:last]])
        
        w = np.maximum(0, xx2 - xx1 + 1)
        h = np.maximum(0, yy2 - yy1 + 1)
        
        overlap = (w * h) / area[idxs[:last]]
        idxs = np.delete(idxs, np.concatenate(([last],
			np.where(overlap > overlapThresh)[0])))
        
    return points[pick]


/////////////////////////////////////////////

proposal_angle.py

import cv2
import numpy as np
import matplotlib.pyplot as plt

def apply_pca(contour):
    # Calculate the centroid
    centroid = np.mean(contour, axis=0)
    centroid_x = centroid[0]
    centroid_y = centroid[1]

    # Calculate the covariance matrix
    covariance_matrix = np.cov((contour[:, 0] - centroid_x).T, (contour[:, 1] - centroid_y).T)

    # Perform PCA
    _, eigenvalues, eigenvectors = cv2.eigen(covariance_matrix, True)

    # Determine the orientation angle
    major_axis = eigenvectors[0]
    orientation_angle = np.degrees(np.arctan2(major_axis[1], major_axis[0]))

    return orientation_angle, orientation_angle+180

def apply_min_area(contour):
    rotated_rect = cv2.minAreaRect(contour)
    rect_points = cv2.boxPoints(rotated_rect).astype(int)
    
    edge1 = np.array(rect_points[1]) - np.array(rect_points[0])
    edge2 = np.array(rect_points[2]) - np.array(rect_points[1])
    
    reference = np.array([1, 0])  # Horizontal edge

    if np.linalg.norm(edge1) > np.linalg.norm(edge2):
        used_edge = edge1
        angle = (180.0 / np.pi) * (np.arccos(np.dot(reference, used_edge) / (np.linalg.norm(reference) * np.linalg.norm(used_edge))))

    # if np.linalg.norm(edge2) > np.linalg.norm(edge1)    
    else:
        used_edge = edge2
        angle = (180.0 / np.pi)*(np.arccos(np.dot(reference, used_edge) / (np.linalg.norm(reference) * np.linalg.norm(used_edge))))
        angle = (180 - angle)
        
    return -angle


/////////////////////////////////

rotate_template.py

import cv2
import numpy as np
import matplotlib.pyplot as plt

def rotate_template(image, angle):
    h, w = image.shape[:2]
    cx, cy = (w // 2, h // 2)

    # get rotation matrix (explained in section below)
    M = cv2.getRotationMatrix2D((cx, cy), -angle, 1.0)

    # get cos and sin value from the rotation matrix
    cos, sin = abs(M[0, 0]), abs(M[0, 1])

    # calculate new width and height after rotation (explained in section below)
    newW = int((h * sin) + (w * cos))
    newH = int((h * cos) + (w * sin))

    # calculate new rotation center
    M[0, 2] += (newW / 2) - cx
    M[1, 2] += (newH / 2) - cy

    # use modified rotation center and rotation matrix in the warpAffine method
    result = cv2.warpAffine(image,
                            M, (newW, newH),
                            borderValue=(0, 0, 0),
                            flags=cv2.INTER_LINEAR)

    pixel_array = np.full((h, w, 1), (255), dtype=np.uint8)
    mask = cv2.warpAffine(pixel_array, M, (newW, newH))

    return result, mask, newW, newH

////////////////////////////////////

scale_template.py:


import cv2

def scale_template(template, percent_scale, img_max_wh):
    max_height, max_width = img_max_wh
    max_percent_height = max_height / template.shape[0] * 100
    max_percent_width = max_width / template.shape[1] * 100

    max_percent = 0
    if max_percent_width < max_percent_height:
        max_percent = max_percent_width
    else:
        max_percent = max_percent_height
    
    if percent_scale > max_percent:
        percent_scale = max_percent

    new_width = int(template.shape[1] * percent_scale / 100)
    new_height = int(template.shape[0] * percent_scale / 100)

    result = cv2.resize(template, (new_width, new_height), interpolation = cv2.INTER_AREA)

    return result, percent_scale


//////////////////////////////////////////////////////\\


socket_client.py:


import socket
import numpy as np
import struct
import logging
import time

logger = logging.getLogger(__name__)

def send_data(data_array, ip_address, port):
    # Create a TCP socket
    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    
    try:
        # Connect to the server
        server_address = (ip_address, port)
        client_socket.connect(server_address)
        
        num_data = 5
        byte_value = num_data.to_bytes(1, byteorder='big')
        client_socket.send(byte_value)
        
        # Receive the data array
        data_bytes = client_socket.recv(16)
        data_array = struct.unpack('!4f', data_bytes)
        
        print(data_array)

    except Exception as e:
        logger.error(f'{e}\n')
        
    finally:
        client_socket.close()
        
def send_float_array_data(data_array, ip_address, port):
    # Create a TCP socket
    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    try:
        # Connect to the server
        server_address = (ip_address, port)
        client_socket.connect(server_address)
        
        num_data = len(data_array)
        byte_value = num_data.to_bytes(1, byteorder='big')
        client_socket.send(byte_value)

        # Send each float element separately
        for data in data_array:
            array_bytes = struct.pack('!4f', *data)
            client_socket.sendall(array_bytes)
            
        response = client_socket.recv(1)
        
        if response[0] == 100:
            logger.info(f"sent successfully")
            client_socket.close()

    except Exception as e:
        logger.error(f'{e}\n')
        
    # finally:
    #     client_socket.close()
    
if __name__ == "__main__":
    points = np.random.randint(0, 255, (15, 4)).astype(np.float32)
    print(points)
    send_float_array_data(points, '192.168.176.1', 48952)
    send_data(np.array(points, dtype=np.float32), '192.168.176.1', 48953)



////////////////////////////
my_cvu_api.py



from Utils import *
from API import *
from Component import *

from time import time

logger = logging.getLogger(__name__)

def get_padded_image(img_gray, box, epsilon_w, epsilon_h):
        x_start, x_end = box[0] - epsilon_w, box[0] + box[2] + epsilon_w
        y_start, y_end = box[1] - epsilon_h, box[1] + box[3] + epsilon_h

        top = min(y_start, 0)
        left = min(x_start, 0)
        bottom = min(img_gray.shape[0] - y_end, 0)
        right = min(img_gray.shape[1] - x_end, 0)
        img_padded = cv2.copyMakeBorder(img_gray, abs(top), abs(bottom), abs(left), abs(right), cv2.BORDER_CONSTANT, value=0)

        return img_padded, x_start, x_end, y_start, y_end, top, left, bottom, right

def process_roi(img_padded, template_gray, method, sub_angle, threshold,
                top, left, bottom, right,
                x_start, x_end, y_start, y_end):
    
    roi = img_padded[y_start + abs(top):y_end + abs(top) + abs(bottom),
                    x_start + abs(left):x_end + abs(left) + abs(right)]

    try:
        point = match_template(roi, template_gray, method, sub_angle, 100, threshold)
    except Exception as e:
        logger.error(f'{e}\n')
        return None

    return point

def match_pattern(img_gray, template_gray, box, sub_angle, method, threshold):
    _, _, w_temp, h_temp = rotate_template(template_gray, sub_angle)
    epsilon_w, epsilon_h = np.abs([box[2] - w_temp, box[3] - h_temp])

    img_padded, x_start, x_end, y_start, y_end, top, left, bottom, right = get_padded_image(img_gray, box, epsilon_w, epsilon_h)

    point = process_roi(img_padded, template_gray, method, sub_angle, threshold,
                        top, left, bottom, right, 
                        x_start, x_end, y_start, y_end)
    
    return point


@app.route('/my_cvu_api', methods=['POST', 'GET'])
def pattern_matching():
    start = time()
    if request.method == 'POST':
        api_folder = request.form.get('api_folder')
        api_folder = api_folder.replace('\\', '/')

        if platform == "linux" or platform == "linux2":
            if api_folder.startswith('//wsl.localhost/'):
                idx = api_folder.index('/home')
                api_folder = api_folder[idx:]

            if api_folder[1] == ':':
                api_folder = os.popen('wslpath "{}"'.format(api_folder)).read().strip()

        elif platform == "win32":
            api_folder = api_folder.replace('/', '\\')

        if api_folder is not None:
            os.chdir(api_folder)
        
        if not os.path.exists('Log'):
            os.makedirs('Log')
        
        logging.basicConfig(level=logging.INFO, 
                    format='%(name)s - %(levelname)s - %(asctime)s - %(message)s', 
                    datefmt='%d-%b-%y %H:%M:%S', 
                    filename='Log/log.txt',
                    filemode='w')
        
        logger.info(f'OS: {platform}\n')
        logger.info(f'Root folder: {api_folder}\n')

        output_folder = request.form.get('output_folder')
        path_to_save_image = os.path.join(output_folder, 'output.jpg')
        path_to_save_csv = os.path.join(output_folder, 'result.csv')

        if not os.path.exists(output_folder):
            os.makedirs(output_folder)
        
        else:
            if os.path.isfile(path_to_save_image) == True:
                os.remove(path_to_save_image)
            if os.path.isfile(path_to_save_csv) == True:
                os.remove(path_to_save_csv)
                
        logger.info(f'Output folder: {output_folder}\n')

        try:
            img_path = request.form.get('img_path')
            img_path = img_path.replace('\\', '/')
            bgr_img, _ = loader.load(img_path)

            template_path = request.form.get('template_path')
            template_path = template_path.replace('\\', '/')
            bgr_template, _ = loader.load(template_path)

            if (bgr_img is None) or (bgr_template is None):
                logger.warning("No image founded\n")
                return "No image founded\n"

        except Exception as e:
            logger.error(f'{e}\n')
            return f'{e}\n'
        
        logger.info('Load images successfully\n')
        
        try:
            threshold = float(request.form.get('threshold'))
            overlap = float(request.form.get('overlap'))
            min_modify = int(request.form.get('min_modify'))
            max_modify = int(request.form.get('max_modify'))
            conf_score = float(request.form.get('conf_score'))
            img_size = int(request.form.get('img_size'))
            server_ip = request.form.get('server_ip')

        except Exception as e:
            logger.error(f'{e}\n')
            return f'{e}\n'

        method = request.form.get('method')
        
        logger.info(f'''
                    threshold: {threshold}
                    overlap: {overlap}
                    min_modify: {min_modify}
                    max_modify: {max_modify}
                    conf_score: {conf_score}
                    method: {method}
                    img_size: {img_size}
                    server_ip: {server_ip}\n
                    ''')
        
        minus_modify_angle = np.arange(-1, min_modify, -1)
        plus_modify_angle = np.arange(1, max_modify, 1)

        template_gray = cv2.cvtColor(bgr_template, cv2.COLOR_BGR2GRAY)
        
        copy_of_template_gray = deepcopy(template_gray)
        copy_of_template_gray = contrast_stretching(copy_of_template_gray, {"low_clip": 10, "high_clip": 90})
        _, copy_of_template_gray = cv2.threshold(copy_of_template_gray, 100, 255, cv2.THRESH_BINARY_INV)
        cv2.imwrite('intensity_template.jpg', copy_of_template_gray)
        
        intensity_of_template_gray = np.sum(copy_of_template_gray == 0)

        try:
            s = time()
            boxes = proposal_box_yolo(bgr_img, model, conf_score=conf_score, img_size=img_size)
            e = time()
            print(f'time: {e-s}')
        except Exception as e:
            logger.error(f'{e}\n')
            return f'{e}\n'
            
        logger.info(f'''
                    Number of proposal boxes: {len(boxes)}\n
                    {np.array(boxes, dtype=object)}\n
                    ''')

        img_gray = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)
        
        copy_of_img_gray = deepcopy(img_gray)

        s = time()
        good_points = []
        for box, angle in boxes:
            center_obj, possible_grasp_ratio = find_center(copy_of_img_gray, box, intensity_of_template_gray)
            if possible_grasp_ratio < 50:
                continue
            
            minus_sub_angles = angle + minus_modify_angle
            plus_sub_angles = angle + plus_modify_angle
            minus_length = len(minus_sub_angles)
            plus_length = len(plus_sub_angles)
            
            minus_pointer, minus_check = 0, False
            plus_pointer, plus_check = 0, False
            sub_minus_points = []
            sub_plus_points = []
            
            point = match_pattern(img_gray, template_gray, box, angle, method, threshold)
            if point is None:
                continue
            
            while True:
                if (minus_length == 0 and plus_length == 0):
                    break

                if minus_length == 0 or minus_pointer >= minus_length:
                    minus_check = True
                elif plus_length == 0 or plus_pointer >= plus_length:
                    plus_check = True

                if not minus_check and minus_length != 0:
                    minus_point = match_pattern(img_gray, template_gray, box, minus_sub_angles[minus_pointer], method, threshold)
                    if minus_point is not None:
                        minus_check = minus_point[4] < point[4] if minus_pointer == 0 else minus_point[4] < sub_minus_points[-1][4]
                    else:
                        minus_check = True
                    
                    if not minus_check:
                        sub_minus_points.append(minus_point)
                        minus_pointer += 1
                
                if not plus_check and plus_length != 0:
                    plus_point = match_pattern(img_gray, template_gray, box, plus_sub_angles[plus_pointer], method, threshold)
                    if plus_point is not None:
                        plus_check = plus_point[4] < point[4] if plus_pointer == 0 else plus_point[4] < sub_plus_points[-1][4]
                    else:
                        plus_check = True
                    
                    if not plus_check:
                        sub_plus_points.append(plus_point)
                        plus_pointer += 1
                
                if minus_check and plus_check:
                    break
            
            best_minus_point = sub_minus_points[-1] if sub_minus_points else None
            best_plus_point = sub_plus_points[-1] if sub_plus_points else None
            
            if (best_minus_point is not None) and (best_plus_point is not None):
                best_point = best_minus_point if best_minus_point[4] > best_plus_point[4] else best_plus_point
            elif (best_minus_point is None) and (best_plus_point is None):
                best_point = point
            else:
                best_point = best_minus_point or best_plus_point
            
            if point:
                good_points.append((best_point, center_obj, possible_grasp_ratio))
        
        good_points.sort(key=lambda x: x[2], reverse=True)
        good_points = np.array(good_points, dtype=object)
        e = time()
        print(f'time: {e-s}')
        
        if len(good_points) == 0:
            logger.warning('No detection found\n')
            return 'No detection found\n'
        
        copy_of_good_points = deepcopy(good_points)

        realistic_points = convert_position(copy_of_good_points, transformation_matrix)
        
        logger.info(f'Result: \n{realistic_points}\n')
        
        s = time()
        send_float_array_data(realistic_points[:, :4], server_ip, 48952)
        e = time()
        print(f'time: {e-s}')
        
        export_csv(realistic_points, output_folder)
        
        for idx, (point_info, center, possible_grasp_ratio) in enumerate(good_points):
            angle = point_info[2]
            
            center_x, center_y = center
            center_x, center_y = int(center_x), int(center_y)
            
            axis_length = 100
            
            angle_rad = np.radians(angle)
            
            # Calculate the endpoint coordinates for the x-axis line
            x1 = center_x
            y1 = center_y
            x2 = int(center_x + axis_length * np.cos(angle_rad))
            y2 = int(center_y + axis_length * np.sin(angle_rad))

            # Calculate the endpoint coordinates for the y-axis line
            x3 = center_x
            y3 = center_y
            x4 = int(center_x + axis_length * np.sin(angle_rad))
            y4 = int(center_y - axis_length * np.cos(angle_rad))
            
            color_x = (0, 255, 0)
            color_y = (0, 0, 255)
            thickness = 3
            
            # Draw the x-axis line
            cv2.line(bgr_img, (x1, y1), (x2, y2), color_x, thickness)

            # Draw the y-axis line
            cv2.line(bgr_img, (x3, y3), (x4, y4), color_y, thickness)
            
            cv2.putText(bgr_img, str(idx), (center_x+50, center_y+50), cv2.FONT_HERSHEY_SIMPLEX, 3, color_x, thickness)

        cv2.line(bgr_img, (0, bgr_img.shape[0]), (axis_length, bgr_img.shape[0]), color_x, thickness)
        cv2.line(bgr_img, (0, bgr_img.shape[0]), (0, bgr_img.shape[0]-axis_length), color_y, thickness)
        
        bgr_img = cv2.resize(bgr_img, (bgr_img.shape[1]//4, bgr_img.shape[0]//4))
        
        cv2.imwrite(path_to_save_image, bgr_img, [cv2.IMWRITE_JPEG_QUALITY, 70])
        
        end = time()
        print(f'Elapsed time: {end-start}\n')
        logger.info(f'Elapsed time: {end-start}\n')

        return f'{len(realistic_points)}'

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
