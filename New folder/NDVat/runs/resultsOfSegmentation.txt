boxes: ƒê√¢y l√† tensor ch·ª©a c√°c th√¥ng tin v·ªÅ bounding boxes c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c ph√°t hi·ªán trong h√¨nh ·∫£nh. M·ªói d√≤ng c·ªßa tensor n√†y t∆∞∆°ng ·ª©ng v·ªõi m·ªôt bounding box v√† bao g·ªìm c√°c th√¥ng tin sau:

C·ªôt 1 v√† 2: T·ªça ƒë·ªô x v√† y c·ªßa g√≥c tr√™n b√™n tr√°i c·ªßa bounding box.
C·ªôt 3 v√† 4: T·ªça ƒë·ªô x v√† y c·ªßa g√≥c d∆∞·ªõi b√™n ph·∫£i c·ªßa bounding box.
C·ªôt 5: ƒê·ªô t·ª± tin (confidence) c·ªßa vi·ªác ph√°t hi·ªán.
C·ªôt 6: ID c·ªßa l·ªõp (class ID) c·ªßa ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c ph√°t hi·ªán.
cls: ƒê√¢y l√† tensor ch·ª©a ID c·ªßa l·ªõp c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c ph√°t hi·ªán.

conf: ƒê√¢y l√† tensor ch·ª©a gi√° tr·ªã ƒë·ªô t·ª± tin (confidence) c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c ph√°t hi·ªán.

data: T∆∞∆°ng t·ª± nh∆∞ boxes, ƒë√¢y l√† tensor ch·ª©a th√¥ng tin v·ªÅ bounding boxes c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng, nh∆∞ng ƒë∆∞·ª£c t·ªï ch·ª©c theo c·∫•u tr√∫c tensor kh√°c.

id: ƒê√¢y l√† m·ªôt thu·ªôc t√≠nh (attribute) kh√¥ng c√≥ gi√° tr·ªã c·ª• th·ªÉ trong k·∫øt qu·∫£.

is_track: ƒê√¢y l√† m·ªôt thu·ªôc t√≠nh (attribute) ƒë√°nh d·∫•u xem li·ªáu c√°c bounding boxes c√≥ ph·∫£i l√† theo d√µi c√°c ƒë·ªëi t∆∞·ª£ng hay kh√¥ng.

orig_shape: ƒê√¢y l√† k√≠ch th∆∞·ªõc g·ªëc c·ªßa h√¨nh ·∫£nh tr∆∞·ªõc khi ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi m√¥ h√¨nh.

shape: ƒê√¢y l√† k√≠ch th∆∞·ªõc c·ªßa h√¨nh ·∫£nh sau khi ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi m√¥ h√¨nh.

xywh: ƒê√¢y l√† tensor ch·ª©a th√¥ng tin v·ªÅ t·ªça ƒë·ªô v√† k√≠ch th∆∞·ªõc c·ªßa bounding boxes d∆∞·ªõi d·∫°ng (x_center, y_center, width, height).

xywhn: ƒê√¢y l√† tensor ch·ª©a th√¥ng tin t∆∞∆°ng t·ª± xywh, nh∆∞ng ƒë∆∞·ª£c chu·∫©n h√≥a trong kho·∫£ng t·ª´ 0 ƒë·∫øn 1.

xyxy: ƒê√¢y l√† tensor ch·ª©a th√¥ng tin v·ªÅ t·ªça ƒë·ªô c·ªßa bounding boxes d∆∞·ªõi d·∫°ng (x_min, y_min, x_max, y_max).

xyxyn: ƒê√¢y l√† tensor ch·ª©a th√¥ng tin t∆∞∆°ng t·ª± xyxy, nh∆∞ng ƒë∆∞·ª£c chu·∫©n h√≥a trong kho·∫£ng t·ª´ 0 ƒë·∫øn 1.



Th√¥ng tin v·ªÅ mask trong ultralytics.engine.results.Masks bao g·ªìm:

data v√† masks: ƒê√¢y l√† hai tensor c√≥ c√πng k√≠ch th∆∞·ªõc, ƒë·∫°i di·ªán cho mask c·ªßa c√°c v·∫≠t th·ªÉ trong h√¨nh ·∫£nh. M·ªói tensor c√≥ k√≠ch th∆∞·ªõc (1, H, W), trong ƒë√≥ H l√† chi·ªÅu cao c·ªßa mask v√† W l√† chi·ªÅu r·ªông c·ªßa mask. Mask ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng ma tr·∫≠n c√°c gi√° tr·ªã s·ªë th·ª±c trong kho·∫£ng t·ª´ 0 ƒë·∫øn 1, trong ƒë√≥ 0 th∆∞·ªùng ƒë·∫°i di·ªán cho v√πng kh√¥ng c√≥ v·∫≠t th·ªÉ v√† 1 ƒë·∫°i di·ªán cho v√πng ch·ª©a v·∫≠t th·ªÉ.

orig_shape: K√≠ch th∆∞·ªõc c·ªßa h√¨nh ·∫£nh g·ªëc tr∆∞·ªõc khi ƒë∆∞·ª£c thay ƒë·ªïi k√≠ch th∆∞·ªõc ƒë·ªÉ ph√π h·ª£p v·ªõi m√¥ h√¨nh.

segments: M·ªôt danh s√°ch c√°c ƒëi·ªÉm tr√™n bi√™n c·ªßa mask, m·ªói ƒëi·ªÉm ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng t·ªça ƒë·ªô (x, y) tr√™n ma tr·∫≠n mask.

shape: K√≠ch th∆∞·ªõc c·ªßa tensor mask, c√≥ d·∫°ng (1, H, W), trong ƒë√≥ H l√† chi·ªÅu cao c·ªßa mask v√† W l√† chi·ªÅu r·ªông c·ªßa mask.

xy v√† xyn: xy l√† danh s√°ch c√°c ƒëi·ªÉm tr√™n bi√™n c·ªßa mask, m·ªói ƒëi·ªÉm ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng t·ªça ƒë·ªô (x, y) trong ƒë·ªãnh d·∫°ng tensor. xyn l√† danh s√°ch t·ªça ƒë·ªô c·ªßa c√°c ƒëi·ªÉm tr√™n bi√™n c·ªßa mask ƒë∆∞·ª£c chu·∫©n h√≥a trong kho·∫£ng t·ª´ 0 ƒë·∫øn 1, th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ v·∫Ω mask tr√™n h√¨nh ·∫£nh.

V·ªõi th√¥ng tin n√†y, b·∫°n c√≥ th·ªÉ truy c·∫≠p v√† x·ª≠ l√Ω mask c·ªßa c√°c v·∫≠t th·ªÉ ƒë∆∞·ª£c ph√°t hi·ªán trong h√¨nh ·∫£nh.


/////////////////////////////////

OpenCV cung c·∫•p m·ªôt s·ªë ph∆∞∆°ng ph√°p ƒë·ªÉ x√°c ƒë·ªãnh g√≥c quay c·ªßa m·ªôt v·∫≠t trong m·ªôt ·∫£nh. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë c√°ch th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng:

S·ª≠ d·ª•ng Hough Line Transform (Bi·∫øn ƒë·ªïi Hough cho ƒë∆∞·ªùng th·∫≥ng): ƒê√¢y l√† m·ªôt trong nh·ªØng ph∆∞∆°ng ph√°p ph·ªï bi·∫øn ƒë·ªÉ x√°c ƒë·ªãnh g√≥c quay c·ªßa v·∫≠t th·ªÉ tr√™n ·∫£nh. B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng h√†m cv2.HoughLines() ƒë·ªÉ t√¨m c√°c ƒë∆∞·ªùng th·∫≥ng trong ·∫£nh sau ƒë√≥ t√≠nh to√°n g√≥c gi·ªØa ƒë∆∞·ªùng th·∫≥ng v√† tr·ª•c t·ªça ƒë·ªô.

S·ª≠ d·ª•ng Feature Matching (Kh·ªõp ƒë·∫∑c tr∆∞ng): B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng thu·∫≠t to√°n kh·ªõp ƒë·∫∑c tr∆∞ng nh∆∞ SIFT (Scale-Invariant Feature Transform) ho·∫∑c SURF (Speeded-Up Robust Features) ƒë·ªÉ t√¨m c√°c ƒëi·ªÉm ƒë·∫∑c tr∆∞ng tr√™n v·∫≠t th·ªÉ v√† sau ƒë√≥ t√≠nh to√°n g√≥c quay t·ª´ c√°c ƒëi·ªÉm n√†y.

=>> (kh√¥ng d√πng ƒë∆∞·ª£c) S·ª≠ d·ª•ng Contour Detection (Ph√°t hi·ªán ƒë∆∞·ªùng vi·ªÅn): N·∫øu v·∫≠t th·ªÉ c√≥ h√¨nh d·∫°ng ƒë·∫∑c tr∆∞ng, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng h√†m cv2.findContours() ƒë·ªÉ t√¨m ƒë∆∞·ªùng vi·ªÅn c·ªßa v·∫≠t th·ªÉ v√† sau ƒë√≥ t√≠nh to√°n g√≥c quay d·ª±a tr√™n h∆∞·ªõng c·ªßa ƒë∆∞·ªùng vi·ªÅn.

S·ª≠ d·ª•ng Homography: N·∫øu b·∫°n c√≥ m·ªôt v·∫≠t th·ªÉ v·ªõi bi·∫øn ƒë·ªïi homography (m·ªôt lo·∫°i bi·∫øn ƒë·ªïi affine ho·∫∑c perspective), b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng h√†m cv2.findHomography() ƒë·ªÉ x√°c ƒë·ªãnh ma tr·∫≠n homography v√† sau ƒë√≥ t√≠nh to√°n g√≥c quay t·ª´ ma tr·∫≠n n√†y.

D∆∞·ªõi ƒë√¢y l√† m·ªôt v√≠ d·ª• s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p Hough Line Transform ƒë·ªÉ x√°c ƒë·ªãnh g√≥c quay c·ªßa v·∫≠t th·ªÉ trong ·∫£nh:





////////////////////////////////////


ƒê·ªÉ th√™m t√¢m c·ªßa bounding box v√†o ·∫£nh, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng th∆∞ vi·ªán OpenCV (ho·∫∑c m·ªôt th∆∞ vi·ªán x·ª≠ l√Ω ·∫£nh t∆∞∆°ng t·ª±). D∆∞·ªõi ƒë√¢y l√† c√°ch b·∫°n c√≥ th·ªÉ th·ª±c hi·ªán ƒëi·ªÅu n√†y:

Tr√≠ch xu·∫•t t·ªça ƒë·ªô t√¢m c·ªßa bounding boxes t·ª´ tensor xywh (ho·∫∑c xyxy n·∫øu b·∫°n mu·ªën).

V·∫Ω m·ªôt ƒëi·ªÉm ho·∫∑c m·ªôt ƒë∆∞·ªùng tr·ªè t·ªõi t√¢m c·ªßa bounding box tr√™n ·∫£nh g·ªëc.

D∆∞·ªõi ƒë√¢y l√† m·ªôt v√≠ d·ª• v·ªÅ c√°ch th·ª±c hi·ªán ƒëi·ªÅu n√†y:

import cv2
import numpy as np

# Load the image
image = cv2.imread('/content/drive/MyDrive/NhanDienvat/NDVat/train/images/20230919_104657_jpg.rf.f419369aed5b63d2614e2c2014ef43e5.jpg')

# Extract the 'xywh' tensor from the results (assuming a single image prediction)
xywh = results.pred[0].xywh

# Iterate through bounding boxes and draw a dot at the center
for bbox in xywh:
    x_center, y_center, width, height = bbox[:4]
    x_center = int(x_center)
    y_center = int(y_center)
    cv2.circle(image, (x_center, y_center), 5, (0, 255, 0), -1)  # Draw a green circle at the center

# Save the image with center points
cv2.imwrite('/content/center_points.jpg', image)

# Display the image (optional)
cv2.imshow('Image with Center Points', image)
cv2.waitKey(0)
cv2.destroyAllWindows()

//////ƒê·ªÉ t√≠nh to√°n g√≥c xoay c·ªßa m·ªôt ƒë∆∞·ªùng th·∫≥ng n·∫±m trong m·ªôt h√¨nh ·∫£nh, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng m·ªôt s·ªë ph∆∞∆°ng ph√°p x·ª≠ l√Ω h√¨nh ·∫£nh v√† to√°n h·ªçc. D∆∞·ªõi ƒë√¢y l√† m·ªôt c√°ch ti√™u bi·ªÉu ƒë·ªÉ th·ª±c hi·ªán ƒëi·ªÅu n√†y:

Ph√°t hi·ªán ƒë∆∞·ªùng th·∫≥ng: ƒê·∫ßu ti√™n, b·∫°n c·∫ßn ph√°t hi·ªán ƒë∆∞·ªùng th·∫≥ng trong h√¨nh ·∫£nh. OpenCV cung c·∫•p m·ªôt s·ªë ph∆∞∆°ng ph√°p ph√°t hi·ªán ƒë∆∞·ªùng th·∫≥ng, ch·∫≥ng h·∫°n nh∆∞ cv2.HoughLines ho·∫∑c cv2.HoughLinesP d·ª±a tr√™n bi·∫øn ƒë·ªïi Hough.

T√≠nh to√°n g√≥c xoay: Sau khi b·∫°n ƒë√£ c√≥ danh s√°ch c√°c ƒë∆∞·ªùng th·∫≥ng ƒë∆∞·ª£c ph√°t hi·ªán, b·∫°n c√≥ th·ªÉ t√≠nh to√°n g√≥c xoay c·ªßa m·ªói ƒë∆∞·ªùng th·∫≥ng. ƒê·ªÉ l√†m ƒëi·ªÅu n√†y, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng h√†m to√°n h·ªçc nh∆∞ atan2 (t√≠nh to√°n g√≥c d·ª±a tr√™n c√°c t·ªça ƒë·ªô c·ªßa ƒë·∫ßu m√∫t c·ªßa ƒë∆∞·ªùng th·∫≥ng) ho·∫∑c arctan (t√≠nh to√°n g√≥c d·ª±a tr√™n h·ªá s·ªë g√≥c c·ªßa ƒë∆∞·ªùng th·∫≥ng).

D∆∞·ªõi ƒë√¢y l√† m·ªôt v√≠ d·ª• c·ª• th·ªÉ s·ª≠ d·ª•ng OpenCV ƒë·ªÉ t√≠nh to√°n g√≥c xoay c·ªßa ƒë∆∞·ªùng th·∫≥ng:

python
Copy code
import cv2
import numpy as np

# ƒê·ªçc h√¨nh ·∫£nh
image = cv2.imread('image_with_line.jpg', cv2.IMREAD_GRAYSCALE)

# Ph√°t hi·ªán ƒë∆∞·ªùng th·∫≥ng b·∫±ng bi·∫øn ƒë·ªïi Hough
lines = cv2.HoughLines(image, 1, np.pi / 180, threshold=100)

# T√≠nh to√°n g√≥c xoay c·ªßa t·ª´ng ƒë∆∞·ªùng th·∫≥ng v√† chuy·ªÉn ƒë·ªïi th√†nh ƒë·ªô
for rho, theta in lines[:, 0]:
    angle = theta * 180 / np.pi
    print(f'G√≥c xoay: {angle} ƒë·ªô')

# Hi·ªÉn th·ªã h√¨nh ·∫£nh v·ªõi ƒë∆∞·ªùng th·∫≥ng ƒë√£ ph√°t hi·ªán
for rho, theta in lines[:, 0]:
    a = np.cos(theta)
    b = np.sin(theta)
    x0 = a * rho
    y0 = b * rho
    x1 = int(x0 + 1000 * (-b))
    y1 = int(y0 + 1000 * (a))
    x2 = int(x0 - 1000 * (-b))
    y2 = int(y0 - 1000 * (a))
    cv2.line(image, (x1, y1), (x2, y2), (0, 0, 255), 2)

cv2.imshow('Image with Lines', image)
cv2.waitKey(0)
cv2.destroyAllWindows()


///////////////////////////////////////////////xac dinh huong orientation.py

# from __future__ import print_function
# from __future__ import division
import cv2 as cv
import numpy as np
import argparse
from math import atan2, cos, sin, sqrt, pi
def drawAxis(img, p_, q_, colour, scale,degree):
 p = list(p_)
 q = list(q_)
 print("diemP: ",p)
 print("diemQ: ",q)
 text1 = f"({p[0]}, {p[1]})"
 
 cv.putText(img, text1, (p[0]+30,p[1]+30), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)
 angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians
 print("angle: ",degree)
 text2 = f"({degree})"
 cv.putText(img, text2, (p[0]+150,p[1]-30), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)

 hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))
 # Here we lengthen the arrow by a factor of scale
 q[0] = p[0] - scale * hypotenuse * cos(angle)
 q[1] = p[1] - scale * hypotenuse * sin(angle)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 # create the arrow hooks
 p[0] = q[0] + 9 * cos(angle + pi / 4)
 p[1] = q[1] + 9 * sin(angle + pi / 4)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 p[0] = q[0] + 9 * cos(angle - pi / 4)
 p[1] = q[1] + 9 * sin(angle - pi / 4)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 
def getOrientation(pts, img):
 
 sz = len(pts)
 data_pts = np.empty((sz, 2), dtype=np.float64)
 for i in range(data_pts.shape[0]):
  data_pts[i,0] = pts[i,0,0]
  data_pts[i,1] = pts[i,0,1]
 # Perform PCA analysis
 mean = np.empty((0))
 mean, eigenvectors, eigenvalues = cv.PCACompute2(data_pts, mean)
 # Store the center of the object
 cntr = (int(mean[0,0]), int(mean[0,1]))
 
 
 cv.circle(img, cntr, 3, (255, 0, 255), 2)
 p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])
 p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])

 angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians
 degree = angle * (180 / np.pi)

 drawAxis(img, cntr, p1, (0, 255, 0), 1,degree)
 drawAxis(img, cntr, p2, (255, 255, 0), 5,degree)

 
 return degree
parser = argparse.ArgumentParser(description='Code for Introduction to Principal Component Analysis (PCA) tutorial.\
 This program demonstrates how to use OpenCV PCA to extract the orientation of an object.')
parser.add_argument('--input', help='Path to input image.', default='pca_test1.jpg')
args = parser.parse_args()
src = cv.imread(cv.samples.findFile(args.input))


# X√°c ƒë·ªãnh c√°c t·ªça ƒë·ªô c·ªßa h√¨nh ·∫£nh b·∫°n mu·ªën c·∫Øt
# x, y, width, height = 900, 900, 1200, 1000  # V√≠ d·ª•: c·∫Øt t·ª´ (100, 100) ƒë·∫øn (300, 300)

# Th·ª±c hi·ªán ph√©p c·∫Øt
# src = image[y:y+height, x:x+width]

# Check if image is loaded successfully
if src is None:
 print('Could not open or find the image: ', args.input)
 exit(0)
# cv.imshow('src', src)
# Convert image to grayscale
gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)
# Convert image to binary
_, bw = cv.threshold(gray, 50, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
contours, _ = cv.findContours(bw, cv.RETR_LIST, cv.CHAIN_APPROX_NONE)
for i, c in enumerate(contours):
 # Calculate the area of each contour
 area = cv.contourArea(c)
 # Ignore contours that are too small or too large
 if area < 1e2 or 1e5 < area:
  continue
 # Draw each contour only for visualisation purposes
 cv.drawContours(src, contours, i, (0, 0, 255), 2)
 # Find the orientation of each shape
 getOrientation(c, src)
 cv.imwrite('orientation_image.jpg',src)
# cv.imshow('output', src)
# cv.waitKey()


///////////////////////////////////////////////xac dinh huong orientation.py

# from __future__ import print_function
# from __future__ import division
import cv2 as cv
import numpy as np
import argparse
from math import atan2, cos, sin, sqrt, pi
def drawAxis(img, p_, q_, colour, scale,degree):
 p = list(p_)
 q = list(q_)
 print("diemP: ",p)
 print("diemQ: ",q)
 text1 = f"({p[0]}, {p[1]})"
 
 cv.putText(img, text1, (p[0]+30,p[1]+30), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)
 angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians
 print("angle: ",degree)
 text2 = f"({degree})"
 cv.putText(img, text2, (p[0]+150,p[1]-30), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)

 hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))
 # Here we lengthen the arrow by a factor of scale
 q[0] = p[0] - scale * hypotenuse * cos(angle)
 q[1] = p[1] - scale * hypotenuse * sin(angle)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 # create the arrow hooks
 p[0] = q[0] + 9 * cos(angle + pi / 4)
 p[1] = q[1] + 9 * sin(angle + pi / 4)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 p[0] = q[0] + 9 * cos(angle - pi / 4)
 p[1] = q[1] + 9 * sin(angle - pi / 4)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 
def getOrientation(pts, img):
 
 sz = len(pts)
 data_pts = np.empty((sz, 2), dtype=np.float64)
 for i in range(data_pts.shape[0]):
  data_pts[i,0] = pts[i,0,0]
  data_pts[i,1] = pts[i,0,1]
 # Perform PCA analysis
 mean = np.empty((0))
 mean, eigenvectors, eigenvalues = cv.PCACompute2(data_pts, mean)
 # Store the center of the object
 cntr = (int(mean[0,0]), int(mean[0,1]))
 
 
 cv.circle(img, cntr, 3, (255, 0, 255), 2)
 p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])
 p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])

 angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians
 degree = angle * (180 / np.pi)

 drawAxis(img, cntr, p1, (0, 255, 0), 1,degree)
 drawAxis(img, cntr, p2, (255, 255, 0), 5,degree)

 
 return degree
parser = argparse.ArgumentParser(description='Code for Introduction to Principal Component Analysis (PCA) tutorial.\
 This program demonstrates how to use OpenCV PCA to extract the orientation of an object.')
parser.add_argument('--input', help='Path to input image.', default='pca_test1.jpg')
args = parser.parse_args()
src = cv.imread(cv.samples.findFile(args.input))


# X√°c ƒë·ªãnh c√°c t·ªça ƒë·ªô c·ªßa h√¨nh ·∫£nh b·∫°n mu·ªën c·∫Øt
# x, y, width, height = 900, 900, 1200, 1000  # V√≠ d·ª•: c·∫Øt t·ª´ (100, 100) ƒë·∫øn (300, 300)

# Th·ª±c hi·ªán ph√©p c·∫Øt
# src = image[y:y+height, x:x+width]

# Check if image is loaded successfully
if src is None:
 print('Could not open or find the image: ', args.input)
 exit(0)
# cv.imshow('src', src)
# Convert image to grayscale
gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)
# Convert image to binary
_, bw = cv.threshold(gray, 50, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
contours, _ = cv.findContours(bw, cv.RETR_LIST, cv.CHAIN_APPROX_NONE)
for i, c in enumerate(contours):
 # Calculate the area of each contour
 area = cv.contourArea(c)
 # Ignore contours that are too small or too large
 if area < 1e2 or 1e5 < area:
  continue
 # Draw each contour only for visualisation purposes
 cv.drawContours(src, contours, i, (0, 0, 255), 2)
 # Find the orientation of each shape
 getOrientation(c, src)
 cv.imwrite('orientation_image.jpg',src)
# cv.imshow('output', src)
# cv.waitKey()


//////////////////////////////////////////////



# ///////////////////////////////////////////////////////////

from ultralytics import YOLO
import cv2
import numpy as np

import argparse
from math import atan2, cos, sin, sqrt, pi
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

link = r'/content/nhandienvat/detectByYolov8/dataset_specialItems/train/images/20230922_154950_jpg.rf.a55265100fc54aa788b672e6a11b49f2.jpg'

image = cv2.imread(link)

def drawAxis(img, p_, q_, colour, scale,degree):
 p = list(p_)
 q = list(q_)
 print("diemP: ",p)
 print("diemQ: ",q)
 text1 = f"({p[0]}, {p[1]})"
 
#  cv.putText(img, text1, (p[0]+30,p[1]+30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)
 angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians
 print("angle: ",degree)
 text2 = f"{round(degree,2)} degree"
 cv2.putText(img, text2, (p[0]+10,p[1]-30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)

 hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))
 # Here we lengthen the arrow by a factor of scale
 q[0] = p[0] - scale * hypotenuse * cos(angle)
 q[1] = p[1] - scale * hypotenuse * sin(angle)
 cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv2.LINE_AA)
 # create the arrow hooks
 p[0] = q[0] + 9 * cos(angle + pi / 4)
 p[1] = q[1] + 9 * sin(angle + pi / 4)
 cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv2.LINE_AA)
 p[0] = q[0] + 9 * cos(angle - pi / 4)
 p[1] = q[1] + 9 * sin(angle - pi / 4)
 cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv2.LINE_AA)
 
def getOrientation(pts, img):
 sz = len(pts)
 data_pts = np.empty((sz, 2), dtype=np.float64)
 for i in range(data_pts.shape[0]):
  data_pts[i,0] = pts[i,0,0]
  data_pts[i,1] = pts[i,0,1]
 # Perform PCA analysis
 mean = np.empty((0))
 mean, eigenvectors, eigenvalues = cv2.PCACompute2(data_pts, mean)
 # Store the center of the object
 cntr = (int(mean[0,0]), int(mean[0,1]))
 
 
 cv2.circle(img, cntr, 2, (255, 0, 255), 1)
 p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])
 p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])
#  cv.circle(img, (eigenvectors[0,1], eigenvectors[0,0]), 2, (0, 0, 255), 1)
 angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians
 degree = angle * (180 / np.pi)

 drawAxis(img, cntr, p1, (0, 255, 0), 2,degree)
 drawAxis(img, cntr, p2, (255, 255, 0), 5,degree)
 return degree


# beta = -50  # Gi·∫£m ƒë·ªô s√°ng

# # S·ª≠ d·ª•ng h√†m cv2.convertScaleAbs ƒë·ªÉ gi·∫£m ƒë·ªô s√°ng c·ªßa ·∫£nh
# src = cv2.convertScaleAbs(image, alpha=1, beta=beta)


# # Check if image is loaded successfully
# if src is None:
# #  print('Could not open or find the image: ', args.input)
#  exit(0)
# # cv2.imshow('src', src)
# # Convert image to grayscale
# gray_picture = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)

# gray_inverted = cv2.bitwise_not(gray_picture)
# # Convert image to binary
# _, bw = cv2.threshold(gray_picture, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
# contours, _ = cv2.findContours(bw, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
# for i, c in enumerate(contours):
#  # Calculate the area of each contour
#  area = cv2.contourArea(c)
#  # Ignore contours that are too small or too large
#  if area < 1e2*3 or 1e5 < area:
#   continue
#  # Draw each contour only for visualisation purposes
#  cv2.drawContours(src, contours, i, (0, 0, 255), 2)
#  # Find the orientation of each shape
#  getOrientation(c, src)
#  cv2.imwrite('orientation_image.jpg',src)
# //////////////
# cv2.imshow('output', src)
# Assuming you have a grayscale image in the 'gray_picture' variable
# Normalize the grayscale values to be between 0 and 1

# plt.imshow(bw, cmap="gray")
# plt.title("Bitwise Gray")
# plt.axis("off")
# plt.show()

# plt.imshow(gray_inverted, cmap="gray")
# plt.title("gray_inverted Gray")
# plt.axis("off")
# plt.show()

# ///////////////////////////////////

# Load a model
model = YOLO('yolov8n-seg.pt')  # load an official model
model = YOLO(r'/content/nhandienvat/runs/segment/train/weights/best.pt')  # load a custom model

# Predict with the model
results = model(r"/content/nhandienvat/detectByYolov8/dataset_specialItems/train/images/20230922_154950_jpg.rf.a55265100fc54aa788b672e6a11b49f2.jpg", save = True)  # predict on an image


myList = []

mylistmask = []

checkitem = []
for r in results:
    # mylistmask = r.masks.xy #ss·ª≠ d·ª•ng n·∫øu d√πng ph∆∞∆°ng ph√°p t√¨m t·ªça ƒë·ªô t√¢m theo c√°c ƒëi·ªÉm masks
    # print("mask: ",r.masks[0].xy[0])
    # print("shape: ",r.masks.shape)
    # print("masks: ",r.masks)
    print("boxes: ",r.boxes)  # print the Boxes object containing the detection bounding boxes
    myList = r.boxes.xywh.tolist()
    checkitem = r.boxes.cls.tolist()
    # print(int(checkitem[2]))
print("myList: ",myList)
print("length: ",len(myList))


# print(list)
count = 0

while count < len(checkitem):
  print("count: ",count)
  if(checkitem[count] == 3.0):
    print("3")
    bBoxitem = myList[count]
    mask = np.zeros_like(image, dtype=np.uint8)
#     # ƒê·ªãnh nghƒ©a m√†u xanh m√† b·∫°n mu·ªën s·ª≠ d·ª•ng (v√≠ d·ª•: m√†u xanh l√° c√¢y)
#     green_color = ( 255,255, 0)  # Xanh l√° c√¢y: (B, G, R)

# # G√°n gi√° tr·ªã m√†u xanh cho m·∫∑t n·∫°
#     mask[:, :] = green_color

    a = (bBoxitem[0] - bBoxitem[2]/2, bBoxitem[1] - bBoxitem[3]/2)
    b = (bBoxitem[0] + bBoxitem[2]/2, bBoxitem[1] - bBoxitem[3]/2)
    c = (bBoxitem[0] + bBoxitem[2]/2, bBoxitem[1] + bBoxitem[3]/2)
    d = (bBoxitem[0] - bBoxitem[2]/2, bBoxitem[1] + bBoxitem[3]/2)

    points = np.array([a, b,c ,d], dtype=np.int32)
    cv2.fillPoly(mask, [points], (255, 255, 255))
    src = cv2.bitwise_and(image, mask)
    beta = -50  # Gi·∫£m ƒë·ªô s√°ng
        # S·ª≠ d·ª•ng h√†m cv2.convertScaleAbs ƒë·ªÉ gi·∫£m ƒë·ªô s√°ng c·ªßa ·∫£nh
    result = cv2.convertScaleAbs(src, alpha=1, beta=beta)


    # Check if image is loaded successfully
    if result is None:
    #  print('Could not open or find the image: ', args.input)
        exit(0)
    # cv2.imshow('src', src)
    # Convert image to grayscale
    gray_picture = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)

    gray_inverted = cv2.bitwise_not(gray_picture)
    # Convert image to binary
    _, bw = cv2.threshold(gray_picture, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    # cv2.imwrite('bibi.jpg', bw)
    
    contours, _ = cv2.findContours(bw, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    for i, c in enumerate(contours):
    # Calculate the area of each contour
      area = cv2.contourArea(c)
    # Ignore contours that are too small or too large
      if area < 1e2*3 or 1e5 < area:
        continue
    # Draw each contour only for visualisation purposes
        cv2.drawContours(result, contours, i, (0, 0, 255), 2)
    # Find the orientation of each shape
      kq =  getOrientation(c, result)
      print("kq: ", kq)
  if checkitem[count] == 4.0:

    list = myList[count]
    point = (list[0],list[1])
    print("point :",point)
    color = (0, 255, 0)
    # K√≠ch th∆∞·ªõc c·ªßa ƒëi·ªÉm
    thickness = -1  # ƒê·∫∑t -1 ƒë·ªÉ v·∫Ω m·ªôt ƒëi·ªÉm ƒë·∫ßy ƒë·ªß
    # V·∫Ω ƒëi·ªÉm tr√™n h√¨nh ·∫£nh
    cv2.circle(image, (int(list[0]),int(list[1])), 3, (0, 0, 255), thickness) #ch·∫•m ƒëi·ªÉm v√†o ·∫£nh ·ªü t·ªça ƒë·ªô t√¢m c·ªßa bounding box c·ªßa l·ªó t√¢m
    # in t·ªça ƒë·ªô c·ªßa t√¢m
    text = f"({int(list[0])}, {int(list[1])})"
    cv2.putText(image, text, (int(list[0])+30,int(list[1])+30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
    cv2.line(image, (int(list[0]),int(list[1])), (int(list[0]) + 50, int(list[1])), (0, 0, 255), 2)  # V·∫Ω tr·ª•c X (m√†u ƒë·ªè)
    cv2.line(image, (int(list[0]),int(list[1])), (int(list[0]), int(list[1]) + 50), (0, 255, 0), 2)  # V·∫Ω tr·ª•c Y (m√†u xanh l√°)
  count = count + 1
# print("list masks: ",mylistmask.tolist()[0])



# ///////////////////C√°ch l·∫•y t√¢m t·ª´ c√°c ƒëi·ªÉm masks/////////////
# count = 0
# while count < len(mylistmask):
#   if checkitem[count] == 4.0:
#     print("count: ", count)
#     print(mylistmask[count].tolist())
#     listitem = mylistmask[count].tolist()
#     count2 = 0
#     totalx= 0
#     totaly = 0
#     x = 0
#     y = 0
    
#     while count2 < len(listitem):
#       print(listitem[count2])
#       coodinate = listitem[count2]
#       totalx = totalx + coodinate[0]
#       totaly = totaly + coodinate[1]
#       # cv2.circle(image, (int(coodinate[0]),int(coodinate[1])), 3, color, thickness)
#       cv2.drawMarker(image, (int(coodinate[0]),int(coodinate[1])), color, markerType=cv2.MARKER_STAR, markerSize=2)
#       count2 = count2 + 1
#     x = totalx / len(listitem)
#     y = totaly / len(listitem)
#     cv2.drawMarker(image, (int(x),int(y)), color, markerType=cv2.MARKER_STAR, markerSize=2)
#   count = count + 1


cv2.imwrite('output_image2.jpg', image)
cv2.imwrite('output_image3.jpg', result)


#K·∫øt lu·∫≠n r·∫±ng l·∫•y t√¢m c·ªßa bounding box c·ªßa l·ªó t√¢m chu·∫©n h∆°n c√°ch l·∫•y t√¢m t·ª´ c√°c ƒëi·ªÉm masks c·ªßa l·ªó t√¢m 


////////////////tim tam

Th·ª±c hi·ªán ti·ªÅn x·ª≠ l√≠ l·∫ßn l∆∞·ª£t b·∫±ng c√°c thu·∫≠t to√°n: Constrast Stretch ƒë·ªÉ
chi·ªÅu ch·ªânh l·∫°i ƒë·ªô t∆∞∆°ng ph·∫£n, Threshold ƒë·ªÉ c√≥ ƒë∆∞·ª£c ·∫£nh binary, t√¨m 
c·∫°nh b·∫±ng Canny Edge, v√† nh·ªØng thu·∫≠t to√°n nh∆∞ dilation, find 
Contours.
Trong ƒë√≥, thu·∫≠t to√°n Constrast Strect, Threshold, find Contours ƒë√£ 
ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p ·ªü c√°c ph·∫ßn b√™n tr√™n
Thu·∫≠t to√°n Canny Edge c√≥ th·ªÉ xem th√™m ·ªü ph·∫ßn ph·ª• l·ª•c 1.3
Thu·∫≠t to√°n Dilation:
- ƒê√¢y l√† thu·∫≠t to√°n b·ªï tr·ª£ th√™m cho thu·∫≠t to√°n Canny Edges nh·∫±m 
l√†m tƒÉng ch·∫•t l∆∞·ª£ng c√°c c·∫°nh tr∆∞·ªõc khi ƒë∆∞a v√†o thu·∫≠t to√°n Find 
Contours. 
- Dilation l√† m·ªôt trong nh·ªØng ph√©p bi·∫øn ƒë·ªïi thao t√°c t√°c ƒë·ªông ƒë·∫øn 
h√¨nh th√°i c·ªßa v·∫≠t th·ªÉ trong ·∫£nh. Th√¥ng qua 1 Kernel c√≥ k√≠ch th∆∞·ªõc 
ƒë∆∞·ª£c ch·ªânh ƒë·ªãnh t·ª´ ban ƒë·∫ßu. N√≥ s·∫Ω th·ª±c hi·ªán t√¨m pixel c√≥ gi√° tr·ªã
l·ªõn nh·∫•t sau ƒë√≥ g√°n to√†n b·ªô c√°c pixel n·∫±m trong v√πng Kernel ƒë√≥ 
b·∫±ng ch√≠nh gi√° tr·ªã l·ªõn nh·∫•t ƒë√≥.
- ƒêi·ªÅu n√†y ƒë·ªìng nghƒ©a v·ªõi vi·ªác, n√≥ s·∫Ω gi√∫p cho h√¨nh d·∫°ng c·ªßa v·∫≠t 
th·ªÉ, c·ªßa c·∫°nh n√≥ s·∫Ω ƒë∆∞·ª£c ph√¨nh to ra h∆°n v√† c√≥ kh·∫£ nƒÉng li√™n k·∫øt 
ƒë∆∞·ª£c nh·ªØng c·∫°nh r·ªùi r·∫°c ·ªü m·ªôt m·ª©c ƒë·ªô n√†o ƒë√≥.
- Thu·∫≠t to√°n n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng nh·∫±m m·ª•c ƒë√≠ch li√™n k·∫øt l·∫°i c√°c c·∫°nh 
thu ƒë∆∞·ª£c t·ª´ thu·∫≠t to√°n Canny ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng s·∫Ω kh√¥ng c√≥ c·∫°nh 
n√†o b·ªã ƒë·ª©t kh√∫c gi·ªØa ch·ª´ng.

Thu·∫≠t to√°n t√¨m t√¢m c·ªßa con h√†ng
Sau khi tr√≠ch xu·∫•t ƒë∆∞·ª£c RoI d·ª±a tr√™n k·∫øt qu·∫£ t·ª´ YOLO, ta th·ª±c hi·ªán √°p 
d·ª•ng 1 s·ªë thu·∫≠t to√°n x·ª≠ l√≠ ·∫£nh nh·∫±m m·ª•c ƒë√≠ch thu ƒë∆∞·ª£c ·∫£nh binary m√† 
ch·ªâ ch·ª©a c√°c c·∫°nh b√™n trong

ƒê·ªÉ th·ª±c hi·ªán ƒë∆∞·ª£c ƒëi·ªÅu ƒë√≥, l·∫ßn l∆∞·ª£t ƒë∆∞a RoI qua c√°c thu·∫≠t to√°n theo th·ª©
t·ª± Contrast stretching, thresholding, canny edge detection, dilation
Sau khi c√≥ ƒë∆∞·ª£c ·∫£nh ƒë·∫ßu ra mong mu·ªën, ta th·ª±c hi·ªán t√¨m contour c·ªßa 
RoI, tuy nhi√™n v√¨ s·ªë l∆∞·ª£ng contour tr·∫£ v·ªÅ s·∫Ω l·ªõn h∆°n 1. V√¨ th·∫ø m√† ta s·∫Ω
c√≥ th√™m 1 v√†i b∆∞·ªõc l·ªçc b·ªõt c√°c contour kh√¥ng ph√π h·ª£p.
Sau khi x·ª≠ l√≠ xong v√† thu ƒë∆∞·ª£c contour t·ªëi ∆∞u, ta th·ª±c hi·ªán t√¨m ƒë∆∞·ªùng 
tr√≤n c√≥ b√°n k√≠nh nh·ªè nh·∫•t m√† ch·ª©a t·∫•t c·∫£ c√°c ƒëi·ªÉm trong contour t·ªëi ∆∞u, 
sau ƒë√≥ thu ƒë∆∞·ª£c t√¢m c·ªßa ƒë∆∞·ªùng tr√≤n. ƒê·ªìng th·ªùi, ta c≈©ng th·ª±c hi·ªán t√≠nh 
to√°n t√¢m c·ªßa contour v√† k·∫øt h·ª£p t√≠nh trung b√¨nh v·ªõi t√¢m c·ªßa ƒë∆∞·ªùng tr√≤n 
thu ƒë∆∞·ª£c l√∫c tr∆∞·ªõc ƒë·ªÉ tr·∫£ v·ªÅ gi√° tr·ªã t·ªça ƒë·ªô pixel c·ªßa t√¢m v·∫≠t h√†ng t·ªët 
nh·∫•t
///////////////////////////////////
 T√¨m g√≥c t·ªëi ∆∞u
 S·ª≠ d·ª•ng Template Matching cho vi·ªác tinh ch·ªânh l·∫°i g√≥c c·ªßa con 
h√†ng
Ti·∫øn h√†nh t·∫°o Template tr∆∞·ªõc:

Sau khi c√≥ ƒë∆∞·ª£c template tr√™n, ta s·∫Ω ti·∫øn h√†nh xoay template m·ªôt gi√° tr·ªã
b·∫±ng Œ± + g√≥c ƒë·ªÅ xu·∫•t (Œ± s·∫Ω n·∫±m trong kho·∫£ng t·ª´ -50 ƒë·∫øn 50, gi√° tr·ªã n√†y c√≥ th·ªÉ thay ƒë·ªïi t√πy thu·ªôc v√†o vi·ªác m√¥ h√¨nh Segmentation c√≥ c·∫ßn ph·∫£i 
ƒë∆∞·ª£c t·ªëi ∆∞u nhi·ªÅu hay kh√¥ng).

Sau ƒë√≥ ta s·∫Ω ti·∫øn h√†nh matching template ƒë√£ ƒë∆∞·ª£c xoay ·ªü tr√™n v·ªõi m·ªôt 
v√πng roi ƒë√£ ƒë∆∞·ª£c chuy·ªÉn sang d·∫°ng binary, v√† ta l·ª±a ch·ªçn c√≥ gi√° tr·ªã g√≥c 
t·ªëi ∆∞u h∆°n d·ª±a tr√™n ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa template v√† RoI

ÔÉò T·ªëi ∆∞u thu·∫≠t to√°n
√ù t∆∞·ªüng tri·ªÉn khai:
- H·∫°n ch·∫ø c·ªßa thu·∫≠t to√°n hi·ªán t·∫°i l√† s·ª± l·∫∑p l·∫°i vi·ªác t√≠nh to√°n m·∫∑c d√π 
c√≥ th·ªÉ g√≥c ƒë·ªÅ xu·∫•t ƒë√£ l√† g√≥c t·ªëi ∆∞u
- Gi·∫£ ƒë·ªãnh r·∫±ng, n·∫øu g√≥c ƒë·ªÅ xu·∫•t c√≥ th·ªÉ ƒë∆∞·ª£c tinh ch·ªânh v√† cho k·∫øt 
qu·∫£ t·ªët h∆°n, t·ª©c l√† gi√° tr·ªã t∆∞∆°ng ƒë·ªìng gi·ªØa Template v√† RoI s·∫Ω cao 
h∆°n. ƒêi·ªÅu ƒë√≥ ƒë·ªìng nghƒ©a v·ªõi vi·ªác, n·∫øu ch√∫ng ta ki·ªÉm tra xem li·ªáu 
v·ªõi g√≥c tinh ch·ªânh ƒë·∫ßu ti√™n gi√° tr·ªã t∆∞∆°ng ƒë·ªìng (Similarity score) 
tr·∫£ v·ªÅ c√≥ cao h∆°n so v·ªõi ƒë·ªô t∆∞∆°ng ƒë·ªìng khi t√≠nh v·ªõi g√≥c ƒë·ªÅ xu·∫•t t·ª´
Segmentation, th√¨ ta s·∫Ω cho ph√©p vi·ªác tinh ch·ªânh ti·∫øp t·ª•c x·∫£y ra.
- Tuy nhi√™n, n·∫øu khi ƒë·∫øn 1 ƒë·ªô tinh ch·ªânh n√†o ƒë√≥, gi√° tr·ªã similarity tr·∫£
v·ªÅ th·∫•p h∆°n so v·ªõi gi√° tr·ªã similarity c·ªßa g√≥c tinh ch·ªânh tr∆∞·ªõc ƒë√≥. 
Vi·ªác tinh ch·ªânh s·∫Ω b·ªã d·ª´ng ngay l·∫≠p t·ª©c.
- ƒê·ªÉ c√¢n b·∫±ng vi·ªác sai l·ªách theo chi·ªÅu √¢m ho·∫∑c d∆∞∆°ng, ta s·∫Ω th·ª±c 
hi·ªán t√≠nh to√°n c√πng l√∫c ƒë·ªô tinh ch·ªânh theo chi·ªÅu d∆∞∆°ng v√† ƒë·ªô tinh 
ch·ªânh theo chi·ªÅu √¢m. V√≤ng l·∫∑p s·∫Ω k·∫øt th√∫c khi c·∫£ 2 b√™n tinh ch·ªânh 
√¢m v√† d∆∞∆°ng ƒë·ªÅu kh√¥ng th·ªÉ tinh ch·ªânh ƒë∆∞·ª£c g√≥c ƒë·ªÅ xu·∫•t sang g√≥c 
t·ªëi ∆∞u ƒë∆∞·ª£c n·ªØa
 Sau ƒë√≥ ta s·∫Ω th·ª±c hi·ªán so s√°nh c√°c k·∫øt qu·∫£ cu·ªëi c√πng c·ªßa c·∫£ 2 b√™n 
so v·ªõi similarity score c·ªßa g√≥c ƒë·ªÅ xu·∫•t t·ª´ Segmentation ƒë·ªÉ ch·ªçn 
ra g√≥c t·ªëi ∆∞u nh·∫•t cho v·∫≠t th·ªÉ.

L·ª±a ch·ªçn th·ª© t·ª± g·∫Øp con h√†ng
ÔÅ∂ T·ªïng qu√°t
ƒê√¢y l√† b∆∞·ªõc ƒë∆∞·ª£c th·ª±c hi·ªán sau khi ƒë√£ qua thu·∫≠t to√°n x·ª≠ l√≠ ·∫£nh v√† ƒë√£ cho 
ra ƒë∆∞·ª£c t·ªça ƒë·ªô c√°c v·∫≠t h√†ng nh·∫≠n di·ªán ƒë∆∞·ª£c.
·ªû b∆∞·ªõc n√†y, ta s·∫Ω th·ª±c hi·ªán x√°c ƒë·ªãnh m·ª©c ƒë·ªô kh·∫£ thi c√≥ th·ªÉ g·∫Øp ƒë∆∞·ª£c c·ªßa 
v·∫≠t h√†ng, ƒë√¢y l√† b∆∞·ªõc ƒë∆∞·ª£c d√πng ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t v√† gi·∫£m thi·ªÉu m·ª©c ƒë·ªô
r·ªßi ro khi g·∫Øp h√†ng.
ÔÅ∂ V·∫•n ƒë·ªÅ v√† gi·∫£i ph√°p
V·∫•n ƒë·ªÅ: 
- Trong ph·∫°m vi c·ªßa ƒë·ªì √°n, c√°c v·∫≠t h√†ng s·∫Ω ·ªü v·ªã tr√≠ ng·∫´u nhi√™n v√† kh√¥ng 
bi·∫øt tr∆∞·ªõc ƒë∆∞·ª£c v·ªã tr√≠ c·ª• th·ªÉ. Ngo·∫°i tr·ª´ c√°c tr∆∞·ªùng h·ª£p c√°c v·∫≠t h√†ng 
ch·ªìng ch√©o nhau, n·∫±m nghi√™ng theo chi·ªÅu ƒë·ª©ng ho·∫∑c ngang v√† n·∫±m √∫p 
s·∫Ω b·ªã lo·∫°i b·ªè v√† kh√¥ng th·ª±c hi·ªán g·∫Øp, c√≤n l·∫°i t·∫•t c·∫£ c√°c con h√†ng n·∫±m 
ng·ª≠a d√π l√† g·∫ßn nhau ƒë·∫øn d√≠nh v√†o nhau th√¨ v·∫´n s·∫Ω ƒë∆∞·ª£c g·∫Øp ƒë·ªÉ ƒë·∫£m 
b·∫£o s·ªë l∆∞·ª£ng t√≠nh theo ph√∫t.
- T·ª´ ƒë√≥ s·∫Ω ph√°t sinh ra v·∫•n ƒë·ªÅ l√† khi Robot th·ª±c hi·ªán g·∫Øp v·∫≠t, c√≥ kh·∫£
nƒÉng s·∫Ω g√¢y t√°c ƒë·ªông nh·∫π ƒë·∫øn v·∫≠t h√†ng ƒëang g·∫Øp. N·∫øu v·∫≠t h√†ng ƒë√≥ n·∫±m 
ƒë·ªôc l·∫≠p kh√¥ng g·∫ßn c√°c v·∫≠t h√†ng kh√°c, ƒëi·ªÅu n√†y s·∫Ω kh√¥ng ph·∫£i ƒë√°ng lo 
ng·∫°i. Nh∆∞ng n·∫øu v·∫≠t h√†ng ƒëang g·∫Øp c√≥ c√°c v·∫≠t h√†ng kh√°c g·∫ßn ƒë√≥ th·∫≠m 
ch√≠ l√† d√≠nh v√†o, vi·ªác t√°c ƒë·ªông nh∆∞ v·∫≠y c√≥ kh·∫£ nƒÉng l√†m d·ªãch chuy·ªÉn v·ªã
tr√≠ c·ªßa c√°c v·∫≠t h√†ng b√™n c·∫°nh.
- Trong khi ƒë√≥, ·ª©ng v·ªõi m·ªói l·∫ßn nh·∫≠n di·ªán c√°c v·∫≠t h√†ng, th√¨ Robot s·∫Ω
nh·∫≠n to√†n b·ªô c√°c t·ªça ƒë·ªô c·ªßa v·∫≠t h√†ng c√≥ th·ªÉ g·∫Øp ƒë∆∞·ª£c. Vi·ªác l√†m thay 
ƒë·ªïi ƒëi t·ªça ƒë·ªô c·ªßa v·∫≠t h√†ng b√™n c·∫°nh s·∫Ω t√°c ƒë·ªông kh√¥ng h·ªÅ nh·ªè ƒë·∫øn kh·∫£
nƒÉng g·∫Øp v·∫≠t sau n√†y c·ªßa Robot.
- Ch√≠nh v√¨ th·∫ø m√† m·ªôt ƒë·ªô ƒëo m·ª©c ƒë·ªô kh·∫£ thi ƒë∆∞·ª£c √°p d·ª•ng ƒë·ªÉ x√°c ƒë·ªãnh 
li·ªáu v·∫≠t h√†ng ƒë√≥ c√≥ bao nhi√™u ph·∫ßn trƒÉm l√† kh·∫£ thi ƒë·ªÉ g·∫Øp ƒë∆∞·ª£c m√† √≠t 
g√¢y ·∫£nh h∆∞·ªüng ƒë·∫øn c√°c v·∫≠t h√†ng xung quanh.
√ù t∆∞·ªüng:
- Ch√∫ng ta s·∫Ω t·∫≠n d·ª•ng 2 thu·∫≠t to√°n x·ª≠ l√≠ ·∫£nh l√† Contrast stretch v√† 
thresholding ƒë·ªÉ th·ª±c hi·ªán l·∫•y ·∫£nh binary c·ªßa c·∫£ Template v√† RoI.
Tuy nhi√™n, ƒë·ªëi v·ªõi b∆∞·ªõc n√†y, ta s·∫Ω kh√¥ng c·∫Øt RoI b·∫±ng v·ªõi Bounding 
Box t·ª´ YOLO m√† s·∫Ω n·ªõi r·ªông RoI ra l·ªõn h∆°n (Padding 100 pixel ch·ªó
m·ªói ph√≠a c·ªßa RoI) nh·∫±m m·ª•c ƒë√≠ch c√≥ th·ªÉ bao tr·ªçn c√°c khu v·ª±c xung 
quanh c·ªßa v·∫≠t h√†ng h∆°n.
- Sau ƒë√≥ ta th·ª±c hi·ªán t√≠nh t·ªïng s·ªë pixel m√† m·ªói pixel ƒë∆∞·ª£c coi l√† thu·ªôc 
v·∫≠t h√†ng, trong tr∆∞·ªùng h·ª£p c·ª• th·ªÉ nh∆∞ hi·ªán t·∫°i, pixel n√†o c√≥ gi√° tr·ªã b·∫±ng 
0 th√¨ ƒë∆∞·ª£c xem nh∆∞ thu·ªôc v·∫≠t h√†ng. Ta s·∫Ω t√≠nh t·ªïng s·ªë l∆∞·ª£ng pixel ƒë√≥ 
l·∫°i v√† xem n√≥ nh∆∞ gi√° tr·ªã ƒë·∫°i di·ªán v·ªÅ ƒë·ªô ƒë·∫≠m ƒë·∫∑c c·ªßa Template hay RoI 
(g·ªçi l√† Intensity).
- Nh∆∞ tr√™n h√¨nh 3.27, ƒë·ªëi v·ªõi c√°c con h√†ng b·ªã bao quanh b·ªüi c√°c con 
h√†ng kh√°c, th√¨ v√πng pixel b·∫±ng 0 s·∫Ω nhi·ªÅu h∆°n so v·ªõi Template
- Ti·∫øp theo ta th·ª±c hi·ªán t√≠nh to√°n m·ª©c ƒë·ªô kh·∫£ thi c√≥ th·ªÉ g·∫Øp ƒë∆∞·ª£c theo 
c√¥ng th·ª©c:
ùëÉ =
œÉ(ùëá)
œÉ(ùëÖùëúùêº)
‚àó 100
ÔÇß Trong ƒë√≥:
ÔÇ∑ P l√† gi√° tr·ªã m·ª©c ƒë·ªô kh·∫£ thi c√≥ th·ªÉ g·∫Øp ƒë∆∞·ª£c
ÔÇ∑ ùúé(ùëá) l√† Intensity c·ªßa Template
ÔÇ∑ ùúé(ùëÖùëúùêº) l√† Intensity c·ªßa RoI
- B·∫±ng c√°ch n√†y, khi RoI c√≥ ch·ª©a c√°c v·∫≠t h√†ng k·∫ø b√™n, th√¨ s·∫Ω c√≥ c√°c pixel 
thu·ªôc v·∫≠t h√†ng ƒë√≥ n·∫±m trong RoI, ƒëi·ªÅu n√†y s·∫Ω d·∫´n ƒë·∫øn Intensity c·ªßa 
RoI s·∫Ω ƒë∆∞·ª£c tƒÉng l√™n cao h∆°n v√† P s·∫Ω gi·∫£m xu·ªëng.
- Sau ƒë√≥ ta th·ª±c hi·ªán s·∫Øp x·∫øp l·∫°i theo m·ª©c ƒë·ªô gi·∫£m d·∫ßn gi√° tr·ªã P t∆∞∆°ng 
·ª©ng v·ªõi m·ª©c ƒë·ªô ∆∞u ti√™n tƒÉng d·∫ßn v√† v·ªõi c√°c con h√†ng c√≥ gi√° tr·ªã P th·∫•p 
h∆°n ng∆∞·ª°ng quy ƒë·ªãnh s·∫Ω ƒë∆∞·ª£c b·ªè qua v√† kh√¥ng g·∫Øp ƒë·ªÉ tr√°nh tr∆∞·ªùng 
h·ª£p v·∫≠t ƒë√≥ b·ªã x√™ d·ªãch ƒëi qu√° nhi·ªÅu b·ªüi nh·ªØng con h√†ng kh√°c l√∫c g·∫Øp
- Tuy nhi√™n, ƒë·ªô ƒëo n√†y n√≥ kh√¥ng ho√†n to√†n th·∫≠t s·ª± t√¨m ƒë∆∞·ª£c v·∫≠t h√†ng 
n√†o ∆∞u ti√™n nh·∫•t, v√¨ c∆° b·∫£n t·ª´ b∆∞·ªõc t√≠nh Intensity th√¨ n√≥ ƒë√£ ph·ª• thu·ªôc 
r·∫•t nhi·ªÅu v√†o ch·∫•t l∆∞·ª£ng c·ªßa ƒë·∫ßu ra thu·∫≠t to√°n x·ª≠ l√≠ ·∫£nh, gi·∫£ s·ª≠ trong 
tr∆∞·ªùng h·ª£p ƒë·∫ßu ra n√≥ b·ªã nhi·ªÖu b·ªüi Background, th√¨ Intensity n√≥ kh√¥ng 
ho√†n to√†n ƒë·∫°i di·ªán ƒë∆∞·ª£c cho ·∫£nh. Ngo√†i ra, v·∫≠t h√†ng trong RoI c≈©ng 
ch∆∞a ho√†n to√†n s·∫Ω gi·ªëng v·ªõi Template v√¨ s·∫Ω b·ªã kh√°c g√≥c ch·ª•p n·∫øu n·∫±m 
xa t√¢m Camera.
- Ch√≠nh v√¨ th·∫ø m√† ƒë·ªô ƒëo n√†y ch·ªâ th·∫≠t s·ª± ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ gi·∫£m b·ªõt kh·∫£
nƒÉng Robot s·∫Ω t√°c ƒë·ªông v√†o nh·ªØng v·∫≠t h√†ng d√≠nh v·ªõi nhau l√†m sai l·ªách 
ƒëi t·ªça ƒë·ªô th·ª±c t·∫ø
 C√°c s·ªë ƒë∆∞·ª£c ƒë√°nh d·∫•u cho t·ª´ng v·∫≠t h√†ng n·∫±m ng·ª≠a thu ƒë∆∞·ª£c ƒë·∫°i di·ªán 
cho th·ª© t·ª± m√† Robot s·∫Ω g·∫Øp, c√≥ th·ªÉ th·∫•y v·∫≠t h√†ng s·ªë 7, 8, 9, 10 l√† c√°c 
v·∫≠t h√†ng ƒë∆∞·ª£c g·∫Øp sau c√πng v√¨ c√≥ hi·ªán t∆∞·ª£ng d√≠nh v·ªõi nhau.
- ƒê·ªëi v·ªõi ƒë·ªô ƒëo n√†y, ta ch·ªâ th·∫≠t s·ª± quan t√¢m ƒë·∫øn c√°c v·∫≠t h√†ng d√≠nh v√†o 
nhau, c√≤n nh·ªØng v·∫≠t h√†ng r·ªùi r·∫°c ch√∫ng ta s·∫Ω kh√¥ng quan t√¢m vi·ªác n√≥ 
c√≥ b·ªã x√™ d·ªãch trong qu√° tr√¨nh g·∫Øp hay kh√¥ng.


////////////////Pseudo code lay goc toi uu
- H·∫°n ch·∫ø c·ªßa thu·∫≠t to√°n hi·ªán t·∫°i l√† s·ª± l·∫∑p l·∫°i vi·ªác t√≠nh to√°n m·∫∑c d√π 
c√≥ th·ªÉ g√≥c ƒë·ªÅ xu·∫•t ƒë√£ l√† g√≥c t·ªëi ∆∞u
- Gi·∫£ ƒë·ªãnh r·∫±ng, n·∫øu g√≥c ƒë·ªÅ xu·∫•t c√≥ th·ªÉ ƒë∆∞·ª£c tinh ch·ªânh v√† cho k·∫øt 
qu·∫£ t·ªët h∆°n, t·ª©c l√† gi√° tr·ªã t∆∞∆°ng ƒë·ªìng gi·ªØa Template v√† RoI s·∫Ω cao 
h∆°n. ƒêi·ªÅu ƒë√≥ ƒë·ªìng nghƒ©a v·ªõi vi·ªác, n·∫øu ch√∫ng ta ki·ªÉm tra xem li·ªáu 
v·ªõi g√≥c tinh ch·ªânh ƒë·∫ßu ti√™n gi√° tr·ªã t∆∞∆°ng ƒë·ªìng (Similarity score) 
tr·∫£ v·ªÅ c√≥ cao h∆°n so v·ªõi ƒë·ªô t∆∞∆°ng ƒë·ªìng khi t√≠nh v·ªõi g√≥c ƒë·ªÅ xu·∫•t t·ª´
Segmentation, th√¨ ta s·∫Ω cho ph√©p vi·ªác tinh ch·ªânh ti·∫øp t·ª•c x·∫£y ra.
- Tuy nhi√™n, n·∫øu khi ƒë·∫øn 1 ƒë·ªô tinh ch·ªânh n√†o ƒë√≥, gi√° tr·ªã similarity tr·∫£
v·ªÅ th·∫•p h∆°n so v·ªõi gi√° tr·ªã similarity c·ªßa g√≥c tinh ch·ªânh tr∆∞·ªõc ƒë√≥. 
Vi·ªác tinh ch·ªânh s·∫Ω b·ªã d·ª´ng ngay l·∫≠p t·ª©c.
- ƒê·ªÉ c√¢n b·∫±ng vi·ªác sai l·ªách theo chi·ªÅu √¢m ho·∫∑c d∆∞∆°ng, ta s·∫Ω th·ª±c 
hi·ªán t√≠nh to√°n c√πng l√∫c ƒë·ªô tinh ch·ªânh theo chi·ªÅu d∆∞∆°ng v√† ƒë·ªô tinh 
ch·ªânh theo chi·ªÅu √¢m. V√≤ng l·∫∑p s·∫Ω k·∫øt th√∫c khi c·∫£ 2 b√™n tinh ch·ªânh 
√¢m v√† d∆∞∆°ng ƒë·ªÅu kh√¥ng th·ªÉ tinh ch·ªânh ƒë∆∞·ª£c g√≥c ƒë·ªÅ xu·∫•t sang g√≥c 
t·ªëi ∆∞u ƒë∆∞·ª£c n·ªØa

- Sau ƒë√≥ ta s·∫Ω th·ª±c hi·ªán so s√°nh c√°c k·∫øt qu·∫£ cu·ªëi c√πng c·ªßa c·∫£ 2 b√™n 
so v·ªõi similarity score c·ªßa g√≥c ƒë·ªÅ xu·∫•t t·ª´ Segmentation ƒë·ªÉ ch·ªçn 
ra g√≥c t·ªëi ∆∞u nh·∫•t cho v·∫≠t th·ªÉ.

Input
    source image, TemplateImage
    Box, angle
Output
    Point( with maximal similarity score)

Function matchTemplate(sourceImage, templateImage, box, angle)
    SET x,y,wR,hR <- box
    CAll rotateTemplate(TemplateImage, angle) RETURN rotatedTemplate, mask, wT, hT
    SET RoI <- ùë†ùëúùë¢ùëüùëêùëíùêºùëöùëéùëîùëí ùë¶ ‚à∂ ùë¶ + ‚ÑéùëÖ, ùë• ‚à∂ ùë• + ùë§ùëÖ 
    CAùë≥ùë≥ ùëùùëéùëëùëëùëñùëõùëîùëÖùëúùêº(ùëÖùëúùêº, ùëüùëúùë°ùëéùë°ùëíùëëùëáùëíùëöùëùùëôùëéùë°ùëí) ùëπùë¨ùëªùëºùëπùëµ ùëùùëéùëëùëëùëíùëëùëÖùëúùêº
    ùë∫ùë¨ùëª ùëêùëúùëüùëüùëíùëôùëéùë°ùëñùëúùëõùëÄùëéùëù ‚Üê ùëéùëüùëüùëéùë¶ [ùëùùëéùëëùëëùëíùëëùëÖùëúùêº. ùëÜ‚Ñéùëéùëùùëí ]

    ùë≠ùë∂ùëπ ùëñ ùë≠ùëπùë∂ùë¥ ùëùùëéùëëùëëùëíùëëùëÖùëúùêº. ùêªùëíùëñùëî‚Ñéùë°
        ùë≠ùë∂ùëπ ùëó ùë≠ùëπùë∂ùë¥ ùëùùëéùëëùëëùëíùëëùëÖùëúùêº. ùëäùëñùëëùë°‚Ñé
            ùë∫ùë¨ùëª ùë†ùë¢ùëèùëÖùëúùêº ‚Üê ùëùùëéùëëùëëùëíùëëùëÖùëúùêº[ ùëñ ‚à∂ ùëñ + ‚Ñéùëá,ùëó ‚à∂ ùëó + ùë§ùëá ]
            ùë™ùë®ùë≥ùë≥ ùëêùëúùëöùëùùë¢ùë°ùëíùëÜùëñùëöùëñùëôùëéùëüùëñùë°ùë¶(ùë†ùë¢ùëèùëÖùëúùêº, ùëüùëúùë°ùëéùë°ùëíùëëùëáùëíùëöùëùùëôùëéùë°ùëí) ùëπùë¨ùëªùëºùëπùëµ ùëêùëúùëüùëüùëíùëôùëéùë°ùëñùëúùëõùëÜùëêùëúùëüùëí
            ùë®ùë´ùë´ ùëêùëúùëüùëüùëíùëôùëéùë°ùëñùëúùëõùëÜùëêùëúùëüùëí ‚Üí ùëêùëúùëüùëüùëíùëôùëéùë°ùëñùëúùëõùëÄùëéùëù ùëñ,ùëó 
        ùë¨ùëµùë´ùë≠ùë∂ùëπ
    ùë¨ùëµùë´ùë≠ùë∂R
    Sùë¨ùëª ùëÉùëúùëñùëõùë° ‚Üê ùëöùëéùë•ùëôùëúùëê(ùëêùëúùëüùëüùëíùëôùëéùë°ùëñùëúùëõùëÄùëéùëù)

ùëπùë¨ùëªùëºùëπùëµ ùëÉùëúùëñùëõt



Input:
    o ùë†ùëúùë¢ùëüùëêùëíùêºùëöùëéùëîùëí, ùë°ùëíùëöùëùùëôùëéùë°ùëíùêºùëöùëéùëîùëí
    o ùë†ùëíùëîùëöùëíùëõùë°ùëéùë°ùëñùëúùëõùëÖùëíùë†ùë¢ùëôùë°
    o ùëöùëñùëõùëÄùëúùëëùëñùëìùë¶, ùëöùëéùë•ùëÄùëúùëëùëñùëìùë¶
Output:
    o ùëîùëúùëúùëëùëÉùëúùëñùëõùë°ÔøΩ

SET ùëöùëñùëõùë¢ùë†ùëÄùëúùëëùëñùëìùë¶ùê¥ùëõùëîùëôùëí ‚Üê ùëéùëüùëüùëéùë¶ùëÖùëéùëõùëîùëí[ ‚àí1, ùëöùëñùëõùëÄùëúùëëùëñùëìùë¶, ‚àí1] 
SET ùëùùëôùë¢ùë†ùëÄùëúùëëùëñùëìùë¶ùê¥ùëõùëîùëôùëí ‚Üê ùëéùëüùëüùëéùë¶ùëÖùëéùëõùëîùëí[ 1, ùëöùëéùë•ùëÄùëúùëëùëñùëìùë¶, 1 ]

SET ùëîùëúùëúùëëùëÉùëúùëñùëõùë°ùë† ‚Üê ùëíùëöùëùùë°ùë¶ùêøùëñùë†ùë°[]

CONVERT ùë†ùëúùë¢ùëüùëêùëíùêºùëöùëéùëîùëí ‚Üí ùëîùëüùëéùë¶ùëÜùëêùëéùëôùëí ùë®ùë∫ ùëñùëöùëî
CONVERT ùë°ùëíùëöùëùùëôùëéùë°ùëíùêºùëöùëéùëîùëí ‚Üí ùëîùëüùëéùë¶ùëÜùëêùëéùëôùëí ùë®ùë∫ ùë°ùëíùëöùëùÔøΩ

FOR ùëèùëúùë•, ùëéùëõùëîùëôùëí ùë≠ùëπùë∂ùë¥ ùë†ùëíùëîùëöùëíùëõùë°ùëéùë°ùëñùëúùëõùëÖùëíùë†ùë¢ùëôs
    ARRAY ùëöùëñùëõùë¢ùë†ùëÜùë¢ùëèùê¥ùëõùëîùëôùëíùë† ‚Üê ùëéùëõùëîùëôùëí + ùëöùëñùëõùë¢ùë†ùëÄùëúùëëùëñùëìùë¶ùê¥ùëõùëîùëôùëí
    ARRAY ùëùùëôùë¢ùë†ùëÜùë¢ùëèùê¥ùëõùëîùëôùëíùë† ‚Üê ùëéùëõùëîùëôùëí + ùëùùëôùë¢ùë†ùëÄùëúùëëùëñùëìùë¶ùê¥ùëõùëîùëôùëí
    SET ùëöùëñùëõùë¢ùë†ùëÉùëúùëñùëõùë°ùëíùëü, ùëùùëôùë¢ùë†ùëÉùëúùëñùëõùë°ùëíùëü ‚Üê 0, 0
    SET ùëöùëñùëõùë¢ùë†ùê∂‚Ñéùëíùëêùëò, ùëùùëôùë¢ùë†ùê∂‚Ñéùëíùëêùëò ‚Üê ùë≠ùë®ùë≥ùë∫ùë¨,ùë≠ùë®ùë≥ùë∫ùë¨
    SET ùë†ùë¢ùëèùëÄùëñùëõùë¢ùë†ùëÉùëúùëñùëõùë°ùë†, ùë†ùë¢ùëèùëÉùëôùë¢ùë†ùëÉùëúùëñùëõùë°ùë† ‚Üê ùëíùëöùëùùë°ùë¶ùêøùëñùë†ùë°[], ùëíùëöùëùùë°ùë¶ùêøùëñùë†ùë°[]
    SET ùë°‚Ñéùëí ùëôùëéùë†ùë° ùëíùëôùëíùëöùëíùëõùë° ùëúùëì ùë†ùë¢ùëèùëÄùëñùëõùë¢ùë†ùëÉùëúùëñùëõùë°ùë† ùë®ùë∫ ùëöùëñùëõùë¢ùë†ùêøùëÉ
    SET ùë°‚Ñéùëí ùëôùëéùë†ùë° ùëíùëôùëíùëöùëíùëõùë° ùëúùëì ùë†ùë¢ùëèùëÉùëôùë¢ùë†ùëÉùëúùëñùëõùë°ùë† ùë®ùë∫ ùëùùëôùë¢ùë†ùêøùëÉ
    ùë™ùë®ùë≥ùë≥ ùëöùëéùë°ùëê‚Ñéùëáùëíùëöùëùùëôùëéùë°ùëí(ùëñùëöùëî,ùë°ùëíùëöùëùùëô, ùëèùëúùë•, ùëéùëõùëîùëôùëí) ùëπùë¨ùëªùëºùëπùëµ ùëëùëíùëìùëéùë¢ùëôùë°ùëÉùëúùëñùëõùë° ùë®ùë∫ P

ùëæùëØùë∞ùë≥ùë¨ ùëöùëñùëõùë¢ùë†ùê∂‚Ñéùëíùëêùëò = ùë≠ùë®ùë≥ùë∫ùë¨ ùë®ùëµùë´ ùëùùëôùë¢ùë†ùê∂‚Ñéùëíùëêùëò = ùë≠ùë®ùë≥ùë∫ùë¨
    CALL
    ùëöùëéùë°ùëê‚Ñéùëáùëíùëöùëùùëôùëéùë°ùëí(ùëñùëöùëî,ùë°ùëíùëöùëùùëô, ùëèùëúùë•, ùëöùëñùëõùë¢ùë†ùëÜùë¢ùëèùê¥ùëõùëîùëôùëíùë† ùëöùëñùëõùë¢ùë†ùëÉùëúùëñùëõùë°ùëíùëü )ùëπùë¨ùëªùëºùëπùëµùëöùëñùëõùë¢ùë†ùëÉùëúùëñùëõùë°
    CALL ùëöùëéùë°ùëê‚Ñéùëáùëíùëöùëùùëôùëéùë°ùëí(ùëñùëöùëî,ùë°ùëíùëöùëùùëô, ùëèùëúùë•, ùëùùëôùë¢ùë†ùëÜùë¢ùëèùê¥ùëõùëîùëôùëíùë† ùëùùëôùë¢ùë†ùëÉùëúùëñùëõùë°ùëíùëü ) ùëπùë¨ùëªùëºùëπùëµ ùëùùëôùë¢ùë†ùëÉùëúùëñùëõùë°
    ùë∞ùë≠ ùëöùëñùëõùë¢ùë†ùëÉùëúùëñùëõùë°ùëíùëü = 0
    ùëöùëñùëõùë¢ùë†ùê∂‚Ñéùëíùëêùëò ‚Üê ùë©ùë∂ùë∂ùë≥(ùë†ùëêùëúùëüùëíùëúùëì(ùëöùëñùëõùë¢ùë†ùëÉùëúùëñùëõùë°) < ùë†ùëêùëúùëüùëíùëúùëì(ùëÉ))
    ùë¨ùë≥ùë∫ùë¨
    ùëöùëñùëõùë¢ùë†ùê∂‚Ñéùëíùëêùëò ‚Üê ùë©ùë∂ùë∂ùë≥(ùë†ùëêùëúùëüùëíùëúùëì(ùëöùëñùëõùë¢ùë†ùëÉùëúùëñùëõùë°) < ùë†ùëêùëúùëüùëíùëúùëì(ùëöùëñùëõùë¢ùë†ùêøùëÉ))
    ùë¨ùëµùë´ùë∞F
    ùë∞ùë≠ ùëµùë∂ùëª ùëöùëñùëõùë¢ùë†ùê∂‚Ñéùëíùëêùëò
    ùë®ùë´ùë´ ùëöùëñùëõùë¢ùë†ùëÉùëúùëñùëõùë° ‚Üí ùë†ùë¢ùëèùëÄùëñùëõùë¢ùë†ùëÉùëúùëñùëõùë°ùë†
    ùëöùëñùëõùë¢ùë†ùëÉùëúùëñùëõùë°ùëíùëü ‚Üê ùëöùëñùëõùë¢ùë†ùëÉùëúùëñùëõùë°ùëíùëü + 1
    ENDIF
    ùë∞ùë≠ ùëùùëôùë¢ùë†ùëÉùëúùëñùëõùë°ùëíùëü = 0
    ùëùùëôùë¢ùë†ùê∂‚Ñéùëíùëêùëò ‚Üê ùë©ùë∂ùë∂ùë≥(ùë†ùëêùëúùëüùëíùëúùëì(ùëùùëôùë¢ùë†ùëÉùëúùëñùëõùë°) < ùë†ùëêùëúùëüùëíùëúùëì(ùëÉ))
    ùë¨ùë≥ùë∫ùë¨
    ùëùùëôùë¢ùë†ùê∂‚Ñéùëíùëêùëò ‚Üê ùë©ùë∂ùë∂ùë≥(ùë†ùëêùëúùëüùëíùëúùëì(ùëùùëôùë¢ùë†ùëÉùëúùëñùëõùë°) < ùë†ùëêùëúùëüùëíùëúùëì(ùëùùëôùë¢ùë†ùêøùëÉ))
    ùë¨ùëµùë´ùë∞ùë≠
    ùë∞ùë≠ ùëµùë∂ùëª ùëùùëôùë¢ùë†ùê∂‚Ñéùëíùëêùëò
    ùë®ùë´ùë´ ùëùùëôùë¢ùë†ùëÉùëúùëñùëõùë° ‚Üí ùë†ùë¢ùëèùëÉùëôùë¢ùë†ùëÉùëúùëñùëõùë°ùë†
    ùëùùëôùë¢ùë†ùëÉùëúùëñùëõùë°ùëíùëü ‚Üê ùëùùëôùë¢ùë†ùëÉùëúùëñùëõùë°ùëíùëü + 1
    ùë¨ùëµùë´ùë∞ÔøΩ
    Eùëµùë´ùëæùëØùë∞ùë≥ùë¨
    ùëèùëíùë†ùë°ùëÉùëúùëñùëõùë° ‚Üê ùëöùëéùë•ùëùùëúùëñùëõùë°
    (ùë†ùëêùëúùëüùëíùëúùëì ùëÉ, ùëöùëñùëõùë¢ùë†ùêøùëÉ, ùëùùëôùë¢ùë†ùêøùëÉ )
    ùë®ùë´ùë´ ùëèùëíùë†ùë°ùëÉùëúùëñùëõùë° ‚Üí ùëîùëúùëúùëëùëÉùëúùëñùëõùë°ùë†
ENDFOR
//////////////////////////////////////////////////////////////////////////

using System;
using System.Collections.Generic;
using System.ComponentModel;
using System.Data;
using System.Drawing;
using System.IO;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using System.Windows.Forms;
using System.Diagnostics;
using System.Globalization;
using System.Drawing.Imaging;
using System.Net.Http;
using System.Net.Sockets;
using System.Net;
using System.Threading;

using NeptuneC_Interface;
using System.Security.AccessControl;
using System.Drawing.Drawing2D;
using static System.Windows.Forms.VisualStyles.VisualStyleElement;

namespace WinFormCVU
{
    public partial class ComputerVisionUI : Form
    {
        private NeptuneC_Interface.NeptuneCUnplugCallback DeviceUnplugCallbackInst;
        private NeptuneC_Interface.NeptuneCFrameCallback FrameCallbackInst;

        string targetImagePath;
        string templatePath;

        Stopwatch stopwatch = new Stopwatch();

        string defaultDirectory = $"{Application.StartupPath}\\";
        string stream_folder = "Stream_camera";
        string output_folder = "Output";
        string template_folder = "Template";

        // these are all variables used in RoI selection 
        private bool isSelecting = false;
        private Rectangle rect;
        private Point startPoint;
        private int cornerSize = 5;
        private Rectangle selectedRect;
        private bool isReshape = false;

        // this is a variable used in zoom in or zoom out
        private bool isResizing = false;

        private TcpListener serverSocket;
        private Thread serverThread;
        private bool isRunning;
        private TcpListener serverSocket2;
        private Thread serverThread2;
        private bool isRunning2;
        private TcpListener serverSocket3;
        private Thread serverThread3;
        private bool isRunning3;
        private List<float[]> sharedData = new List<float[]>();
        private TaskCompletionSource<bool> matchTemplateCompletionSource;

        // this is a variable used in camera connection
        public IntPtr m_pCameraHandle = IntPtr.Zero;
        private NEPTUNE_FEATURE[] m_arrFeatureInfo;
        private CheckBox[] m_arrCheckBox;
        private ENeptuneFeature[] m_arrMapping = new ENeptuneFeature[]
        {
            ENeptuneFeature.NEPTUNE_FEATURE_GAMMA,
            ENeptuneFeature.NEPTUNE_FEATURE_GAIN,
            ENeptuneFeature.NEPTUNE_FEATURE_RGAIN,
            ENeptuneFeature.NEPTUNE_FEATURE_GGAIN,
            ENeptuneFeature.NEPTUNE_FEATURE_BGAIN,
            ENeptuneFeature.NEPTUNE_FEATURE_AUTOGAIN_MIN,
            ENeptuneFeature.NEPTUNE_FEATURE_AUTOGAIN_MAX,
            ENeptuneFeature.NEPTUNE_FEATURE_SHUTTER,
            ENeptuneFeature.NEPTUNE_FEATURE_AUTOSHUTTER_MIN,
            ENeptuneFeature.NEPTUNE_FEATURE_AUTOSHUTTER_MAX,
            ENeptuneFeature.NEPTUNE_FEATURE_AUTOEXPOSURE,
            ENeptuneFeature.NEPTUNE_FEATURE_BLACKLEVEL,
            ENeptuneFeature.NEPTUNE_FEATURE_CONTRAST,
            ENeptuneFeature.NEPTUNE_FEATURE_HUE,
            ENeptuneFeature.NEPTUNE_FEATURE_SATURATION,
            ENeptuneFeature.NEPTUNE_FEATURE_SHARPNESS,
            ENeptuneFeature.NEPTUNE_FEATURE_TRIGNOISEFILTER,
            ENeptuneFeature.NEPTUNE_FEATURE_BRIGHTLEVELIRIS,
            ENeptuneFeature.NEPTUNE_FEATURE_SNOWNOISEREMOVE,
            ENeptuneFeature.NEPTUNE_FEATURE_OPTFILTER,
            ENeptuneFeature.NEPTUNE_FEATURE_PAN,
            ENeptuneFeature.NEPTUNE_FEATURE_TILT,
            ENeptuneFeature.NEPTUNE_FEATURE_LCD_BLUE_GAIN,
            ENeptuneFeature.NEPTUNE_FEATURE_LCD_RED_GAIN,
            ENeptuneFeature.NEPTUNE_FEATURE_WHITEBALANCE
        };

        public ComputerVisionUI()
        {
            InitializeComponent();
        }

        private void ComputerVisionUI_Load(object sender, EventArgs e)
        {
            Template_box.AllowDrop = true;
            Image_box.AllowDrop = true;
            Similarity_score.Text = "0.95";
            Overlap.Text = "0.4";
            Min_modify.Text = "-20";
            Max_modify.Text = "20";
            Scale_ratio.Text = "20";
            Method_similarity.Text = "cv2.TM_CCORR_NORMED";
            conf_score.Text = "0.75";
            img_size.Text = "750";
            serverIP.Text = "192.168.176.1";

            Cam_capture.Enabled = false;
            Match_template.Enabled = false;
            add_template.Enabled = false;
            down_scale.Enabled = false;
            up_scale.Enabled = false;
            Elasped_time.ReadOnly = true;
            CVU_status.ReadOnly = true;
            serverStatus.ReadOnly = true;

            InitCameraList();
        }

        private void ComputerVisionUI_FormClosing(object sender, FormClosingEventArgs e)
        {
            CloseCameraHandle();
            NeptuneC.ntcUninit();

            DisconnectServer();
        }

        public class ItemData
        {
            public String m_strLabel = "";
            public Int32 m_nValue = 0;
            public NEPTUNE_CAM_INFO m_stCameraInfo;

            public ItemData(String strLabel, Int32 nValue)
            {
                m_strLabel = strLabel;
                m_nValue = nValue;
            }

            public ItemData(NEPTUNE_CAM_INFO stCameraInfo)
            {
                m_stCameraInfo = stCameraInfo;
                m_strLabel = m_stCameraInfo.strVendor + ": [" + m_stCameraInfo.strSerial + "] " + m_stCameraInfo.strModel;
            }

            public override String ToString()
            {
                return m_strLabel;
            }
        };

        private void CloseCameraHandle()
        {
            if (m_pCameraHandle != IntPtr.Zero)
            {
                NeptuneC.ntcClose(m_pCameraHandle);
                m_pCameraHandle = IntPtr.Zero;
            }
        }

        private void My_Refresh_Click(object sender, EventArgs e)
        {
            InitCameraList();
        }

        private void InitCameraList()
        {
            NeptuneC.ntcInit();
            ItemData CurSelItem = (ItemData)m_cbCameraList.SelectedItem;

            m_cbCameraList.Items.Clear();

            UInt32 uiCount = 0;
            if (NeptuneC.ntcGetCameraCount(ref uiCount) == ENeptuneError.NEPTUNE_ERR_Success)
            {
                if (uiCount > 0)
                {
                    NEPTUNE_CAM_INFO[] pCameraInfo = new NEPTUNE_CAM_INFO[uiCount];
                    ENeptuneError emErr = NeptuneC.ntcGetCameraInfo(pCameraInfo, uiCount);
                    if (emErr == ENeptuneError.NEPTUNE_ERR_Success)
                    {
                        for (UInt32 i = 0; i < uiCount; i++)
                        {
                            Int32 nItem = m_cbCameraList.Items.Add(new ItemData(pCameraInfo[i]));
                            if (CurSelItem != null)
                            {
                                if (((ItemData)m_cbCameraList.Items[nItem]).m_stCameraInfo.strSerial.Equals(CurSelItem.m_stCameraInfo.strSerial) == true)
                                {
                                    m_cbCameraList.SelectedIndex = nItem;
                                }
                            }
                        }
                    }
                }
            }

            if (CurSelItem != null && m_cbCameraList.SelectedIndex == -1)
            {
                CloseCameraHandle();
            }
        }

        private void m_cbCameraList_SelectedIndexChanged(object sender, EventArgs e)
        {
            CloseCameraHandle();

            if (m_cbCameraList.SelectedIndex != -1)
            {
                ENeptuneError emErr = NeptuneC.ntcOpen(((ItemData)m_cbCameraList.SelectedItem).m_stCameraInfo.strCamID, ref m_pCameraHandle, ENeptuneDevAccess.NEPTUNE_DEV_ACCESS_EXCLUSIVE);
                if (emErr == ENeptuneError.NEPTUNE_ERR_Success)
                {
                    //NeptuneC.ntcSetDisplay(m_pCameraHandle, Image_box.Handle);
                    NeptuneC.ntcSetUnplugCallback(m_pCameraHandle, DeviceUnplugCallbackInst, this.Handle);
                    NeptuneC.ntcSetFrameCallback(m_pCameraHandle, FrameCallbackInst, this.Handle);
                }

                _ = NeptuneC.ntcSetAcquisition(m_pCameraHandle, ENeptuneBoolean.NEPTUNE_BOOL_TRUE);

                Int32 nEffectFlags = 0;
                if (NeptuneC.ntcGetEffect(m_pCameraHandle, ref nEffectFlags) == ENeptuneError.NEPTUNE_ERR_Success)
                {
                    nEffectFlags |= (Int32)ENeptuneEffect.NEPTUNE_EFFECT_FLIP;

                    _ = NeptuneC.ntcSetEffect(m_pCameraHandle, nEffectFlags);
                }

                Cam_capture.Enabled = true;
                NEPTUNE_IMAGE_SIZE stImageSize = new NEPTUNE_IMAGE_SIZE();
                stImageSize.nStartX = 0;
                stImageSize.nStartY = 0;
                stImageSize.nSizeX = 3220;
                stImageSize.nSizeY = 3448;
                _ = NeptuneC.ntcSetImageSize(m_pCameraHandle, stImageSize);

                ENeptuneError emErr1 = NeptuneC.ntcSetPixelFormat(m_pCameraHandle, (ENeptunePixelFormat)108);

                NEPTUNE_FEATURE stInfo = new NEPTUNE_FEATURE();
                m_arrFeatureInfo = new NEPTUNE_FEATURE[m_arrMapping.Length];
                stInfo = m_arrFeatureInfo[m_arrMapping.Length - 1];
                stInfo.AutoMode = ENeptuneAutoMode.NEPTUNE_AUTO_OFF;
                NeptuneC.ntcSetFeature(m_pCameraHandle, m_arrMapping[m_arrMapping.Length - 1], stInfo);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Gamma", (int)0);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Gain", (int)0);

                // Red
                stInfo = m_arrFeatureInfo[2];
                stInfo.Value = 469;
                NeptuneC.ntcSetFeature(m_pCameraHandle, m_arrMapping[2], stInfo);

                // Green
                stInfo = m_arrFeatureInfo[3];
                stInfo.Value = 363;
                NeptuneC.ntcSetFeature(m_pCameraHandle, m_arrMapping[3], stInfo);

                // Blue
                stInfo = m_arrFeatureInfo[4];
                stInfo.Value = 874;
                NeptuneC.ntcSetFeature(m_pCameraHandle, m_arrMapping[4], stInfo);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "BlackLevel", (int)108);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Contrast", (int)435);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Hue", (int)255);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Saturation", (int)255);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Sharpness", (int)0);

                //m_arrCheckBox = new CheckBox[m_arrMapping.Length - 1];
                //stInfo = m_arrFeatureInfo[7];
                //stInfo.bOnOff = ENeptuneBoolean.NEPTUNE_BOOL_TRUE;
                //stInfo.Value = 47900;
                //NeptuneC.ntcSetFeature(m_pCameraHandle, m_arrMapping[7], stInfo);

                //stInfo = m_arrFeatureInfo[7];
                //stInfo.Value = 47900;
                //ENeptuneError emErr1 = NeptuneC.ntcSetFeature(m_pCameraHandle, m_arrMapping[7], stInfo);
                //MessageBox.Show($"{emErr1}");
                //NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Shutter", (int)47900);
            }
        }

        private async Task captureFrame()
        {
            if (m_pCameraHandle != IntPtr.Zero)
            {
                if (!Directory.Exists(Path.Combine(defaultDirectory, stream_folder)))
                {
                    Directory.CreateDirectory(Path.Combine(defaultDirectory, stream_folder));
                }

                else
                {
                    DirectoryInfo directory = new DirectoryInfo(Path.Combine(defaultDirectory, stream_folder));

                    // Delete all files within the folder
                    foreach (FileInfo file in directory.GetFiles())
                    {
                        file.Delete();
                    }
                }
                string cameraPath = Path.Combine(defaultDirectory, Path.Combine(stream_folder, "input_image.jpg"));
                _ = await Task.Run(() => NeptuneC.ntcSaveImage(m_pCameraHandle, cameraPath, 50));
            }
        }

        private void Template_box_DragDrop(object sender, DragEventArgs e)
        {
            string[] files = (string[])e.Data.GetData(DataFormats.FileDrop);
            if (files.Length > 0)
            {
                string imagePath = files[0];
                Template_box.ImageLocation = imagePath;
                string fullTemplatePath = imagePath;
                templatePath = get_relativePath(fullTemplatePath, defaultDirectory);

                string templatePathCopy = Path.Combine(Path.GetDirectoryName(templatePath), "copy_" + Path.GetFileName(templatePath));
                File.Copy(templatePath, templatePathCopy, true);

                ShowImage(templatePathCopy, Template_box);

                File.Delete(templatePathCopy);
            }
        }

        private void Template_box_DragEnter(object sender, DragEventArgs e)
        {
            if (e.Data.GetDataPresent(DataFormats.FileDrop))
            {
                e.Effect = DragDropEffects.Copy;
            }
        }

        private void Image_box_DragEnter(object sender, DragEventArgs e)
        {
            if (e.Data.GetDataPresent(DataFormats.FileDrop))
            {
                string[] file = (string[])e.Data.GetData(DataFormats.FileDrop);
                if (file.Length == 1 && (Path.GetExtension(file[0]).ToLower() == ".png" || Path.GetExtension(file[0]).ToLower() == ".jpg" || Path.GetExtension(file[0]).ToLower() == ".jpeg" || Path.GetExtension(file[0]).ToLower() == ".bmp" || Path.GetExtension(file[0]).ToLower() == ".gif"))
                {
                    e.Effect = DragDropEffects.Copy;
                }
            }
        }

        private void Image_box_DragDrop(object sender, DragEventArgs e)
        {
            string[] file = (string[])e.Data.GetData(DataFormats.FileDrop);
            if (file.Length > 0)
            {
                string imagePath = file[0];
                string fullTargetImagePath = imagePath;
                targetImagePath = get_relativePath(fullTargetImagePath, defaultDirectory);

                string targetImagePathCopy = Path.Combine(Path.GetDirectoryName(targetImagePath), "copy_" + Path.GetFileName(targetImagePath));
                File.Copy(targetImagePath, targetImagePathCopy, true);

                ShowImage(targetImagePathCopy, Image_box);

                File.Delete(targetImagePathCopy);
            }
        }

        private void LoadCsvToDataGridView(string csvFilePath, DataGridView dataGridView)
        {
            if (!File.Exists(Path.Combine(defaultDirectory, csvFilePath)))
            {
                throw new ArgumentException("The specified csv file does not exist.", nameof(csvFilePath));
            }

            // Create a new DataTable
            DataTable dataTable = new DataTable();

            // Read the CSV file line by line
            string[] csvLines = File.ReadAllLines(Path.Combine(defaultDirectory, csvFilePath));

            // Add the column headers to the DataTable
            string[] headers = csvLines[0].Split(',');
            foreach (string header in headers)
            {
                dataTable.Columns.Add(header);
            }

            // Add the data rows to the DataTable
            for (int i = 1; i < csvLines.Length; i++)
            {
                string[] fields = csvLines[i].Split(',');
                dataTable.Rows.Add(fields);
            }

            // Bind the DataTable to the DataGridView
            dataGridView.DataSource = dataTable;
        }

        private void ShowImage(string imagePath, PictureBox pictureBox)
        {
            if (!File.Exists(Path.Combine(defaultDirectory, imagePath)))
            {
                throw new ArgumentException("The specified image file does not exist.", nameof(imagePath));
            }

            if (pictureBox.Image != null)
            {
                pictureBox.Image.Dispose();
                pictureBox.Image = null;
            }

            using (Image image = Image.FromFile(Path.Combine(defaultDirectory, imagePath)))
            {
                if (image.PropertyIdList.Contains(0x0112)) // Check if the image has orientation metadata
                {
                    int orientation = (int)image.GetPropertyItem(0x0112).Value[0];
                    switch (orientation)
                    {
                        case 2: // Flip horizontally
                            image.RotateFlip(RotateFlipType.RotateNoneFlipX);
                            break;
                        case 3: // Rotate 180 degrees
                            image.RotateFlip(RotateFlipType.Rotate180FlipNone);
                            break;
                        case 4: // Flip vertically
                            image.RotateFlip(RotateFlipType.RotateNoneFlipY);
                            break;
                        case 5: // Rotate 90 degrees clockwise and flip horizontally
                            image.RotateFlip(RotateFlipType.Rotate90FlipX);
                            break;
                        case 6: // Rotate 90 degrees clockwise
                            image.RotateFlip(RotateFlipType.Rotate90FlipNone);
                            break;
                        case 7: // Rotate 90 degrees clockwise and flip vertically
                            image.RotateFlip(RotateFlipType.Rotate90FlipY);
                            break;
                        case 8: // Rotate 270 degrees clockwise
                            image.RotateFlip(RotateFlipType.Rotate270FlipNone);
                            break;
                        default:
                            break;
                    }
                }

                Invoke(new MethodInvoker(() =>
                {
                    pictureBox.Image = new Bitmap(image);
                    percentageScale(pictureBox);

                    if (pictureBox.Name == "Image_box")
                    {
                        down_scale.Enabled = true;
                        up_scale.Enabled = true;
                        add_template.Enabled = false;
                    }

                    if ((templatePath != null) && (targetImagePath != null))
                    {
                        Match_template.Enabled = true;
                    }
                }));
            }
        }


        private void percentageScale(PictureBox pictureBox)
        {
            if (pictureBox.Name != "Image_box")
            {
                return;
            }

            float originalWidth = pictureBox.Image.Width;
            float originalHeight = pictureBox.Image.Height;
            float aspectRatio = originalWidth / originalHeight;

            float resizedWidth = pictureBox.Width;
            float resizedHeight = pictureBox.Height;

            if (resizedWidth / resizedHeight > aspectRatio)
            {
                resizedWidth = resizedHeight * aspectRatio;
            }
            else
            {
                resizedHeight = resizedWidth / aspectRatio;
            }

            float widthPercentage = resizedWidth / originalWidth * 100;
            float heightPercentage = resizedHeight / originalHeight * 100;

            percentage_scale.Text = $"{(int)Math.Round(widthPercentage)}%";
        }

        private void ResizePictureBox(float percentage)
        {
            float originalWidth = Image_box.Image.Width;
            float originalHeight = Image_box.Image.Height;
            float aspectRatio = originalWidth / originalHeight;

            float resizedWidth = originalWidth * (percentage / 100);
            float resizedHeight = resizedWidth / aspectRatio;

            Image_box.Width = (int)resizedWidth;
            Image_box.Height = (int)resizedHeight;
        }

        private void percentage_scale_TextChanged(object sender, EventArgs e)
        {
            if (float.TryParse(percentage_scale.Text.Replace("%", ""), out float percentage))
            {
                ResizePictureBox(percentage);
            }
        }

        private string get_relativePath(string path, string defaultPaht)
        {
            Uri fullUri = new Uri(path);
            Uri defaultUri = new Uri(defaultPaht);
            string relativePath = Uri.UnescapeDataString(defaultUri.MakeRelativeUri(fullUri).ToString());
            return relativePath;
        }

        private void StartServer()
        {   
            IPAddress ipAddress = IPAddress.Parse(serverIP.Text);
            int port = 48951;
            int port2 = 48952;
            int port3 = 48953;

            // Robot - Winforms - Run CVU algorithm
            serverSocket = new TcpListener(ipAddress, port);
            serverSocket.Start();
            isRunning = true;

            // CVU - Winforms - Get positions of detected objects
            serverSocket2 = new TcpListener(ipAddress, port2);
            serverSocket2.Start();
            isRunning2 = true;

            // Robot - Winforms - Send each position when receiving the request
            serverSocket3 = new TcpListener(ipAddress, port3);
            serverSocket3.Start();
            isRunning3 = true;

            // Start a new thread to accept client connections
            serverThread = new Thread(new ThreadStart(AcceptClients));
            serverThread.Start();

            serverThread2 = new Thread(new ThreadStart(AcceptClients2));
            serverThread2.Start();

            serverThread3 = new Thread(new ThreadStart(AcceptClients3));
            serverThread3.Start();
        }

        private void AcceptClients()
        {
            while (isRunning)
            {
                try
                {
                    TcpClient clientSocket = serverSocket.AcceptTcpClient();
                    Thread clientThread = new Thread(new ParameterizedThreadStart(HandleClient));
                    clientThread.Start(clientSocket);
                }

                catch (SocketException)
                {
                    break;
                }
            }
        }

        private void AcceptClients2()
        {
            while (isRunning2)
            {
                try
                {
                    TcpClient clientSocket = serverSocket2.AcceptTcpClient();
                    Thread clientThread = new Thread(new ParameterizedThreadStart(HandleClient2));
                    clientThread.Start(clientSocket);
                }
                catch (SocketException)
                {
                    break;
                }
            }
        }

        private void AcceptClients3()
        {
            while (isRunning3)
            {
                try
                {
                    TcpClient clientSocket = serverSocket3.AcceptTcpClient();
                    Thread clientThread = new Thread(new ParameterizedThreadStart(HandleClient3));
                    clientThread.Start(clientSocket);
                }
                catch (SocketException)
                {
                    break;
                }
            }
        }

        private void HandleClient(object clientObj)
        {
            TcpClient clientSocket = (TcpClient)clientObj;

            try
            {
                NetworkStream networkStream = clientSocket.GetStream();
                byte[] buffer = new byte[clientSocket.ReceiveBufferSize];

                int bytesRead = networkStream.Read(buffer, 0, clientSocket.ReceiveBufferSize);
                sbyte receivedInteger = (sbyte)buffer[0];

                if (receivedInteger == 100)
                {
                    // Create a new TaskCompletionSource and assign it to matchTemplateCompletionSource
                    matchTemplateCompletionSource = new TaskCompletionSource<bool>();

                    // Invoke the conduct_match_template method on the UI thread
                    string response = null;
                    int numberOfObjects = 0;
                    BeginInvoke(new MethodInvoker(async () =>
                    {
                        stopwatch.Reset();
                        stopwatch.Start();
                        Elasped_time.Text = "...";
                        CVU_status.Text = "Running...";

                        await conduct_capture_frame();

                        response = await conduct_match_template();
                        numberOfObjects = int.Parse(response);

                        // After the conduct_match_template method is complete, set the task completion source result
                        matchTemplateCompletionSource.SetResult(true);
                    }));

                    // Wait for the task completion source to complete
                    matchTemplateCompletionSource.Task.Wait();

                    byte byteNumberOfObjects = (byte)numberOfObjects;
                    networkStream.WriteByte(byteNumberOfObjects);
                    networkStream.Flush();

                    stopwatch.Stop();
                    TimeSpan elapsedTime = stopwatch.Elapsed;
                    double roundedElapsedTime = Math.Round(elapsedTime.TotalSeconds, 2);
                    string elapsedTimeString = roundedElapsedTime.ToString();

                    BeginInvoke(new MethodInvoker(() =>
                    {
                        Elasped_time.Text = $"{elapsedTimeString} s";

                        string resultImagePath = Path.Combine(output_folder, "output.jpg");
                        string csvPath = Path.Combine(output_folder, "result.csv");

                        try
                        {
                            ShowImage(resultImagePath, Image_box);
                            LoadCsvToDataGridView(csvPath, dataGridView1);
                            CVU_status.Text = "Completed";
                        }
                        catch (Exception)
                        {
                            CVU_status.Text = "No detection found";
                        }
                    }));
                }
            }

            catch (Exception e)
            {
                MessageBox.Show($"{e}");
            }
            finally
            {
                clientSocket.Close();
            }
        }

        private void HandleClient2(object clientObj)
        {
            TcpClient clientSocket = (TcpClient)clientObj;

            try
            {
                byte[] numBytes = new byte[1];
                NetworkStream networkStream = clientSocket.GetStream();
                int bytesRead = networkStream.Read(numBytes, 0, 1);

                if (bytesRead == 0)
                {
                    return;
                }
                int numArrays = numBytes[0];

                lock (sharedData)
                {
                    sharedData.Clear();

                    for (int i = 0; i < numArrays; i++)
                    {
                        // Receive the array of 4 float values (x, y, z, r)
                        byte[] arrayBytes = new byte[16];
                        bytesRead = clientSocket.GetStream().Read(arrayBytes, 0, 16);
                        if (bytesRead == 0)
                        {
                            return;
                        }

                        float[] receivedArray = new float[4];
                        for (int j = 0; j < 4; j++)
                        {
                            if (BitConverter.IsLittleEndian)
                                Array.Reverse(arrayBytes, j * 4, 4);
                            receivedArray[j] = BitConverter.ToSingle(arrayBytes, j * 4);
                        }
                        sharedData.Add(receivedArray);
                    }
                }

                byte response = 100;
                networkStream.WriteByte(response);
                networkStream.Flush();

            }
            catch (Exception e)
            {
                MessageBox.Show($"{e}");
            }
            finally
            {
                clientSocket.Close();
            }
        }

        private void HandleClient3(object clientObj)
        {
            TcpClient clientSocket = (TcpClient)clientObj;
            try
            {
                // Access the shared data
                float[][] dataArray;
                lock (sharedData)
                {
                    dataArray = sharedData.ToArray();
                }

                NetworkStream networkStream = clientSocket.GetStream();
                byte[] buffer = new byte[clientSocket.ReceiveBufferSize];

                int bytesRead = networkStream.Read(buffer, 0, clientSocket.ReceiveBufferSize);
                sbyte receivedIndex = (sbyte)buffer[0];

                float[] data = dataArray[receivedIndex];

                byte[] arrayBytes = new byte[16];
                for (int i = 0; i < 4; i++)
                {
                    byte[] floatBytes = BitConverter.GetBytes(data[i]);
                    if (BitConverter.IsLittleEndian)
                        Array.Reverse(floatBytes);
                    Buffer.BlockCopy(floatBytes, 0, arrayBytes, i * 4, 4);
                }

                networkStream.Write(arrayBytes, 0, arrayBytes.Length);
            }
            catch (Exception e)
            {
                MessageBox.Show($"{e}");
            }
            finally
            {
                clientSocket.Close();
            }
        }


        private void DisconnectServer()
        {
            if (serverStatus.Text == "Connected")
            {
                // Set the flag to stop the server thread
                isRunning = false;
                isRunning2 = false;
                isRunning3 = false;

                // Close the server socket to stop AcceptTcpClient() blocking
                serverSocket.Stop();
                serverSocket2.Stop();
                serverSocket3.Stop();

                // Wait for the server thread to exit gracefully
                serverThread.Join();
                serverThread2.Join();
                serverThread3.Join();
            }
        }

        private void Socket_connect_Click(object sender, EventArgs e)
        {
            StartServer();
            serverStatus.Text = "Connected";
            Socket_connect.Enabled = false;
            Socket_disconnect.Enabled = true;
        }

        private void Socket_disconnect_Click(object sender, EventArgs e)
        {
            DisconnectServer();
            serverStatus.Text = "Disconnected";
            Socket_connect.Enabled = true;
            Socket_disconnect.Enabled = false;
        }

        async Task<string> SendRequest(string url, Dictionary<string, string> formFields)
        {
            using (var client = new HttpClient())
            {
                using (var formData = new MultipartFormDataContent())
                {
                    foreach (var field in formFields)
                    {
                        formData.Add(new StringContent(field.Value), field.Key);
                    }

                    var response = await client.PostAsync(url, formData);

                    response.EnsureSuccessStatusCode();

                    return await response.Content.ReadAsStringAsync();
                }
            }
        }

        private async Task<string> conduct_match_template()
        {
            // Clear the DataGridView before assigning the new data source
            dataGridView1.DataSource = null;
            dataGridView1.Rows.Clear();
            dataGridView1.Columns.Clear();

            var formFields = new Dictionary<string, string>
            {
                {"api_folder", defaultDirectory},
                {"img_path", targetImagePath},
                {"template_path", templatePath},
                {"threshold", Similarity_score.Text},
                {"overlap", Overlap.Text},
                {"method", Method_similarity.Text},
                {"min_modify", Min_modify.Text},
                {"max_modify", Max_modify.Text},
                {"conf_score", conf_score.Text},
                {"img_size", img_size.Text},
                {"server_ip", serverIP.Text},
                {"output_folder", output_folder}
            };

            var response = await SendRequest("http://127.0.0.1:5000/my_cvu_api", formFields);

            return response;
        }

        private async void Match_template_Click(object sender, EventArgs e)
        {
            stopwatch.Reset();
            stopwatch.Start();
            Elasped_time.Text = "...";
            CVU_status.Text = "Running...";

            _ = await conduct_match_template();

            stopwatch.Stop();
            TimeSpan elapsedTime = stopwatch.Elapsed;
            double roundedElapsedTime = Math.Round(elapsedTime.TotalSeconds, 2);
            string elapsedTimeString = roundedElapsedTime.ToString();

            Elasped_time.Text = $"{elapsedTimeString} s";

            string resultImagePath = Path.Combine(output_folder, "output.jpg");
            string csvPath = Path.Combine(output_folder, "result.csv");

            try
            {
                ShowImage(resultImagePath, Image_box);
                LoadCsvToDataGridView(csvPath, dataGridView1);
                CVU_status.Text = "Completed";
            }
            catch (Exception)
            {
                CVU_status.Text = "No detection found";
            }
        }

        private async Task conduct_capture_frame()
        {
            string cameraPath = Path.Combine(stream_folder, "input_image.jpg");

            try
            {
                await captureFrame();
            }
            catch (Exception e)
            {
                MessageBox.Show($"{e}");
            }

            targetImagePath = cameraPath;
        }

        private async void Cam_capture_Click(object sender, EventArgs e)
        {
            await conduct_capture_frame();

            string cameraPath = Path.Combine(stream_folder, "input_image.jpg");
            Match_template.Enabled = true;
            try
            {
                ShowImage(cameraPath, Image_box);
            }
            catch (Exception)
            {
                MessageBox.Show("No connection with camera");
            }
        }

        private void Image_box_MouseDown(object sender, MouseEventArgs e)
        {
            if (e.Button == MouseButtons.Left)
            {
                isSelecting = true;
                startPoint = e.Location;
            }
        }

        private void Image_box_MouseUp(object sender, MouseEventArgs e)
        {
            if (e.Button == MouseButtons.Left)
            {
                isSelecting = false;
                isReshape = false;

                // update selectedRect with the selected region's rectangle
                selectedRect = new Rectangle(Math.Min(rect.Left, rect.Right),
                                             Math.Min(rect.Top, rect.Bottom),
                                             Math.Abs(rect.Width),
                                             Math.Abs(rect.Height));

                add_template.Enabled = true;
            }
        }

        private void Image_box_MouseMove(object sender, MouseEventArgs e)
        {
            if (isSelecting)
            {
                int x = Math.Min(startPoint.X, e.Location.X);
                int y = Math.Min(startPoint.Y, e.Location.Y);
                int width = Math.Abs(startPoint.X - e.Location.X);
                int height = Math.Abs(startPoint.Y - e.Location.Y);

                rect = new Rectangle(x, y, width, height);
                Image_box.Invalidate();
            }
        }

        private void Image_box_Paint(object sender, PaintEventArgs e)
        {
            if ((isSelecting || isReshape) && rect.Width > 0 && rect.Height > 0)
            {
                using (Pen pen = new Pen(Color.Green, 2))
                {
                    e.Graphics.DrawRectangle(pen, rect);

                    int rectX = rect.X;
                    int rectY = rect.Y;
                    int rectWidth = rect.Width;
                    int rectHeight = rect.Height;

                    // Draw small rectangles at each corner of the rect
                    e.Graphics.FillRectangle(Brushes.Green, rectX - cornerSize + 1, rectY - cornerSize + 1, cornerSize * 2 - 2, cornerSize * 2 - 2);
                    e.Graphics.FillRectangle(Brushes.Green, rectX + rectWidth - cornerSize + 1, rectY - cornerSize + 1, cornerSize * 2 - 2, cornerSize * 2 - 2);
                    e.Graphics.FillRectangle(Brushes.Green, rectX - cornerSize + 1, rectY + rectHeight - cornerSize + 1, cornerSize * 2 - 2, cornerSize * 2 - 2);
                    e.Graphics.FillRectangle(Brushes.Green, rectX + rectWidth - cornerSize + 1, rectY + rectHeight - cornerSize + 1, cornerSize * 2 - 2, cornerSize * 2 - 2);
                }
            }
        }

        private void add_template_Click(object sender, EventArgs e)
        {
            string streamTemplatePath = Path.Combine(template_folder, "template.jpg");

            if (selectedRect.Width > 0 && selectedRect.Height > 0)
            {
                Bitmap bitmap = new Bitmap(Image_box.ClientSize.Width, Image_box.ClientSize.Height);
                Image_box.DrawToBitmap(bitmap, Image_box.ClientRectangle);

                Bitmap croppedBitmap = bitmap.Clone(selectedRect, bitmap.PixelFormat);

                if (!Directory.Exists(template_folder))
                {
                    Directory.CreateDirectory(template_folder);
                }

                croppedBitmap.Save(streamTemplatePath, ImageFormat.Jpeg);
            }

            try
            {
                ShowImage(streamTemplatePath, Template_box);
            }
            catch (Exception)
            {
                MessageBox.Show("Please select a region of interest first");
            }

            templatePath = streamTemplatePath;
        }

        private void updateScrollBarPositions()
        {
            // Calculate the difference between the old and new size of the picture box
            int deltaX = Image_box.Width - hScrollBar1.Maximum;
            int deltaY = Image_box.Height - vScrollBar1.Maximum;

            // Check if the picture box is smaller than the panel and adjust the scroll bar values accordingly
            if (deltaX < 0)
            {
                hScrollBar1.Value = 0;
                deltaX = 0;
            }
            if (deltaY < 0)
            {
                vScrollBar1.Value = 0;
                deltaY = 0;
            }

            // Calculate the new position of the scroll bars based on the difference
            int newHValue = Math.Max(-Image_box.Location.X, 0);
            int newVValue = Math.Max(-Image_box.Location.Y, 0);

            // Update the scrollbars' maximum values to reflect the new size of the picturebox
            hScrollBar1.Maximum = Math.Max(Image_box.Width - panel13.Width, 0);
            vScrollBar1.Maximum = Math.Max(Image_box.Height - panel13.Height, 0);

            // Make sure the scrollbars' values are still within their maximum range
            hScrollBar1.Value = Math.Min(newHValue, hScrollBar1.Maximum);
            vScrollBar1.Value = Math.Min(newVValue, vScrollBar1.Maximum);

            // Update the position of the picturebox based on the scrollbar values
            Image_box.Location = new Point(Math.Max(-hScrollBar1.Value, 6), Math.Max(-vScrollBar1.Value, 21));
        }

        private void vScrollBar1_Scroll(object sender, ScrollEventArgs e)
        {
            Image_box.Top = -e.NewValue;
        }

        private void hScrollBar1_Scroll(object sender, ScrollEventArgs e)
        {
            Image_box.Left = -e.NewValue;
        }

        private void Image_box_Resize(object sender, EventArgs e)
        {
            Image_box.SizeMode = PictureBoxSizeMode.Zoom;
            updateScrollBarPositions();
        }

        private void hScrollBar1_ValueChanged(object sender, EventArgs e)
        {
            Image_box.Left = -hScrollBar1.Value;
        }

        private void vScrollBar1_ValueChanged(object sender, EventArgs e)
        {
            Image_box.Top = -vScrollBar1.Value;
        }

        private void down_scale_MouseDown(object sender, MouseEventArgs e)
        {

            isResizing = true;
            int scaleFactor = Convert.ToInt32(Scale_ratio.Text);
            while (isResizing)
            {
                float aspectRatio = (float)Image_box.Image.Width / (float)Image_box.Image.Height;
                int newWidth = Image_box.Width - (int)(scaleFactor * 1);
                int newHeight = (int)(newWidth / aspectRatio);

                Point oldCenter = new Point(Image_box.Location.X + Image_box.Width / 2,
                                             Image_box.Location.Y + Image_box.Height / 2);

                Image_box.Size = new Size(newWidth, newHeight);

                Point newCenter = new Point(oldCenter.X + (Image_box.Width - newWidth) / 2,
                                             oldCenter.Y + (Image_box.Height - newHeight) / 2);

                Image_box.Location = new Point(newCenter.X - Image_box.Width / 2,
                                                 newCenter.Y - Image_box.Height / 2);

                percentageScale(Image_box);
                updateScrollBarPositions();
                Application.DoEvents();
            }
        }

        private void down_scale_MouseUp(object sender, MouseEventArgs e)
        {
            isResizing = false;
        }

        private void up_scale_MouseDown(object sender, MouseEventArgs e)
        {
            isResizing = true;
            int scaleFactor = Convert.ToInt32(Scale_ratio.Text);
            while (isResizing)
            {
                float aspectRatio = (float)Image_box.Image.Width / (float)Image_box.Image.Height;
                int newWidth = Image_box.Width + (int)(scaleFactor * 1);
                int newHeight = (int)(newWidth / aspectRatio);

                Point oldCenter = new Point(Image_box.Location.X + Image_box.Width / 2,
                                             Image_box.Location.Y + Image_box.Height / 2);

                Image_box.Size = new Size(newWidth, newHeight);

                Point newCenter = new Point(oldCenter.X - (newWidth - Image_box.Width) / 2,
                                             oldCenter.Y - (newHeight - Image_box.Height) / 2);

                Image_box.Location = new Point(newCenter.X - Image_box.Width / 2,
                                                 newCenter.Y - Image_box.Height / 2);

                percentageScale(Image_box);
                updateScrollBarPositions();
                Application.DoEvents();
            }
        }

        private void up_scale_MouseUp(object sender, MouseEventArgs e)
        {
            isResizing = false;
        }

        private void Similarity_score_TextChanged(object sender, EventArgs e)
        {

        }

        private void Overlap_TextChanged(object sender, EventArgs e)
        {

        }

        private void Template_box_Click(object sender, EventArgs e)
        {

        }
    }
}

/////////////////////////////////////////////////////

REM "GRASPING ROBOT PROGRAM"
DIM NUMBER_PALLET AS INTEGER
DIM TOTAL_NUMBER_PALLET AS INTEGER
NUMBER_PALLET=1
SETM O56,0
USE 27
REM "HOME POSITION"
MOVEX A=1, M1X, P, (477.02, 24.34, 535.0, -54.0, -22.33, 25.11), R=V100%, H=31, MS
REM "DOI TIN HIEU KHI CO PALLET"
WAITI I50
REM "DOI_TIN_HIEU_KHI_CO_THONG_SO_VI_TRI_CON_HANG"
CALLMCR 12, 100 
IF V5%>9
V5%=9
ENDIF
FOR V1! = 1 TO V5% STEP 1
REM "VI_TRI_GAP_VAT"
MOVEX A=1, M1X, P, (449.20, 13.51, 340.67, -88.26, 0.00, 1.21), R=40, H=31, MS
REM "MO_TAY_GAP"
SETM O54,1
MOVEX A=1, M1X, P, (511.23, 24.34, 534.94, -54.00, -22.33, 25.10), R=40, MS
MOVEX A=1, M1X, P, (202.23, 337.96, 603.44, -4.10, -20.74, 6.79), R=40, MS
MOVEX A=1, M1X, P, (-122.48, 480.19, 507.24, 48.20, -20.91, 23.10), R=40, H=31, MS
REM "VI_TRI_THA_VAT"
USE 11
MOVEX A=1, M1X,P, P[NUMBER_PALLET], R=40, H=31, MS
REM "DONG_TAY_GAP"
SETM O54,0
NUMBER_PALLET=NUMBER_PALLET+1
IF NUMBER_PALLET>9
GOTO 39
ENDIF
MOVEX A=1, M1X, P, (212.14, 450.14, 140.04, -6.30, 1, 4.84), R=40, H=31, MS
NEXT
IF NUMBER_PALLET<9
GOTO 6
ENDIF
DELAY 0.1
USE 27
MOVEX A=1, M1X, P,(212.14, 450.14, 140.04, -6.3, 1.0, 4.84), R=V100%, H=31,MS
SETM O56,1
END 
 
 

 ///////////////usertask
'CALL SOCKET COMMUNICATE
DIM COUNT_4_BYTE AS INTEGER
SET O50
V5% = 1
'CREATING SOCKET
SOCKCREATE 1,0

'CONNECTING TO SERVER SOCKET
SOCKCONNECT 1,112, 48951, 0

SETBYTE 1,100,0
'TRANSMITING DATA
SOCKSEND 1,1,1,0,V5%

SOCKRECV 1,1,1,0,V6%
'CLOSE SOCKET
SOCKCLOSE 1
EXIT



REM "GRASPING ROBOT PROGRAM"
DIM NUMBER_PALLET AS INTEGER
DIM TOTAL_NUMBER_PALLET AS INTEGER
NUMBER_PALLET=1
V152% = 0
SETM O56,0
USE 27
REM "HOME POSITION"
MOVEX A=1, M1X, P, (477.02, 24.34, 535.0, -54.0, -22.33, 25.11), R=V100%, H=31, MS
REM "DOI TIN HIEU KHI CO PALLET"
WAITI I50
REM "DOI_TIN_HIEU_KHI_CO_THONG_SO_VI_TRI_CON_HANG"
CALLMCR 12, 100 
MOVEX A=1, M1X, P, (V1%, 24.34, 535.0, -54.0, -22.33, 25.11), R=40, H=31, MS

SETM O45,0
IF V5%>9
V5%=9
ENDIF
FOR V1! = 1 TO V5% STEP 1
REM "VI_TRI_GAP_VAT"
MOVEX A=1, M1X, P, (449.20, 13.51, 340.67, -88.26, 0.00, 1.21), R=40, H=31, MS
REM "MO_TAY_GAP"
SETM O54,1
MOVEX A=1, M1X, P, (511.23, 24.34, 534.94, -54.00, -22.33, 25.10), R=40, MS
MOVEX A=1, M1X, P, (202.23, 337.96, 603.44, -4.10, -20.74, 6.79), R=40, MS
MOVEX A=1, M1X, P, (-122.48, 480.19, 507.24, 48.20, -20.91, 23.10), R=40, H=31, MS
REM "VI_TRI_THA_VAT"
USE 11
MOVEX A=1, M1X,P, P[NUMBER_PALLET], R=40, H=31, MS
REM "DONG_TAY_GAP"
SETM O54,0
NUMBER_PALLET=NUMBER_PALLET+1
IF NUMBER_PALLET>9
GOTO 39
ENDIF
MOVEX A=1, M1X, P, (212.14, 450.14, 140.04, -6.30, 1, 4.84), R=40, H=31, MS
NEXT
IF NUMBER_PALLET<9
GOTO 6
ENDIF
DELAY 0.1
USE 27
MOVEX A=1, M1X, P,(212.14, 450.14, 140.04, -6.3, 1.0, 4.84), R=V100%, H=31,MS
SETM O56,1
END 
 
 

 'CALL SOCKET COMMUNICATE
SET O50

'CREATING SOCKET
SOCKCREATE 3,0

'CONNECTING TO SERVER SOCKET
SOCKCONNECT 3,1, 48951, 10

SETBYTE 2,56,0
'TRANSMITING DATA
SOCKSEND 3,2,1,0,V5%

SOCKRECV 3, 2, 2, 0, V150%

GETINT 1, V151%, 0
V1% = V151%


'CLOSE SOCKET
SOCKCLOSE 1
EXIT

//////////////////////test socket:
python server:
# server.py
import socket
import numpy as np
import struct


# ƒê·ªãnh nghƒ©a host v√† port m√† server s·∫Ω ch·∫°y v√† l·∫Øng nghe
host = '172.25.144.1'
port = 48951



s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.bind((host, port))
s.listen(1)  # Ch·ªâ ch·∫•p nh·∫≠n 1 k·∫øt n·ªëi ƒë·ªìng th·ªùi
print("Server listening on port", port)
data_arr = [225.56,255.3,301.4,186.2]




try:
    while True:
        c, addr = s.accept()
        print("Connected from", str(addr))

        # # Server s·ª≠ d·ª•ng k·∫øt n·ªëi ƒë·ªÉ g·ª≠i d·ªØ li·ªáu t·ªõi client d∆∞·ªõi d·∫°ng binary
        # c.send(b"Hello, how are you")

        # Nh·∫≠n 1 byte d·ªØ li·ªáu t·ª´ client
        data = c.recv(1)
        # decode_data = struct.unpack('!f', data)[0]
        decode_data = int.from_bytes(data, byteorder='little')
        print(f"Received data: {decode_data}")
        if decode_data == 56:
            for element in data_arr:

                
                print("hjkhjk: ",element)
                byte_value = struct.pack('!1f', element)
                c.sendall(byte_value)
                print(f"D·ªØ li·ªáu gui t·ª´ server: {byte_value}")

        # if decode_data == "8":
        #     for element in data_arr:
        #         byte_value = struct.pack('!i', element)
        #         c.send(byte_value)
        #         print(f"D·ªØ li·ªáu g·ª≠i t·ª´ server: {element}")

    


        # c.close()

except Exception as e:
    print("Error:", str(e))

finally:
    print("loi")
    s.close()

main:

REM "TEST CONNECT SOCKET"
DIM CLOSE AS INTEGER
REM "HOME POSITION"
MOVEX A=1, M1X, P, (477.02, 24.34, 535.0, -54.0, -22.33, 25.11), R=40, H=31, MS
V50% = 1
V27% = 0
V3! = 0
V1%=0
V4%=0
FORKMCR 3, 100
FOR V2! = 1 TO 4 STEP 1
V4%=1
DELAY 5
REM "RUN"
MOVEX A=1, M1X, P, (V3!,  24.34, 535.0, -54.0, -22.33, 25.11), R=40, H=31, MS
SETM O10,0
V27% = V27% + 1
NEXT


V50% = 0
V1%=1
SETM O11,0
END

usertask:
'sample
INCLUDE "SAMPLE"
'CALL SOCKET COMMUNICATE
'CREATING SOCKET
SOCKCREATE 3,0
'CONNECTING TO SERVER SOCKET
SOCKCONNECT 3,1, 48951, 10
V50% = 1
DIM COUNT AS INTEGER
WAITI I55

SETBYTE 2,V27!,0
'TRANSMITING DATA
SOCKSEND 3,2,1,5,V5%
SOCKRECV 3, 1, 16, 5, V150%

WHILE (V1%=0)

IF (V4%=1)
COUNT = V27%*4 
SET O10
V27! = 56


GETREAL 1, V15!, COUNT
V3! = V15!
V4%=0
ENDIF
ENDW

SET O11
'CLOSE SOCKET
SOCKCLOSE 1
EXIT

/////////////////////////////////////////
REM "TEST CONNECT SOCKET"
DIM CLOSE AS INTEGER
REM "NUMBER OF OBJECT DETECT
V20!=0
REM "HOME POSITION"
MOVEX A=1, M1X, P, (477.02, 24.34, 535.0, -54.0, -22.33, 25.11), R=40, H=31, MS
V15!=0
V27% = 0
V3! = 0
V4! = 0
V5! = 0
V6! = 0
V1%=0
V5%=0
V4%=0
FORKMCR 3, 100
V4%=1
FOR V30! = 1 TO V20! STEP 1
FOR V2! = 1 TO 4 STEP 1
V5%=1
DELAY 1
REM "RUN"
MOVEX A=1, M1X, P, (477.02, 24.34, 535.0, -54.0, -22.33, 25.11), R=40, H=31, MS
SETM O10,0

NEXT
V20!=0
NEXT
IF I50 = 1
V50% = 1
V27% = 0
V3! = 0
V4%=1
DELAY 5
GOTO 14
ENDIF
V1%=1
SETM O11,0
END

///////////////////////
'sample
INCLUDE "SAMPLE"
'CALL SOCKET COMMUNICATE
'CREATING SOCKET
SOCKCREATE 3,0
DIM COUNT AS INTEGER
WAITI I55
IF (V4%=1)
SET O10
'CONNECTING TO SERVER SOCKET
SOCKCONNECT 3,1, 48951, 10
V27! = 1
SETBYTE 2,V27!,0
'TRANSMITING DATA
SOCKSEND 3,2,1,5,V5%
SOCKRECV 3, 2, 36, 15, V150%
'lay do dai
GETREAL 2, V15!, 0
V20!=V15!
V27% = V27% + 1
V4%=0
ENDIF
IF(V5%=1)
GETREAL 2, V15!, V27%*4
V3! = V15!
V27%=V27%+1
GETREAL 2, V15!, V27%*4
V4! = V15!
V27%=V27%+1
GETREAL 2, V15!, V27%*4
V5! = V15!
V27%=V27%+1
GETREAL 2, V15!, V27%*4
V6! = V15!
V27%=V27%+1
V5%=0
ENDIF
IF (V1%=0)
GOTO 7
ENDIF

SET O11
'CLOSE SOCKET
SOCKCLOSE 1
EXIT
/////////////////////////////////////////////
# server.py
import socket
import numpy as np
import struct
import requests
import time

# ƒê·ªãnh nghƒ©a host v√† port m√† server s·∫Ω ch·∫°y v√† l·∫Øng nghe
host = '172.25.144.1'
port = 48951

headers = {"Content-Type": "application/json", "Authorization": "Bearer your_token"}

s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.bind((host, port))
s.listen(1)  # Ch·ªâ ch·∫•p nh·∫≠n 1 k·∫øt n·ªëi ƒë·ªìng th·ªùi
print("Server listening on port", port)
# data_arr=[]




try:
    while True:
        c, addr = s.accept()
        print("Connected from", str(addr))

        # # Server s·ª≠ d·ª•ng k·∫øt n·ªëi ƒë·ªÉ g·ª≠i d·ªØ li·ªáu t·ªõi client d∆∞·ªõi d·∫°ng binary
        # c.send(b"Hello, how are you")
        while True:
            # Nh·∫≠n 1 byte d·ªØ li·ªáu t·ª´ client
            data = c.recv(1)
            # decode_data = struct.unpack('!f', data)[0]
            decode_data = int.from_bytes(data, byteorder='little')
            print(f"Received data: {decode_data}")
            if decode_data == 1:
                # for i in range(len(data_arr)):
                #     data_arr[i] = data_arr[i] + 10
                # for element in data_arr:
                    
                #     print("hjkhjk: ",element)
                #     byte_value = struct.pack('!1f', element)
                #     c.sendall(byte_value)
                #     print(f"D·ªØ li·ªáu gui t·ª´ server: {byte_value}")
                #     # ////////////////////////////////
                api_url = "http://127.0.0.1:5000/cvu_process"
                form_data={
                    "imgLink" : r"C:\Users\TKD01A-1\Documents\yolov8_test\nhandienvat-main\nhandienvat-main\test\dataset_specialItems\train\images\20230922_155434_jpg.rf.ac089e9cd5afade05e7ba5d035c4d823.jpg",
                    "templateLink" : r"C:\Users\TKD01A-1\Documents\yolov8_test\nhandienvat-main\nhandienvat-main\test\template.jpg",
                    "modelLink" : r"C:\Users\TKD01A-1\Documents\yolov8_test\nhandienvat-main\nhandienvat-main\test\runs\segment\train\weights\last.pt",
                    "pathSaveOutputImg" : "",
                    "csvLink" : "",
                    "outputImgLink" : "",
                    "min_modify" : "-20",
                    "max_modify" : "20",
                    "configScore" : "0.9",
                    "img_size" : "640",
                    "method" : "cv2.TM_CCORR_NORMED",
                    "server_ip" : ""
                }
                response = requests.post(api_url, data=form_data)
                if response.status_code == 200:
                    print("respons and type: ", response.json(), type(response))
                    data_arr = response.json()
                    byte_value_length = struct.pack('!1f', len(data_arr))
                    c.sendall(byte_value_length)
                    print(f"D·ªØ li·ªáu gui t·ª´ server: {byte_value_length}")
                    for object in data_arr:
                        for element in object:
                            print("hjkhjk: ",element)
                            byte_value = struct.pack('!f', element)
                            c.sendall(byte_value)
                            print(f"D·ªØ li·ªáu gui t·ª´ server: {byte_value}")
                    print("done!!")
                    break
            # c.close()

except Exception as e:
    print("Error:", str(e))

finally:
    print("loi")
    s.close()

////////////////////////////////////////////////////////
cvu_multi.py

from flask import Flask, request
import numpy as np
from components import *
from ultis import *
from copy import deepcopy
from API_flask import create_app
import csv
import time
import threading
# import keyboard

logger = logging.getLogger(__name__)

app = create_app()

@app.route('/cvu_process', methods=['GET','POST'])
def cvu_process():

  start_time = time.time()
# /////////Input////////////////////
  if request.method == "POST":
      #///Form data
      imgLink = request.form.get('imgLink')
      templateLink = request.form.get('templateLink')
      modelLink = request.form.get('modelLink')
      pathSaveOutputImg = request.form.get('pathSaveOutputImg')

      try:
            csvLink = request.form.get('csvLink')
            outputImgLink = request.form.get('outputImgLink')
            min_modify = int(request.form.get('min_modify'))
            max_modify = int(request.form.get('max_modify'))
            configScore = float(request.form.get('configScore'))
            img_size = int(request.form.get('img_size'))
            method = request.form.get('method')
            server_ip = request.form.get('server_ip')

      except Exception as e:
            logger.error(f'{e}\n')
            return f'{e}\n'

      #/////////Begin process/////////////////
      imgLink = imgLink.replace('\\', '/')
      templateLink = templateLink.replace('\\', '/')
      img = cv2.imread(imgLink)
      template = cv2.imread(templateLink)
      template = cv2.resize(template, (255,165))
      gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
      template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)
      copy_of_template_gray = deepcopy(template_gray)
      minus_modify_angle = np.arange(0, min_modify, -2) 
      plus_modify_angle = np.arange(0, max_modify, 2) 
      low_clip, high_clip=5.0, 97.0
      copy_of_template_gray = contrast_stretching(copy_of_template_gray,  low_clip,high_clip)
      _, copy_of_template_gray = cv2.threshold(copy_of_template_gray, 100, 255, cv2.THRESH_BINARY_INV)
      # cv2.imwrite("thresTemp.jpg",copy_of_template_gray)
      intensity_of_template_gray = np.sum(copy_of_template_gray == 0)
      findCenter_type = 0
      good_points = []
      try:
            object_item = proposal_box_yolo(imgLink,modelLink,img_size,configScore)#object_item s·∫Ω g·ªìm list th√¥ng tin g√≥c v√† t·ªça ƒë·ªô c·ªßa ƒë∆∞·ªùng bao
           
            # print("number class: ",object_item[0][2])
            if object_item == None:
                 return good_points
            good_points.append([object_item[0][2]])
            for angle,bboxes,_ in object_item:
                #   print("------------------------------------------------------------")
                  result_queue = []
                  minus_sub_angles, plus_sub_angles = angle + minus_modify_angle, angle + plus_modify_angle
                  
                  # threshold = 0.95
                  point = match_pattern(gray_img, template_gray, bboxes, angle, eval(method)) 
                  if point is None:
                        continue
                  p1 = threading.Thread(target=find_center2, args=(gray_img,bboxes,low_clip,high_clip, intensity_of_template_gray, findCenter_type,result_queue,))
                  p2 = threading.Thread(target=compare_angle, args=(point,minus_sub_angles,plus_sub_angles, gray_img, template_gray, bboxes, angle, eval(method),result_queue,))
                  p1.start(), p2.start()                       
                  p1.join(), p2.join()
                  
                  # if len(result_queue) == 2:
                  bestAngle, bestPoint = result_queue[1]
                  center_0,center_1, possible_grasp_ratio =  result_queue[0]
                  if center_0 == None and center_1 == None:
                        continue          
                  if possible_grasp_ratio < 90:
                        print("score<95!")
                        continue
                  good_points.append([center_0,center_1,bestAngle,possible_grasp_ratio])
                  # Vi·∫øt ch·ªØ l√™n h√¨nh ·∫£nh
                  # cv2.putText(img, f"{bestAngle}", (int(center_0),int(center_1)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
                  # x2, y2 = int(center_0 + 150* np.cos(np.radians(bestAngle)) ),int(center_1 + 150* np.sin(np.radians(bestAngle)) )
                  # x3, y3 = int(center_0 + 100* np.cos(np.radians(bestAngle+90)) ),int(center_1 + 100* np.sin(np.radians(bestAngle+90)) )
                  # cv2.line(img,(center_0,center_1),(x2,y2),(255,255,0),2)
                  # cv2.line(img,(center_0,center_1),(x3,y3),(255,0,255),2)
                  # cv2.imwrite("amTam.jpg",img)
                  # print("total: ",center_0,center_1,bestAngle,possible_grasp_ratio)
            print("good point arr: ",good_points)

            print("process time: ",time.time() - start_time)
            return good_points
      except Exception as e:
           print("System error: ", e)
           return good_points

#   if request.method == "GET":
#        return f'<div><h1>Get result</h1></div>'
       

if __name__ == "__main__":
     app.run(debug=True)

//////////////////////////////////////////////////////////////////////////

proposal_bb.py

import numpy as np
from ultis.apply_min_area import apply_min_area
from ultralytics import YOLO

class YOLOSegmentation:
    def __init__(self,model):
        self.model =YOLO(model)  # load a custom model
        

    def predict(self,img,img_size,configScore):

        #Th·ª±c hi·ªán nh·∫≠n di·ªán
        pred_img = self.model(img, save=False, conf=configScore, imgsz=img_size)
        #l·∫•y danh s√°ch c√°c bounding box
        bboxes = np.array(pred_img[0].boxes.xyxy, dtype="int")
        # l·∫•y danh s√°ch m·∫∑t n·∫° masks c·ªßa m·ªói ƒë·ªëi t∆∞·ª£ng ƒëc∆∞·ª£c nh·∫≠n di·ªán
        masks = np.array(pred_img[0].masks.xy,dtype=object)
        # l·∫•y danh s√°ch c√°c lo·∫°i ƒë·ªëi t∆∞·ª£ng ƒë√£ ƒë∆∞·ª£c nh·∫≠n di·ªán
        class_ids = np.array(pred_img[0].boxes.cls,dtype="int")
        # print(class_ids)
        # l·∫•y danh s√°ch ƒë·ªô ch√≠nh x√°c c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng ƒë√£ ƒë∆∞·ª£c nh·∫≠n di·ªán
        score = np.array(pred_img[0].boxes.conf,dtype="float").round(2)
        return bboxes, masks, class_ids, score
    
    @staticmethod
    def filter_boxes(bboxes,masks,class_ids,score):
        # ch·ªâ l·∫•y c√°c ƒë·ªëi t∆∞·ª£ng v·ªõi ch·ªâ s·ªë l√† 3(mat_dung)
        obj_detect = class_ids == 0
        # object th√¥ng tin bounding box, masks, score c·ªßa c√°c d·ªëi t∆∞·ª£ng ƒë√∫ng
        object_true = bboxes[obj_detect,:], masks[obj_detect], score[obj_detect]
        # print(object_true)
        # object th√¥ng tin bounding box, masks, score c·ªßa c√°c d·ªëi t∆∞·ª£ng sai
        object_fail = bboxes[~obj_detect,:],masks[~obj_detect],score[~obj_detect]
        # print("detect: ",object_true)
        return object_true,object_fail

    @staticmethod
    def create_angle(mask_true):  
         #truy c·∫≠p v√†o t·ª´ng ƒëi·ªÉm c·ªßa mask v√† t√≠nh to√°n
         angle = list(map(lambda x: apply_min_area(x), mask_true))
         return angle
    @staticmethod
    def convert_xywh(boxes):
    #    t·∫°o th√¥ng tin bounding box t·ª´ dang xyxy chuy·ªÉn sang d·∫°ng xywh(xy ·ªü ƒë√¢y l√† ƒëi·ªÉm x_min y_min ch·ª© ko ph·∫£i l√† center)
        boxes[:, 2], boxes[:, 3] = boxes[:, 2] - boxes[:, 0], boxes[:, 3] - boxes[:, 1]
        return boxes
    
        

def proposal_box_yolo(img,model,image_size,configScore):
    
    ys = YOLOSegmentation(model)
    try:
        bboxes,masks,class_ids, score = ys.predict(img,image_size, configScore)
        # print("just say hello!")
        obj,_ = ys.filter_boxes(bboxes,masks,class_ids,score)
        # print("dodai conhang: ", len(obj[0]))
        # t√≠nh to√°n g√≥c xoay d·ª±a v√†o c√°c ƒëi·ªÉm masks(obj[1] l√† list c√°c ƒëi·ªÉm masks ƒë√∫ng)
        angle_test = ys.create_angle(obj[1])
        # print(" conhang: ", angle_test)
        # print("angle: ",angle_test)
        xywh_boxes = ys.convert_xywh(obj[0])
        number_items = np.full(len(xywh_boxes),len(obj[0]),dtype=float)
        # print("number_items",number_items)
        return list(zip(angle_test, xywh_boxes,number_items))
    except Exception as e:
        print("Yolo detection error or no detection: ",e)
        
    

//////////////////////////////
apply_min_area.py


import cv2
import numpy as np

def apply_min_area(contour):
 contour = np.array(contour, dtype=np.int32)
 rotated_rect= cv2.minAreaRect(contour)
 #l·∫•y g√≥c m·∫∑c ƒë·ªãnh
 #l·∫•y t·ªça ƒë·ªô c√°c ƒë·ªânh c·ªßa rotated_rect
 rect_points = cv2.boxPoints(rotated_rect).astype(int)

 edge1 = np.array(rect_points[1]) - np.array(rect_points[0])
 edge2 = np.array(rect_points[2]) - np.array(rect_points[1])
 reference = np.array([1, 0])
#  v·ªõi ta c√≥ adge1 v√† edge2 l√† toa ƒë·ªô l·∫ßn l∆∞·ª£t c·ªßa vecto 01 v√† vecto 12
#  v√† c√πng v·ªõi c√∫ ph√°p numpy np.linalg.norm(edge) ta s·∫Ω t√≠nh ƒë∆∞·ª£c ƒë·ªô d√†i c·ªßa 2 vecto ƒë√≥, ƒë·ªô d√†i n√†o l·ªõn h∆°n(v√¨ ƒë√¢y l√† h√¨nh ch·ªØ nh·∫≠t v√† ta l·∫•y c·∫°nh d√†i ƒë·ªÉ cƒÉn g√≥c) th√¨ 
#  ƒë∆∞·ª£c d√πng l√†m ƒëo·∫°n th·∫≥ng k·∫øt h·ª£p v·ªõi ƒëi·ªÉm tham chi·∫øu ƒë·ªÉ t√≠nh to√°n g√≥c b·∫±ng c√¥ng th·ª©c cosin
#  print("dodai: ",np.linalg.norm(edge1),np.linalg.norm(edge2))
#  print("point: ", rect_points)
 if np.linalg.norm(edge1) > np.linalg.norm(edge2):    

         used_edge = edge1
         #  np.arccos d√πng ƒë·ªÉ t√≠nh ph√©p t√≠nh arccos trong l∆∞·ª£ng gi√°c, np.dot d√πng ƒë·ªÉ t√≠nh t√≠ch v√¥ h∆∞·ªõng
         pre_angle = (180.0 / np.pi) * (np.arccos(np.dot(reference, used_edge) / (np.linalg.norm(reference) * np.linalg.norm(used_edge))))
         angle = 360 - pre_angle
        #  print("angle AS: ",angle)    
 else:
         
         used_edge = edge2
         angle = (180.0 / np.pi)*(np.arccos(np.dot(reference, used_edge) / (np.linalg.norm(reference) * np.linalg.norm(used_edge))))
        #  print("angle AC: ",angle)
 if angle <= 0:
        angle = 360 + angle
 return angle


////////////////////////////////////////////////

compare_angle.py

from components.match_pattern import match_pattern

def compare_angle(point,minus_sub_angles,plus_sub_angles, gray_img, template_gray, bboxes, angle, method,result_queue ):
    minus_pointer, plus_pointer = 0,0
    exactly_minus, exactly_plus = 0, 0
    high_point_minus, high_point_plus = point, point
    bestAngle,bestPoint = 0,0

    while minus_pointer < len(minus_sub_angles)  or plus_pointer < len(plus_sub_angles):
        point_minus = match_pattern(gray_img, template_gray, bboxes, minus_sub_angles[minus_pointer], method)
        point_plus = match_pattern(gray_img, template_gray, bboxes, plus_sub_angles[plus_pointer], method)
        if point_minus >= high_point_minus:
            if minus_sub_angles[minus_pointer] >= 360:
                exactly_minus = minus_sub_angles[minus_pointer] - 360
            else:
                exactly_minus = minus_sub_angles[minus_pointer]
            if point_minus > bestPoint:
                bestAngle = exactly_minus
                bestPoint = point_minus
            high_point_minus = point_minus
        
        if point_plus >= high_point_plus:
            if plus_sub_angles[plus_pointer] >= 360:
                exactly_plus = plus_sub_angles[plus_pointer] - 360
            else:
                exactly_plus = plus_sub_angles[minus_pointer]
            if point_plus > bestPoint:
                bestAngle = exactly_plus
                bestPoint = point_plus
            high_point_plus = point_plus
        
        minus_pointer = minus_pointer + 1
        plus_pointer = plus_pointer + 1
        # print("---------------------------------------")
    if bestPoint < point:
        bestAngle = angle
    result_queue.append((bestAngle, bestPoint))
    return bestAngle, bestPoint

#ki·ªÉm tra t·ª´ng ph·∫ßn t·ª≠ minus_sub_angles,plus_sub_angles, l·∫•y angle m√† c√≥ s·ªë ƒëi·ªÉm cao nh·∫•t 


///////////////////////////////////////////////////////////////////
match_pattern.py

import cv2
from ultis.rotate_object import rotate_object 
import logging


logger = logging.getLogger(__name__)
# m·ªü r·ªông khung ·∫£nh m·ªôt khi t·ªça ƒë·ªô bbox ·ªü s√°t khung ·∫£nh, c·∫£n tr·ªü vi·ªác roi ƒë·ªëi t∆∞·ª£ng
def padded_image(img_gray, bboxes, esilon_w,epsilon_h):
   # print("b0,b1", bboxes[0],  bboxes[1])
   x_start,y_start  = bboxes[0] - esilon_w, bboxes[1] - epsilon_h
   x_end, y_end = bboxes[0] + bboxes[2] + esilon_w, bboxes[1] + bboxes[3] + epsilon_h
   padded_left, padded_top = min(x_start,0), min(y_start,0)
   padded_right, padded_bottom = min(img_gray.shape[1] - x_end, 0), min(img_gray.shape[0] - y_end,0)
    
   # print("paded: ",padded_left, padded_top, padded_right, padded_bottom)
   img_padded = cv2.copyMakeBorder(img_gray,abs(padded_top),abs(padded_bottom), abs(padded_left), abs(padded_right),cv2.BORDER_CONSTANT, value=0 )
   # cv2.imwrite("img_paded.jpg",img_padded)
   return img_padded, x_start, x_end, y_start, y_end


def match_pattern(img_gray, template_gray, boxes, sub_angle, method ):
   rotated_template,mask, w_temp,h_temp = rotate_object(template_gray,sub_angle)
   epsilon_w, epsilon_h = abs(boxes[2]-w_temp), abs(boxes[3]-h_temp)
   # print("epsilon_w, epsilon_h", epsilon_w, epsilon_h,boxes[2],boxes[3])
   img_padded, x_start, x_end, y_start, y_end = padded_image(img_gray,boxes, epsilon_w, epsilon_h)
   img_roi = img_padded[abs(y_start) : abs(y_end)  ,abs(x_start) : abs(x_end)]
   # cv2.imwrite( "img_roi.jpg",img_roi)
   matched_points = cv2.matchTemplate(img_roi, rotated_template, method, None, mask)
   _, max_val, _, _ = cv2.minMaxLoc(matched_points)
   # print("point mark: ", max_val)
   return max_val




# matched_points l√† m·ªôt m·∫£ng hai chi·ªÅu ch·ª©a c√°c ƒëi·ªÉm t∆∞∆°ng t·ª± ho·∫∑c gi√° tr·ªã li√™n quan ƒë·∫øn m·ª©c ƒë·ªô kh·ªõp gi·ªØa m·∫´u v√† ·∫£nh. M·ªói ph·∫ßn t·ª≠ trong m·∫£ng t∆∞∆°ng ·ª©ng v·ªõi m·ª©c ƒë·ªô kh·ªõp t·∫°i m·ªôt v·ªã tr√≠ c·ª• th·ªÉ trong ·∫£nh.

# H√†m cv2.minMaxLoc ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√¨m gi√° tr·ªã nh·ªè nh·∫•t v√† l·ªõn nh·∫•t trong m·∫£ng matched_points v√† c≈©ng tr·∫£ v·ªÅ v·ªã tr√≠ (t·ªça ƒë·ªô x, y) c·ªßa gi√° tr·ªã l·ªõn nh·∫•t.

# Khi b·∫°n g√°n k·∫øt qu·∫£ t·ª´ cv2.minMaxLoc cho c√°c bi·∫øn _, max_val, _, v√† max_loc, b·∫°n c√≥ c√°c gi√° tr·ªã sau:

# _ (underscore ƒë·∫ßu ti√™n) l√† gi√° tr·ªã nh·ªè nh·∫•t trong m·∫£ng matched_points, nh∆∞ng b·∫°n kh√¥ng s·ª≠ d·ª•ng n√≥ trong v√≠ d·ª• n√†y, v√¨ b·∫°n ch·ªâ quan t√¢m ƒë·∫øn gi√° tr·ªã l·ªõn nh·∫•t.
# max_val l√† gi√° tr·ªã l·ªõn nh·∫•t trong m·∫£ng matched_points. ƒê√¢y c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† m·ª©c ƒë·ªô kh·ªõp t·ªëi ƒëa gi·ªØa m·∫´u v√† v√πng quan t√¢m c·ªßa ·∫£nh.
# _ (underscore th·ª© hai) l√† v·ªã tr√≠ (t·ªça ƒë·ªô x, y) c·ªßa gi√° tr·ªã nh·ªè nh·∫•t trong m·∫£ng, b·∫°n c≈©ng kh√¥ng s·ª≠ d·ª•ng n√≥.
# max_loc l√† v·ªã tr√≠ (t·ªça ƒë·ªô x, y) c·ªßa gi√° tr·ªã l·ªõn nh·∫•t trong m·∫£ng matched_points. N√≥ cho bi·∫øt v·ªã tr√≠ t∆∞∆°ng ·ª©ng tr√™n ·∫£nh g·ªëc (img_gray) m√† m·∫´u c√≥ m·ª©c ƒë·ªô kh·ªõp t·ªëi ƒëa.
# Sau d√≤ng m√£ n√†y, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng gi√° tr·ªã max_val ƒë·ªÉ xem m·ª©c ƒë·ªô kh·ªõp t·ªëi ƒëa v√† max_loc ƒë·ªÉ bi·∫øt v·ªã tr√≠ t∆∞∆°ng ·ª©ng tr√™n ·∫£nh.

///////////////////////////////////////////////////////////////////////

finde_center_2.py


import cv2
import numpy as np
from ultis.processing_image import contrast_stretching
import time


def find_center2(gray_img,bboxes, low_clip,high_clip,intensity_of_template_gray,findCenter_type,result_queue):
    # print("time excute: ", time.time())
    x1,y1,w,h = bboxes[0], bboxes[1], bboxes[2], bboxes[3]
    # c1,c2 = x1 + w/2, y1 + h/2
    center_obj = (0,0)
    if(x1<10 or y1 <10):
     gray_img = cv2.copyMakeBorder(gray_img,20,20, 20, 20,cv2.BORDER_CONSTANT, value=0 )
    imgRoi = gray_img[y1 - 3:y1+h +3,x1 - 3:x1+w +3]
    # cv2.imwrite("roi_findCenter.jpg",imgRoi)
    
    padded_roi_gray = gray_img[y1-20:y1+h + 20,x1-20:x1+w + 20]
    
    thresholdImg = contrast_stretching(imgRoi,low_clip,high_clip)
    
    padded_thresholdImg = contrast_stretching(padded_roi_gray,  low_clip,high_clip)
    _,thresholdImg = cv2.threshold(thresholdImg,100,255, cv2.THRESH_BINARY_INV)
    # cv2.imwrite("thes1.jpg",padded_thresholdImg)
    _,padded_roi_gray = cv2.threshold(padded_thresholdImg,100,255, cv2.THRESH_BINARY_INV)
    intensity_of_roi_gray = np.sum(padded_roi_gray == 0)
    # print("intensity_of_roi_gray: ",intensity_of_roi_gray)
    # print("intensity_of_temp_gray: ",intensity_of_template_gray)
    possible_grasp_ratio = (intensity_of_template_gray / intensity_of_roi_gray) * 100
    # print("possible_grasp_ratio: ",possible_grasp_ratio)
    if findCenter_type == 0:
        try:
            contours,_ = cv2.findContours(thresholdImg,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
            # print("len: ",len(contours))
            check_true = False
            for element in contours:
                s = cv2.contourArea(element)
                # print('S contour: ',s)
                if s < 1500 or s > 3000:
                    continue
                check_true = True
                (x_axis, y_axis), radius = cv2.minEnclosingCircle(element)
                center = (int(x_axis + x1),int(y_axis + y1)) 
                radius = int(radius) 
                # print("kq: ",(int(abs(center[0] - c1)),int( abs(center[1] - c2))))
                # result = (int(c1 + abs(center[0] - c1)/2),int(c2 + abs(center[1] - c2)/2))
                cv2.circle(gray_img,center,radius,(0,255,0),1) 
                cv2.circle(gray_img,center,1,(255,255,0),3) 
                # cv2.imwrite("thes.jpg",gray_img)
                center_obj = (center[0],center[1])
            if check_true == False:
                center_obj = (None, None)

        except Exception as e:
                # print("S < 1500 or s > 3000")
                center_obj = (None, None)

    result_queue.append((center[0],center[1],possible_grasp_ratio))
    
    return center_obj, possible_grasp_ratio





