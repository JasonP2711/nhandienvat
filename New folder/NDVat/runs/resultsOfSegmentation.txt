boxes: Đây là tensor chứa các thông tin về bounding boxes của các đối tượng được phát hiện trong hình ảnh. Mỗi dòng của tensor này tương ứng với một bounding box và bao gồm các thông tin sau:

Cột 1 và 2: Tọa độ x và y của góc trên bên trái của bounding box.
Cột 3 và 4: Tọa độ x và y của góc dưới bên phải của bounding box.
Cột 5: Độ tự tin (confidence) của việc phát hiện.
Cột 6: ID của lớp (class ID) của đối tượng được phát hiện.
cls: Đây là tensor chứa ID của lớp của các đối tượng được phát hiện.

conf: Đây là tensor chứa giá trị độ tự tin (confidence) của các đối tượng được phát hiện.

data: Tương tự như boxes, đây là tensor chứa thông tin về bounding boxes của các đối tượng, nhưng được tổ chức theo cấu trúc tensor khác.

id: Đây là một thuộc tính (attribute) không có giá trị cụ thể trong kết quả.

is_track: Đây là một thuộc tính (attribute) đánh dấu xem liệu các bounding boxes có phải là theo dõi các đối tượng hay không.

orig_shape: Đây là kích thước gốc của hình ảnh trước khi được xử lý bởi mô hình.

shape: Đây là kích thước của hình ảnh sau khi được xử lý bởi mô hình.

xywh: Đây là tensor chứa thông tin về tọa độ và kích thước của bounding boxes dưới dạng (x_center, y_center, width, height).

xywhn: Đây là tensor chứa thông tin tương tự xywh, nhưng được chuẩn hóa trong khoảng từ 0 đến 1.

xyxy: Đây là tensor chứa thông tin về tọa độ của bounding boxes dưới dạng (x_min, y_min, x_max, y_max).

xyxyn: Đây là tensor chứa thông tin tương tự xyxy, nhưng được chuẩn hóa trong khoảng từ 0 đến 1.



Thông tin về mask trong ultralytics.engine.results.Masks bao gồm:

data và masks: Đây là hai tensor có cùng kích thước, đại diện cho mask của các vật thể trong hình ảnh. Mỗi tensor có kích thước (1, H, W), trong đó H là chiều cao của mask và W là chiều rộng của mask. Mask được biểu diễn dưới dạng ma trận các giá trị số thực trong khoảng từ 0 đến 1, trong đó 0 thường đại diện cho vùng không có vật thể và 1 đại diện cho vùng chứa vật thể.

orig_shape: Kích thước của hình ảnh gốc trước khi được thay đổi kích thước để phù hợp với mô hình.

segments: Một danh sách các điểm trên biên của mask, mỗi điểm được biểu diễn bằng tọa độ (x, y) trên ma trận mask.

shape: Kích thước của tensor mask, có dạng (1, H, W), trong đó H là chiều cao của mask và W là chiều rộng của mask.

xy và xyn: xy là danh sách các điểm trên biên của mask, mỗi điểm được biểu diễn bằng tọa độ (x, y) trong định dạng tensor. xyn là danh sách tọa độ của các điểm trên biên của mask được chuẩn hóa trong khoảng từ 0 đến 1, thường được sử dụng để vẽ mask trên hình ảnh.

Với thông tin này, bạn có thể truy cập và xử lý mask của các vật thể được phát hiện trong hình ảnh.


/////////////////////////////////

OpenCV cung cấp một số phương pháp để xác định góc quay của một vật trong một ảnh. Dưới đây là một số cách thường được sử dụng:

Sử dụng Hough Line Transform (Biến đổi Hough cho đường thẳng): Đây là một trong những phương pháp phổ biến để xác định góc quay của vật thể trên ảnh. Bạn có thể sử dụng hàm cv2.HoughLines() để tìm các đường thẳng trong ảnh sau đó tính toán góc giữa đường thẳng và trục tọa độ.

Sử dụng Feature Matching (Khớp đặc trưng): Bạn có thể sử dụng thuật toán khớp đặc trưng như SIFT (Scale-Invariant Feature Transform) hoặc SURF (Speeded-Up Robust Features) để tìm các điểm đặc trưng trên vật thể và sau đó tính toán góc quay từ các điểm này.

=>> (không dùng được) Sử dụng Contour Detection (Phát hiện đường viền): Nếu vật thể có hình dạng đặc trưng, bạn có thể sử dụng hàm cv2.findContours() để tìm đường viền của vật thể và sau đó tính toán góc quay dựa trên hướng của đường viền.

Sử dụng Homography: Nếu bạn có một vật thể với biến đổi homography (một loại biến đổi affine hoặc perspective), bạn có thể sử dụng hàm cv2.findHomography() để xác định ma trận homography và sau đó tính toán góc quay từ ma trận này.

Dưới đây là một ví dụ sử dụng phương pháp Hough Line Transform để xác định góc quay của vật thể trong ảnh:





////////////////////////////////////


Để thêm tâm của bounding box vào ảnh, bạn có thể sử dụng thư viện OpenCV (hoặc một thư viện xử lý ảnh tương tự). Dưới đây là cách bạn có thể thực hiện điều này:

Trích xuất tọa độ tâm của bounding boxes từ tensor xywh (hoặc xyxy nếu bạn muốn).

Vẽ một điểm hoặc một đường trỏ tới tâm của bounding box trên ảnh gốc.

Dưới đây là một ví dụ về cách thực hiện điều này:

import cv2
import numpy as np

# Load the image
image = cv2.imread('/content/drive/MyDrive/NhanDienvat/NDVat/train/images/20230919_104657_jpg.rf.f419369aed5b63d2614e2c2014ef43e5.jpg')

# Extract the 'xywh' tensor from the results (assuming a single image prediction)
xywh = results.pred[0].xywh

# Iterate through bounding boxes and draw a dot at the center
for bbox in xywh:
    x_center, y_center, width, height = bbox[:4]
    x_center = int(x_center)
    y_center = int(y_center)
    cv2.circle(image, (x_center, y_center), 5, (0, 255, 0), -1)  # Draw a green circle at the center

# Save the image with center points
cv2.imwrite('/content/center_points.jpg', image)

# Display the image (optional)
cv2.imshow('Image with Center Points', image)
cv2.waitKey(0)
cv2.destroyAllWindows()

//////Để tính toán góc xoay của một đường thẳng nằm trong một hình ảnh, bạn có thể sử dụng một số phương pháp xử lý hình ảnh và toán học. Dưới đây là một cách tiêu biểu để thực hiện điều này:

Phát hiện đường thẳng: Đầu tiên, bạn cần phát hiện đường thẳng trong hình ảnh. OpenCV cung cấp một số phương pháp phát hiện đường thẳng, chẳng hạn như cv2.HoughLines hoặc cv2.HoughLinesP dựa trên biến đổi Hough.

Tính toán góc xoay: Sau khi bạn đã có danh sách các đường thẳng được phát hiện, bạn có thể tính toán góc xoay của mỗi đường thẳng. Để làm điều này, bạn có thể sử dụng hàm toán học như atan2 (tính toán góc dựa trên các tọa độ của đầu mút của đường thẳng) hoặc arctan (tính toán góc dựa trên hệ số góc của đường thẳng).

Dưới đây là một ví dụ cụ thể sử dụng OpenCV để tính toán góc xoay của đường thẳng:

python
Copy code
import cv2
import numpy as np

# Đọc hình ảnh
image = cv2.imread('image_with_line.jpg', cv2.IMREAD_GRAYSCALE)

# Phát hiện đường thẳng bằng biến đổi Hough
lines = cv2.HoughLines(image, 1, np.pi / 180, threshold=100)

# Tính toán góc xoay của từng đường thẳng và chuyển đổi thành độ
for rho, theta in lines[:, 0]:
    angle = theta * 180 / np.pi
    print(f'Góc xoay: {angle} độ')

# Hiển thị hình ảnh với đường thẳng đã phát hiện
for rho, theta in lines[:, 0]:
    a = np.cos(theta)
    b = np.sin(theta)
    x0 = a * rho
    y0 = b * rho
    x1 = int(x0 + 1000 * (-b))
    y1 = int(y0 + 1000 * (a))
    x2 = int(x0 - 1000 * (-b))
    y2 = int(y0 - 1000 * (a))
    cv2.line(image, (x1, y1), (x2, y2), (0, 0, 255), 2)

cv2.imshow('Image with Lines', image)
cv2.waitKey(0)
cv2.destroyAllWindows()


///////////////////////////////////////////////xac dinh huong orientation.py

# from __future__ import print_function
# from __future__ import division
import cv2 as cv
import numpy as np
import argparse
from math import atan2, cos, sin, sqrt, pi
def drawAxis(img, p_, q_, colour, scale,degree):
 p = list(p_)
 q = list(q_)
 print("diemP: ",p)
 print("diemQ: ",q)
 text1 = f"({p[0]}, {p[1]})"
 
 cv.putText(img, text1, (p[0]+30,p[1]+30), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)
 angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians
 print("angle: ",degree)
 text2 = f"({degree})"
 cv.putText(img, text2, (p[0]+150,p[1]-30), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)

 hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))
 # Here we lengthen the arrow by a factor of scale
 q[0] = p[0] - scale * hypotenuse * cos(angle)
 q[1] = p[1] - scale * hypotenuse * sin(angle)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 # create the arrow hooks
 p[0] = q[0] + 9 * cos(angle + pi / 4)
 p[1] = q[1] + 9 * sin(angle + pi / 4)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 p[0] = q[0] + 9 * cos(angle - pi / 4)
 p[1] = q[1] + 9 * sin(angle - pi / 4)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 
def getOrientation(pts, img):
 
 sz = len(pts)
 data_pts = np.empty((sz, 2), dtype=np.float64)
 for i in range(data_pts.shape[0]):
  data_pts[i,0] = pts[i,0,0]
  data_pts[i,1] = pts[i,0,1]
 # Perform PCA analysis
 mean = np.empty((0))
 mean, eigenvectors, eigenvalues = cv.PCACompute2(data_pts, mean)
 # Store the center of the object
 cntr = (int(mean[0,0]), int(mean[0,1]))
 
 
 cv.circle(img, cntr, 3, (255, 0, 255), 2)
 p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])
 p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])

 angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians
 degree = angle * (180 / np.pi)

 drawAxis(img, cntr, p1, (0, 255, 0), 1,degree)
 drawAxis(img, cntr, p2, (255, 255, 0), 5,degree)

 
 return degree
parser = argparse.ArgumentParser(description='Code for Introduction to Principal Component Analysis (PCA) tutorial.\
 This program demonstrates how to use OpenCV PCA to extract the orientation of an object.')
parser.add_argument('--input', help='Path to input image.', default='pca_test1.jpg')
args = parser.parse_args()
src = cv.imread(cv.samples.findFile(args.input))


# Xác định các tọa độ của hình ảnh bạn muốn cắt
# x, y, width, height = 900, 900, 1200, 1000  # Ví dụ: cắt từ (100, 100) đến (300, 300)

# Thực hiện phép cắt
# src = image[y:y+height, x:x+width]

# Check if image is loaded successfully
if src is None:
 print('Could not open or find the image: ', args.input)
 exit(0)
# cv.imshow('src', src)
# Convert image to grayscale
gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)
# Convert image to binary
_, bw = cv.threshold(gray, 50, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
contours, _ = cv.findContours(bw, cv.RETR_LIST, cv.CHAIN_APPROX_NONE)
for i, c in enumerate(contours):
 # Calculate the area of each contour
 area = cv.contourArea(c)
 # Ignore contours that are too small or too large
 if area < 1e2 or 1e5 < area:
  continue
 # Draw each contour only for visualisation purposes
 cv.drawContours(src, contours, i, (0, 0, 255), 2)
 # Find the orientation of each shape
 getOrientation(c, src)
 cv.imwrite('orientation_image.jpg',src)
# cv.imshow('output', src)
# cv.waitKey()


///////////////////////////////////////////////xac dinh huong orientation.py

# from __future__ import print_function
# from __future__ import division
import cv2 as cv
import numpy as np
import argparse
from math import atan2, cos, sin, sqrt, pi
def drawAxis(img, p_, q_, colour, scale,degree):
 p = list(p_)
 q = list(q_)
 print("diemP: ",p)
 print("diemQ: ",q)
 text1 = f"({p[0]}, {p[1]})"
 
 cv.putText(img, text1, (p[0]+30,p[1]+30), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)
 angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians
 print("angle: ",degree)
 text2 = f"({degree})"
 cv.putText(img, text2, (p[0]+150,p[1]-30), cv.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)

 hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))
 # Here we lengthen the arrow by a factor of scale
 q[0] = p[0] - scale * hypotenuse * cos(angle)
 q[1] = p[1] - scale * hypotenuse * sin(angle)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 # create the arrow hooks
 p[0] = q[0] + 9 * cos(angle + pi / 4)
 p[1] = q[1] + 9 * sin(angle + pi / 4)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 p[0] = q[0] + 9 * cos(angle - pi / 4)
 p[1] = q[1] + 9 * sin(angle - pi / 4)
 cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 5, cv.LINE_AA)
 
def getOrientation(pts, img):
 
 sz = len(pts)
 data_pts = np.empty((sz, 2), dtype=np.float64)
 for i in range(data_pts.shape[0]):
  data_pts[i,0] = pts[i,0,0]
  data_pts[i,1] = pts[i,0,1]
 # Perform PCA analysis
 mean = np.empty((0))
 mean, eigenvectors, eigenvalues = cv.PCACompute2(data_pts, mean)
 # Store the center of the object
 cntr = (int(mean[0,0]), int(mean[0,1]))
 
 
 cv.circle(img, cntr, 3, (255, 0, 255), 2)
 p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])
 p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])

 angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians
 degree = angle * (180 / np.pi)

 drawAxis(img, cntr, p1, (0, 255, 0), 1,degree)
 drawAxis(img, cntr, p2, (255, 255, 0), 5,degree)

 
 return degree
parser = argparse.ArgumentParser(description='Code for Introduction to Principal Component Analysis (PCA) tutorial.\
 This program demonstrates how to use OpenCV PCA to extract the orientation of an object.')
parser.add_argument('--input', help='Path to input image.', default='pca_test1.jpg')
args = parser.parse_args()
src = cv.imread(cv.samples.findFile(args.input))


# Xác định các tọa độ của hình ảnh bạn muốn cắt
# x, y, width, height = 900, 900, 1200, 1000  # Ví dụ: cắt từ (100, 100) đến (300, 300)

# Thực hiện phép cắt
# src = image[y:y+height, x:x+width]

# Check if image is loaded successfully
if src is None:
 print('Could not open or find the image: ', args.input)
 exit(0)
# cv.imshow('src', src)
# Convert image to grayscale
gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)
# Convert image to binary
_, bw = cv.threshold(gray, 50, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
contours, _ = cv.findContours(bw, cv.RETR_LIST, cv.CHAIN_APPROX_NONE)
for i, c in enumerate(contours):
 # Calculate the area of each contour
 area = cv.contourArea(c)
 # Ignore contours that are too small or too large
 if area < 1e2 or 1e5 < area:
  continue
 # Draw each contour only for visualisation purposes
 cv.drawContours(src, contours, i, (0, 0, 255), 2)
 # Find the orientation of each shape
 getOrientation(c, src)
 cv.imwrite('orientation_image.jpg',src)
# cv.imshow('output', src)
# cv.waitKey()


//////////////////////////////////////////////



# ///////////////////////////////////////////////////////////

from ultralytics import YOLO
import cv2
import numpy as np

import argparse
from math import atan2, cos, sin, sqrt, pi
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

link = r'/content/nhandienvat/detectByYolov8/dataset_specialItems/train/images/20230922_154950_jpg.rf.a55265100fc54aa788b672e6a11b49f2.jpg'

image = cv2.imread(link)

def drawAxis(img, p_, q_, colour, scale,degree):
 p = list(p_)
 q = list(q_)
 print("diemP: ",p)
 print("diemQ: ",q)
 text1 = f"({p[0]}, {p[1]})"
 
#  cv.putText(img, text1, (p[0]+30,p[1]+30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)
 angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians
 print("angle: ",degree)
 text2 = f"{round(degree,2)} degree"
 cv2.putText(img, text2, (p[0]+10,p[1]-30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)

 hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))
 # Here we lengthen the arrow by a factor of scale
 q[0] = p[0] - scale * hypotenuse * cos(angle)
 q[1] = p[1] - scale * hypotenuse * sin(angle)
 cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv2.LINE_AA)
 # create the arrow hooks
 p[0] = q[0] + 9 * cos(angle + pi / 4)
 p[1] = q[1] + 9 * sin(angle + pi / 4)
 cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv2.LINE_AA)
 p[0] = q[0] + 9 * cos(angle - pi / 4)
 p[1] = q[1] + 9 * sin(angle - pi / 4)
 cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv2.LINE_AA)
 
def getOrientation(pts, img):
 sz = len(pts)
 data_pts = np.empty((sz, 2), dtype=np.float64)
 for i in range(data_pts.shape[0]):
  data_pts[i,0] = pts[i,0,0]
  data_pts[i,1] = pts[i,0,1]
 # Perform PCA analysis
 mean = np.empty((0))
 mean, eigenvectors, eigenvalues = cv2.PCACompute2(data_pts, mean)
 # Store the center of the object
 cntr = (int(mean[0,0]), int(mean[0,1]))
 
 
 cv2.circle(img, cntr, 2, (255, 0, 255), 1)
 p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])
 p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])
#  cv.circle(img, (eigenvectors[0,1], eigenvectors[0,0]), 2, (0, 0, 255), 1)
 angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians
 degree = angle * (180 / np.pi)

 drawAxis(img, cntr, p1, (0, 255, 0), 2,degree)
 drawAxis(img, cntr, p2, (255, 255, 0), 5,degree)
 return degree


# beta = -50  # Giảm độ sáng

# # Sử dụng hàm cv2.convertScaleAbs để giảm độ sáng của ảnh
# src = cv2.convertScaleAbs(image, alpha=1, beta=beta)


# # Check if image is loaded successfully
# if src is None:
# #  print('Could not open or find the image: ', args.input)
#  exit(0)
# # cv2.imshow('src', src)
# # Convert image to grayscale
# gray_picture = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)

# gray_inverted = cv2.bitwise_not(gray_picture)
# # Convert image to binary
# _, bw = cv2.threshold(gray_picture, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
# contours, _ = cv2.findContours(bw, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
# for i, c in enumerate(contours):
#  # Calculate the area of each contour
#  area = cv2.contourArea(c)
#  # Ignore contours that are too small or too large
#  if area < 1e2*3 or 1e5 < area:
#   continue
#  # Draw each contour only for visualisation purposes
#  cv2.drawContours(src, contours, i, (0, 0, 255), 2)
#  # Find the orientation of each shape
#  getOrientation(c, src)
#  cv2.imwrite('orientation_image.jpg',src)
# //////////////
# cv2.imshow('output', src)
# Assuming you have a grayscale image in the 'gray_picture' variable
# Normalize the grayscale values to be between 0 and 1

# plt.imshow(bw, cmap="gray")
# plt.title("Bitwise Gray")
# plt.axis("off")
# plt.show()

# plt.imshow(gray_inverted, cmap="gray")
# plt.title("gray_inverted Gray")
# plt.axis("off")
# plt.show()

# ///////////////////////////////////

# Load a model
model = YOLO('yolov8n-seg.pt')  # load an official model
model = YOLO(r'/content/nhandienvat/runs/segment/train/weights/best.pt')  # load a custom model

# Predict with the model
results = model(r"/content/nhandienvat/detectByYolov8/dataset_specialItems/train/images/20230922_154950_jpg.rf.a55265100fc54aa788b672e6a11b49f2.jpg", save = True)  # predict on an image


myList = []

mylistmask = []

checkitem = []
for r in results:
    # mylistmask = r.masks.xy #ssử dụng nếu dùng phương pháp tìm tọa độ tâm theo các điểm masks
    # print("mask: ",r.masks[0].xy[0])
    # print("shape: ",r.masks.shape)
    # print("masks: ",r.masks)
    print("boxes: ",r.boxes)  # print the Boxes object containing the detection bounding boxes
    myList = r.boxes.xywh.tolist()
    checkitem = r.boxes.cls.tolist()
    # print(int(checkitem[2]))
print("myList: ",myList)
print("length: ",len(myList))


# print(list)
count = 0

while count < len(checkitem):
  print("count: ",count)
  if(checkitem[count] == 3.0):
    print("3")
    bBoxitem = myList[count]
    mask = np.zeros_like(image, dtype=np.uint8)
#     # Định nghĩa màu xanh mà bạn muốn sử dụng (ví dụ: màu xanh lá cây)
#     green_color = ( 255,255, 0)  # Xanh lá cây: (B, G, R)

# # Gán giá trị màu xanh cho mặt nạ
#     mask[:, :] = green_color

    a = (bBoxitem[0] - bBoxitem[2]/2, bBoxitem[1] - bBoxitem[3]/2)
    b = (bBoxitem[0] + bBoxitem[2]/2, bBoxitem[1] - bBoxitem[3]/2)
    c = (bBoxitem[0] + bBoxitem[2]/2, bBoxitem[1] + bBoxitem[3]/2)
    d = (bBoxitem[0] - bBoxitem[2]/2, bBoxitem[1] + bBoxitem[3]/2)

    points = np.array([a, b,c ,d], dtype=np.int32)
    cv2.fillPoly(mask, [points], (255, 255, 255))
    src = cv2.bitwise_and(image, mask)
    beta = -50  # Giảm độ sáng
        # Sử dụng hàm cv2.convertScaleAbs để giảm độ sáng của ảnh
    result = cv2.convertScaleAbs(src, alpha=1, beta=beta)


    # Check if image is loaded successfully
    if result is None:
    #  print('Could not open or find the image: ', args.input)
        exit(0)
    # cv2.imshow('src', src)
    # Convert image to grayscale
    gray_picture = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)

    gray_inverted = cv2.bitwise_not(gray_picture)
    # Convert image to binary
    _, bw = cv2.threshold(gray_picture, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    # cv2.imwrite('bibi.jpg', bw)
    
    contours, _ = cv2.findContours(bw, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    for i, c in enumerate(contours):
    # Calculate the area of each contour
      area = cv2.contourArea(c)
    # Ignore contours that are too small or too large
      if area < 1e2*3 or 1e5 < area:
        continue
    # Draw each contour only for visualisation purposes
        cv2.drawContours(result, contours, i, (0, 0, 255), 2)
    # Find the orientation of each shape
      kq =  getOrientation(c, result)
      print("kq: ", kq)
  if checkitem[count] == 4.0:

    list = myList[count]
    point = (list[0],list[1])
    print("point :",point)
    color = (0, 255, 0)
    # Kích thước của điểm
    thickness = -1  # Đặt -1 để vẽ một điểm đầy đủ
    # Vẽ điểm trên hình ảnh
    cv2.circle(image, (int(list[0]),int(list[1])), 3, (0, 0, 255), thickness) #chấm điểm vào ảnh ở tọa độ tâm của bounding box của lỗ tâm
    # in tọa độ của tâm
    text = f"({int(list[0])}, {int(list[1])})"
    cv2.putText(image, text, (int(list[0])+30,int(list[1])+30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
    cv2.line(image, (int(list[0]),int(list[1])), (int(list[0]) + 50, int(list[1])), (0, 0, 255), 2)  # Vẽ trục X (màu đỏ)
    cv2.line(image, (int(list[0]),int(list[1])), (int(list[0]), int(list[1]) + 50), (0, 255, 0), 2)  # Vẽ trục Y (màu xanh lá)
  count = count + 1
# print("list masks: ",mylistmask.tolist()[0])



# ///////////////////Cách lấy tâm từ các điểm masks/////////////
# count = 0
# while count < len(mylistmask):
#   if checkitem[count] == 4.0:
#     print("count: ", count)
#     print(mylistmask[count].tolist())
#     listitem = mylistmask[count].tolist()
#     count2 = 0
#     totalx= 0
#     totaly = 0
#     x = 0
#     y = 0
    
#     while count2 < len(listitem):
#       print(listitem[count2])
#       coodinate = listitem[count2]
#       totalx = totalx + coodinate[0]
#       totaly = totaly + coodinate[1]
#       # cv2.circle(image, (int(coodinate[0]),int(coodinate[1])), 3, color, thickness)
#       cv2.drawMarker(image, (int(coodinate[0]),int(coodinate[1])), color, markerType=cv2.MARKER_STAR, markerSize=2)
#       count2 = count2 + 1
#     x = totalx / len(listitem)
#     y = totaly / len(listitem)
#     cv2.drawMarker(image, (int(x),int(y)), color, markerType=cv2.MARKER_STAR, markerSize=2)
#   count = count + 1


cv2.imwrite('output_image2.jpg', image)
cv2.imwrite('output_image3.jpg', result)


#Kết luận rằng lấy tâm của bounding box của lỗ tâm chuẩn hơn cách lấy tâm từ các điểm masks của lỗ tâm 


////////////////tim tam

Thực hiện tiền xử lí lần lượt bằng các thuật toán: Constrast Stretch để
chiều chỉnh lại độ tương phản, Threshold để có được ảnh binary, tìm 
cạnh bằng Canny Edge, và những thuật toán như dilation, find 
Contours.
Trong đó, thuật toán Constrast Strect, Threshold, find Contours đã 
được đề cập ở các phần bên trên
Thuật toán Canny Edge có thể xem thêm ở phần phụ lục 1.3
Thuật toán Dilation:
- Đây là thuật toán bổ trợ thêm cho thuật toán Canny Edges nhằm 
làm tăng chất lượng các cạnh trước khi đưa vào thuật toán Find 
Contours. 
- Dilation là một trong những phép biến đổi thao tác tác động đến 
hình thái của vật thể trong ảnh. Thông qua 1 Kernel có kích thước 
được chỉnh định từ ban đầu. Nó sẽ thực hiện tìm pixel có giá trị
lớn nhất sau đó gán toàn bộ các pixel nằm trong vùng Kernel đó 
bằng chính giá trị lớn nhất đó.
- Điều này đồng nghĩa với việc, nó sẽ giúp cho hình dạng của vật 
thể, của cạnh nó sẽ được phình to ra hơn và có khả năng liên kết 
được những cạnh rời rạc ở một mức độ nào đó.
- Thuật toán này được sử dụng nhằm mục đích liên kết lại các cạnh 
thu được từ thuật toán Canny để đảm bảo rằng sẽ không có cạnh 
nào bị đứt khúc giữa chừng.

Thuật toán tìm tâm của con hàng
Sau khi trích xuất được RoI dựa trên kết quả từ YOLO, ta thực hiện áp 
dụng 1 số thuật toán xử lí ảnh nhằm mục đích thu được ảnh binary mà 
chỉ chứa các cạnh bên trong

Để thực hiện được điều đó, lần lượt đưa RoI qua các thuật toán theo thứ
tự Contrast stretching, thresholding, canny edge detection, dilation
Sau khi có được ảnh đầu ra mong muốn, ta thực hiện tìm contour của 
RoI, tuy nhiên vì số lượng contour trả về sẽ lớn hơn 1. Vì thế mà ta sẽ
có thêm 1 vài bước lọc bớt các contour không phù hợp.
Sau khi xử lí xong và thu được contour tối ưu, ta thực hiện tìm đường 
tròn có bán kính nhỏ nhất mà chứa tất cả các điểm trong contour tối ưu, 
sau đó thu được tâm của đường tròn. Đồng thời, ta cũng thực hiện tính 
toán tâm của contour và kết hợp tính trung bình với tâm của đường tròn 
thu được lúc trước để trả về giá trị tọa độ pixel của tâm vật hàng tốt 
nhất
///////////////////////////////////
 Tìm góc tối ưu
 Sử dụng Template Matching cho việc tinh chỉnh lại góc của con 
hàng
Tiến hành tạo Template trước:

Sau khi có được template trên, ta sẽ tiến hành xoay template một giá trị
bằng α + góc đề xuất (α sẽ nằm trong khoảng từ -50 đến 50, giá trị này có thể thay đổi tùy thuộc vào việc mô hình Segmentation có cần phải 
được tối ưu nhiều hay không).

Sau đó ta sẽ tiến hành matching template đã được xoay ở trên với một 
vùng roi đã được chuyển sang dạng binary, và ta lựa chọn có giá trị góc 
tối ưu hơn dựa trên độ tương đồng giữa template và RoI

 Tối ưu thuật toán
Ý tưởng triển khai:
- Hạn chế của thuật toán hiện tại là sự lặp lại việc tính toán mặc dù 
có thể góc đề xuất đã là góc tối ưu
- Giả định rằng, nếu góc đề xuất có thể được tinh chỉnh và cho kết 
quả tốt hơn, tức là giá trị tương đồng giữa Template và RoI sẽ cao 
hơn. Điều đó đồng nghĩa với việc, nếu chúng ta kiểm tra xem liệu 
với góc tinh chỉnh đầu tiên giá trị tương đồng (Similarity score) 
trả về có cao hơn so với độ tương đồng khi tính với góc đề xuất từ
Segmentation, thì ta sẽ cho phép việc tinh chỉnh tiếp tục xảy ra.
- Tuy nhiên, nếu khi đến 1 độ tinh chỉnh nào đó, giá trị similarity trả
về thấp hơn so với giá trị similarity của góc tinh chỉnh trước đó. 
Việc tinh chỉnh sẽ bị dừng ngay lập tức.
- Để cân bằng việc sai lệch theo chiều âm hoặc dương, ta sẽ thực 
hiện tính toán cùng lúc độ tinh chỉnh theo chiều dương và độ tinh 
chỉnh theo chiều âm. Vòng lặp sẽ kết thúc khi cả 2 bên tinh chỉnh 
âm và dương đều không thể tinh chỉnh được góc đề xuất sang góc 
tối ưu được nữa
 Sau đó ta sẽ thực hiện so sánh các kết quả cuối cùng của cả 2 bên 
so với similarity score của góc đề xuất từ Segmentation để chọn 
ra góc tối ưu nhất cho vật thể.

Lựa chọn thứ tự gắp con hàng
 Tổng quát
Đây là bước được thực hiện sau khi đã qua thuật toán xử lí ảnh và đã cho 
ra được tọa độ các vật hàng nhận diện được.
Ở bước này, ta sẽ thực hiện xác định mức độ khả thi có thể gắp được của 
vật hàng, đây là bước được dùng để tối ưu hiệu suất và giảm thiểu mức độ
rủi ro khi gắp hàng.
 Vấn đề và giải pháp
Vấn đề: 
- Trong phạm vi của đồ án, các vật hàng sẽ ở vị trí ngẫu nhiên và không 
biết trước được vị trí cụ thể. Ngoại trừ các trường hợp các vật hàng 
chồng chéo nhau, nằm nghiêng theo chiều đứng hoặc ngang và nằm úp 
sẽ bị loại bỏ và không thực hiện gắp, còn lại tất cả các con hàng nằm 
ngửa dù là gần nhau đến dính vào nhau thì vẫn sẽ được gắp để đảm 
bảo số lượng tính theo phút.
- Từ đó sẽ phát sinh ra vấn đề là khi Robot thực hiện gắp vật, có khả
năng sẽ gây tác động nhẹ đến vật hàng đang gắp. Nếu vật hàng đó nằm 
độc lập không gần các vật hàng khác, điều này sẽ không phải đáng lo 
ngại. Nhưng nếu vật hàng đang gắp có các vật hàng khác gần đó thậm 
chí là dính vào, việc tác động như vậy có khả năng làm dịch chuyển vị
trí của các vật hàng bên cạnh.
- Trong khi đó, ứng với mỗi lần nhận diện các vật hàng, thì Robot sẽ
nhận toàn bộ các tọa độ của vật hàng có thể gắp được. Việc làm thay 
đổi đi tọa độ của vật hàng bên cạnh sẽ tác động không hề nhỏ đến khả
năng gắp vật sau này của Robot.
- Chính vì thế mà một độ đo mức độ khả thi được áp dụng để xác định 
liệu vật hàng đó có bao nhiêu phần trăm là khả thi để gắp được mà ít 
gây ảnh hưởng đến các vật hàng xung quanh.
Ý tưởng:
- Chúng ta sẽ tận dụng 2 thuật toán xử lí ảnh là Contrast stretch và 
thresholding để thực hiện lấy ảnh binary của cả Template và RoI.
Tuy nhiên, đối với bước này, ta sẽ không cắt RoI bằng với Bounding 
Box từ YOLO mà sẽ nới rộng RoI ra lớn hơn (Padding 100 pixel chỗ
mỗi phía của RoI) nhằm mục đích có thể bao trọn các khu vực xung 
quanh của vật hàng hơn.
- Sau đó ta thực hiện tính tổng số pixel mà mỗi pixel được coi là thuộc 
vật hàng, trong trường hợp cụ thể như hiện tại, pixel nào có giá trị bằng 
0 thì được xem như thuộc vật hàng. Ta sẽ tính tổng số lượng pixel đó 
lại và xem nó như giá trị đại diện về độ đậm đặc của Template hay RoI 
(gọi là Intensity).
- Như trên hình 3.27, đối với các con hàng bị bao quanh bởi các con 
hàng khác, thì vùng pixel bằng 0 sẽ nhiều hơn so với Template
- Tiếp theo ta thực hiện tính toán mức độ khả thi có thể gắp được theo 
công thức:
𝑃 =
σ(𝑇)
σ(𝑅𝑜𝐼)
∗ 100
 Trong đó:
 P là giá trị mức độ khả thi có thể gắp được
 𝜎(𝑇) là Intensity của Template
 𝜎(𝑅𝑜𝐼) là Intensity của RoI
- Bằng cách này, khi RoI có chứa các vật hàng kế bên, thì sẽ có các pixel 
thuộc vật hàng đó nằm trong RoI, điều này sẽ dẫn đến Intensity của 
RoI sẽ được tăng lên cao hơn và P sẽ giảm xuống.
- Sau đó ta thực hiện sắp xếp lại theo mức độ giảm dần giá trị P tương 
ứng với mức độ ưu tiên tăng dần và với các con hàng có giá trị P thấp 
hơn ngưỡng quy định sẽ được bỏ qua và không gắp để tránh trường 
hợp vật đó bị xê dịch đi quá nhiều bởi những con hàng khác lúc gắp
- Tuy nhiên, độ đo này nó không hoàn toàn thật sự tìm được vật hàng 
nào ưu tiên nhất, vì cơ bản từ bước tính Intensity thì nó đã phụ thuộc 
rất nhiều vào chất lượng của đầu ra thuật toán xử lí ảnh, giả sử trong 
trường hợp đầu ra nó bị nhiễu bởi Background, thì Intensity nó không 
hoàn toàn đại diện được cho ảnh. Ngoài ra, vật hàng trong RoI cũng 
chưa hoàn toàn sẽ giống với Template vì sẽ bị khác góc chụp nếu nằm 
xa tâm Camera.
- Chính vì thế mà độ đo này chỉ thật sự được sử dụng để giảm bớt khả
năng Robot sẽ tác động vào những vật hàng dính với nhau làm sai lệch 
đi tọa độ thực tế
 Các số được đánh dấu cho từng vật hàng nằm ngửa thu được đại diện 
cho thứ tự mà Robot sẽ gắp, có thể thấy vật hàng số 7, 8, 9, 10 là các 
vật hàng được gắp sau cùng vì có hiện tượng dính với nhau.
- Đối với độ đo này, ta chỉ thật sự quan tâm đến các vật hàng dính vào 
nhau, còn những vật hàng rời rạc chúng ta sẽ không quan tâm việc nó 
có bị xê dịch trong quá trình gắp hay không.


////////////////Pseudo code lay goc toi uu
- Hạn chế của thuật toán hiện tại là sự lặp lại việc tính toán mặc dù 
có thể góc đề xuất đã là góc tối ưu
- Giả định rằng, nếu góc đề xuất có thể được tinh chỉnh và cho kết 
quả tốt hơn, tức là giá trị tương đồng giữa Template và RoI sẽ cao 
hơn. Điều đó đồng nghĩa với việc, nếu chúng ta kiểm tra xem liệu 
với góc tinh chỉnh đầu tiên giá trị tương đồng (Similarity score) 
trả về có cao hơn so với độ tương đồng khi tính với góc đề xuất từ
Segmentation, thì ta sẽ cho phép việc tinh chỉnh tiếp tục xảy ra.
- Tuy nhiên, nếu khi đến 1 độ tinh chỉnh nào đó, giá trị similarity trả
về thấp hơn so với giá trị similarity của góc tinh chỉnh trước đó. 
Việc tinh chỉnh sẽ bị dừng ngay lập tức.
- Để cân bằng việc sai lệch theo chiều âm hoặc dương, ta sẽ thực 
hiện tính toán cùng lúc độ tinh chỉnh theo chiều dương và độ tinh 
chỉnh theo chiều âm. Vòng lặp sẽ kết thúc khi cả 2 bên tinh chỉnh 
âm và dương đều không thể tinh chỉnh được góc đề xuất sang góc 
tối ưu được nữa

- Sau đó ta sẽ thực hiện so sánh các kết quả cuối cùng của cả 2 bên 
so với similarity score của góc đề xuất từ Segmentation để chọn 
ra góc tối ưu nhất cho vật thể.

Input
    source image, TemplateImage
    Box, angle
Output
    Point( with maximal similarity score)

Function matchTemplate(sourceImage, templateImage, box, angle)
    SET x,y,wR,hR <- box
    CAll rotateTemplate(TemplateImage, angle) RETURN rotatedTemplate, mask, wT, hT
    SET RoI <- 𝑠𝑜𝑢𝑟𝑐𝑒𝐼𝑚𝑎𝑔𝑒 𝑦 ∶ 𝑦 + ℎ𝑅, 𝑥 ∶ 𝑥 + 𝑤𝑅 
    CA𝑳𝑳 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑅𝑜𝐼(𝑅𝑜𝐼, 𝑟𝑜𝑡𝑎𝑡𝑒𝑑𝑇𝑒𝑚𝑝𝑙𝑎𝑡𝑒) 𝑹𝑬𝑻𝑼𝑹𝑵 𝑝𝑎𝑑𝑑𝑒𝑑𝑅𝑜𝐼
    𝑺𝑬𝑻 𝑐𝑜𝑟𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛𝑀𝑎𝑝 ← 𝑎𝑟𝑟𝑎𝑦 [𝑝𝑎𝑑𝑑𝑒𝑑𝑅𝑜𝐼. 𝑆ℎ𝑎𝑝𝑒 ]

    𝑭𝑶𝑹 𝑖 𝑭𝑹𝑶𝑴 𝑝𝑎𝑑𝑑𝑒𝑑𝑅𝑜𝐼. 𝐻𝑒𝑖𝑔ℎ𝑡
        𝑭𝑶𝑹 𝑗 𝑭𝑹𝑶𝑴 𝑝𝑎𝑑𝑑𝑒𝑑𝑅𝑜𝐼. 𝑊𝑖𝑑𝑡ℎ
            𝑺𝑬𝑻 𝑠𝑢𝑏𝑅𝑜𝐼 ← 𝑝𝑎𝑑𝑑𝑒𝑑𝑅𝑜𝐼[ 𝑖 ∶ 𝑖 + ℎ𝑇,𝑗 ∶ 𝑗 + 𝑤𝑇 ]
            𝑪𝑨𝑳𝑳 𝑐𝑜𝑚𝑝𝑢𝑡𝑒𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦(𝑠𝑢𝑏𝑅𝑜𝐼, 𝑟𝑜𝑡𝑎𝑡𝑒𝑑𝑇𝑒𝑚𝑝𝑙𝑎𝑡𝑒) 𝑹𝑬𝑻𝑼𝑹𝑵 𝑐𝑜𝑟𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛𝑆𝑐𝑜𝑟𝑒
            𝑨𝑫𝑫 𝑐𝑜𝑟𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛𝑆𝑐𝑜𝑟𝑒 → 𝑐𝑜𝑟𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛𝑀𝑎𝑝 𝑖,𝑗 
        𝑬𝑵𝑫𝑭𝑶𝑹
    𝑬𝑵𝑫𝑭𝑶R
    S𝑬𝑻 𝑃𝑜𝑖𝑛𝑡 ← 𝑚𝑎𝑥𝑙𝑜𝑐(𝑐𝑜𝑟𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛𝑀𝑎𝑝)

𝑹𝑬𝑻𝑼𝑹𝑵 𝑃𝑜𝑖𝑛t



Input:
    o 𝑠𝑜𝑢𝑟𝑐𝑒𝐼𝑚𝑎𝑔𝑒, 𝑡𝑒𝑚𝑝𝑙𝑎𝑡𝑒𝐼𝑚𝑎𝑔𝑒
    o 𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑎𝑡𝑖𝑜𝑛𝑅𝑒𝑠𝑢𝑙𝑡
    o 𝑚𝑖𝑛𝑀𝑜𝑑𝑖𝑓𝑦, 𝑚𝑎𝑥𝑀𝑜𝑑𝑖𝑓𝑦
Output:
    o 𝑔𝑜𝑜𝑑𝑃𝑜𝑖𝑛𝑡�

SET 𝑚𝑖𝑛𝑢𝑠𝑀𝑜𝑑𝑖𝑓𝑦𝐴𝑛𝑔𝑙𝑒 ← 𝑎𝑟𝑟𝑎𝑦𝑅𝑎𝑛𝑔𝑒[ −1, 𝑚𝑖𝑛𝑀𝑜𝑑𝑖𝑓𝑦, −1] 
SET 𝑝𝑙𝑢𝑠𝑀𝑜𝑑𝑖𝑓𝑦𝐴𝑛𝑔𝑙𝑒 ← 𝑎𝑟𝑟𝑎𝑦𝑅𝑎𝑛𝑔𝑒[ 1, 𝑚𝑎𝑥𝑀𝑜𝑑𝑖𝑓𝑦, 1 ]

SET 𝑔𝑜𝑜𝑑𝑃𝑜𝑖𝑛𝑡𝑠 ← 𝑒𝑚𝑝𝑡𝑦𝐿𝑖𝑠𝑡[]

CONVERT 𝑠𝑜𝑢𝑟𝑐𝑒𝐼𝑚𝑎𝑔𝑒 → 𝑔𝑟𝑎𝑦𝑆𝑐𝑎𝑙𝑒 𝑨𝑺 𝑖𝑚𝑔
CONVERT 𝑡𝑒𝑚𝑝𝑙𝑎𝑡𝑒𝐼𝑚𝑎𝑔𝑒 → 𝑔𝑟𝑎𝑦𝑆𝑐𝑎𝑙𝑒 𝑨𝑺 𝑡𝑒𝑚𝑝�

FOR 𝑏𝑜𝑥, 𝑎𝑛𝑔𝑙𝑒 𝑭𝑹𝑶𝑴 𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑎𝑡𝑖𝑜𝑛𝑅𝑒𝑠𝑢𝑙s
    ARRAY 𝑚𝑖𝑛𝑢𝑠𝑆𝑢𝑏𝐴𝑛𝑔𝑙𝑒𝑠 ← 𝑎𝑛𝑔𝑙𝑒 + 𝑚𝑖𝑛𝑢𝑠𝑀𝑜𝑑𝑖𝑓𝑦𝐴𝑛𝑔𝑙𝑒
    ARRAY 𝑝𝑙𝑢𝑠𝑆𝑢𝑏𝐴𝑛𝑔𝑙𝑒𝑠 ← 𝑎𝑛𝑔𝑙𝑒 + 𝑝𝑙𝑢𝑠𝑀𝑜𝑑𝑖𝑓𝑦𝐴𝑛𝑔𝑙𝑒
    SET 𝑚𝑖𝑛𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑒𝑟, 𝑝𝑙𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑒𝑟 ← 0, 0
    SET 𝑚𝑖𝑛𝑢𝑠𝐶ℎ𝑒𝑐𝑘, 𝑝𝑙𝑢𝑠𝐶ℎ𝑒𝑐𝑘 ← 𝑭𝑨𝑳𝑺𝑬,𝑭𝑨𝑳𝑺𝑬
    SET 𝑠𝑢𝑏𝑀𝑖𝑛𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑠, 𝑠𝑢𝑏𝑃𝑙𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑠 ← 𝑒𝑚𝑝𝑡𝑦𝐿𝑖𝑠𝑡[], 𝑒𝑚𝑝𝑡𝑦𝐿𝑖𝑠𝑡[]
    SET 𝑡ℎ𝑒 𝑙𝑎𝑠𝑡 𝑒𝑙𝑒𝑚𝑒𝑛𝑡 𝑜𝑓 𝑠𝑢𝑏𝑀𝑖𝑛𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑠 𝑨𝑺 𝑚𝑖𝑛𝑢𝑠𝐿𝑃
    SET 𝑡ℎ𝑒 𝑙𝑎𝑠𝑡 𝑒𝑙𝑒𝑚𝑒𝑛𝑡 𝑜𝑓 𝑠𝑢𝑏𝑃𝑙𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑠 𝑨𝑺 𝑝𝑙𝑢𝑠𝐿𝑃
    𝑪𝑨𝑳𝑳 𝑚𝑎𝑡𝑐ℎ𝑇𝑒𝑚𝑝𝑙𝑎𝑡𝑒(𝑖𝑚𝑔,𝑡𝑒𝑚𝑝𝑙, 𝑏𝑜𝑥, 𝑎𝑛𝑔𝑙𝑒) 𝑹𝑬𝑻𝑼𝑹𝑵 𝑑𝑒𝑓𝑎𝑢𝑙𝑡𝑃𝑜𝑖𝑛𝑡 𝑨𝑺 P

𝑾𝑯𝑰𝑳𝑬 𝑚𝑖𝑛𝑢𝑠𝐶ℎ𝑒𝑐𝑘 = 𝑭𝑨𝑳𝑺𝑬 𝑨𝑵𝑫 𝑝𝑙𝑢𝑠𝐶ℎ𝑒𝑐𝑘 = 𝑭𝑨𝑳𝑺𝑬
    CALL
    𝑚𝑎𝑡𝑐ℎ𝑇𝑒𝑚𝑝𝑙𝑎𝑡𝑒(𝑖𝑚𝑔,𝑡𝑒𝑚𝑝𝑙, 𝑏𝑜𝑥, 𝑚𝑖𝑛𝑢𝑠𝑆𝑢𝑏𝐴𝑛𝑔𝑙𝑒𝑠 𝑚𝑖𝑛𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑒𝑟 )𝑹𝑬𝑻𝑼𝑹𝑵𝑚𝑖𝑛𝑢𝑠𝑃𝑜𝑖𝑛𝑡
    CALL 𝑚𝑎𝑡𝑐ℎ𝑇𝑒𝑚𝑝𝑙𝑎𝑡𝑒(𝑖𝑚𝑔,𝑡𝑒𝑚𝑝𝑙, 𝑏𝑜𝑥, 𝑝𝑙𝑢𝑠𝑆𝑢𝑏𝐴𝑛𝑔𝑙𝑒𝑠 𝑝𝑙𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑒𝑟 ) 𝑹𝑬𝑻𝑼𝑹𝑵 𝑝𝑙𝑢𝑠𝑃𝑜𝑖𝑛𝑡
    𝑰𝑭 𝑚𝑖𝑛𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑒𝑟 = 0
    𝑚𝑖𝑛𝑢𝑠𝐶ℎ𝑒𝑐𝑘 ← 𝑩𝑶𝑶𝑳(𝑠𝑐𝑜𝑟𝑒𝑜𝑓(𝑚𝑖𝑛𝑢𝑠𝑃𝑜𝑖𝑛𝑡) < 𝑠𝑐𝑜𝑟𝑒𝑜𝑓(𝑃))
    𝑬𝑳𝑺𝑬
    𝑚𝑖𝑛𝑢𝑠𝐶ℎ𝑒𝑐𝑘 ← 𝑩𝑶𝑶𝑳(𝑠𝑐𝑜𝑟𝑒𝑜𝑓(𝑚𝑖𝑛𝑢𝑠𝑃𝑜𝑖𝑛𝑡) < 𝑠𝑐𝑜𝑟𝑒𝑜𝑓(𝑚𝑖𝑛𝑢𝑠𝐿𝑃))
    𝑬𝑵𝑫𝑰F
    𝑰𝑭 𝑵𝑶𝑻 𝑚𝑖𝑛𝑢𝑠𝐶ℎ𝑒𝑐𝑘
    𝑨𝑫𝑫 𝑚𝑖𝑛𝑢𝑠𝑃𝑜𝑖𝑛𝑡 → 𝑠𝑢𝑏𝑀𝑖𝑛𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑠
    𝑚𝑖𝑛𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑒𝑟 ← 𝑚𝑖𝑛𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑒𝑟 + 1
    ENDIF
    𝑰𝑭 𝑝𝑙𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑒𝑟 = 0
    𝑝𝑙𝑢𝑠𝐶ℎ𝑒𝑐𝑘 ← 𝑩𝑶𝑶𝑳(𝑠𝑐𝑜𝑟𝑒𝑜𝑓(𝑝𝑙𝑢𝑠𝑃𝑜𝑖𝑛𝑡) < 𝑠𝑐𝑜𝑟𝑒𝑜𝑓(𝑃))
    𝑬𝑳𝑺𝑬
    𝑝𝑙𝑢𝑠𝐶ℎ𝑒𝑐𝑘 ← 𝑩𝑶𝑶𝑳(𝑠𝑐𝑜𝑟𝑒𝑜𝑓(𝑝𝑙𝑢𝑠𝑃𝑜𝑖𝑛𝑡) < 𝑠𝑐𝑜𝑟𝑒𝑜𝑓(𝑝𝑙𝑢𝑠𝐿𝑃))
    𝑬𝑵𝑫𝑰𝑭
    𝑰𝑭 𝑵𝑶𝑻 𝑝𝑙𝑢𝑠𝐶ℎ𝑒𝑐𝑘
    𝑨𝑫𝑫 𝑝𝑙𝑢𝑠𝑃𝑜𝑖𝑛𝑡 → 𝑠𝑢𝑏𝑃𝑙𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑠
    𝑝𝑙𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑒𝑟 ← 𝑝𝑙𝑢𝑠𝑃𝑜𝑖𝑛𝑡𝑒𝑟 + 1
    𝑬𝑵𝑫𝑰�
    E𝑵𝑫𝑾𝑯𝑰𝑳𝑬
    𝑏𝑒𝑠𝑡𝑃𝑜𝑖𝑛𝑡 ← 𝑚𝑎𝑥𝑝𝑜𝑖𝑛𝑡
    (𝑠𝑐𝑜𝑟𝑒𝑜𝑓 𝑃, 𝑚𝑖𝑛𝑢𝑠𝐿𝑃, 𝑝𝑙𝑢𝑠𝐿𝑃 )
    𝑨𝑫𝑫 𝑏𝑒𝑠𝑡𝑃𝑜𝑖𝑛𝑡 → 𝑔𝑜𝑜𝑑𝑃𝑜𝑖𝑛𝑡𝑠
ENDFOR
//////////////////////////////////////////////////////////////////////////

using System;
using System.Collections.Generic;
using System.ComponentModel;
using System.Data;
using System.Drawing;
using System.IO;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using System.Windows.Forms;
using System.Diagnostics;
using System.Globalization;
using System.Drawing.Imaging;
using System.Net.Http;
using System.Net.Sockets;
using System.Net;
using System.Threading;

using NeptuneC_Interface;
using System.Security.AccessControl;
using System.Drawing.Drawing2D;
using static System.Windows.Forms.VisualStyles.VisualStyleElement;

namespace WinFormCVU
{
    public partial class ComputerVisionUI : Form
    {
        private NeptuneC_Interface.NeptuneCUnplugCallback DeviceUnplugCallbackInst;
        private NeptuneC_Interface.NeptuneCFrameCallback FrameCallbackInst;

        string targetImagePath;
        string templatePath;

        Stopwatch stopwatch = new Stopwatch();

        string defaultDirectory = $"{Application.StartupPath}\\";
        string stream_folder = "Stream_camera";
        string output_folder = "Output";
        string template_folder = "Template";

        // these are all variables used in RoI selection 
        private bool isSelecting = false;
        private Rectangle rect;
        private Point startPoint;
        private int cornerSize = 5;
        private Rectangle selectedRect;
        private bool isReshape = false;

        // this is a variable used in zoom in or zoom out
        private bool isResizing = false;

        private TcpListener serverSocket;
        private Thread serverThread;
        private bool isRunning;
        private TcpListener serverSocket2;
        private Thread serverThread2;
        private bool isRunning2;
        private TcpListener serverSocket3;
        private Thread serverThread3;
        private bool isRunning3;
        private List<float[]> sharedData = new List<float[]>();
        private TaskCompletionSource<bool> matchTemplateCompletionSource;

        // this is a variable used in camera connection
        public IntPtr m_pCameraHandle = IntPtr.Zero;
        private NEPTUNE_FEATURE[] m_arrFeatureInfo;
        private CheckBox[] m_arrCheckBox;
        private ENeptuneFeature[] m_arrMapping = new ENeptuneFeature[]
        {
            ENeptuneFeature.NEPTUNE_FEATURE_GAMMA,
            ENeptuneFeature.NEPTUNE_FEATURE_GAIN,
            ENeptuneFeature.NEPTUNE_FEATURE_RGAIN,
            ENeptuneFeature.NEPTUNE_FEATURE_GGAIN,
            ENeptuneFeature.NEPTUNE_FEATURE_BGAIN,
            ENeptuneFeature.NEPTUNE_FEATURE_AUTOGAIN_MIN,
            ENeptuneFeature.NEPTUNE_FEATURE_AUTOGAIN_MAX,
            ENeptuneFeature.NEPTUNE_FEATURE_SHUTTER,
            ENeptuneFeature.NEPTUNE_FEATURE_AUTOSHUTTER_MIN,
            ENeptuneFeature.NEPTUNE_FEATURE_AUTOSHUTTER_MAX,
            ENeptuneFeature.NEPTUNE_FEATURE_AUTOEXPOSURE,
            ENeptuneFeature.NEPTUNE_FEATURE_BLACKLEVEL,
            ENeptuneFeature.NEPTUNE_FEATURE_CONTRAST,
            ENeptuneFeature.NEPTUNE_FEATURE_HUE,
            ENeptuneFeature.NEPTUNE_FEATURE_SATURATION,
            ENeptuneFeature.NEPTUNE_FEATURE_SHARPNESS,
            ENeptuneFeature.NEPTUNE_FEATURE_TRIGNOISEFILTER,
            ENeptuneFeature.NEPTUNE_FEATURE_BRIGHTLEVELIRIS,
            ENeptuneFeature.NEPTUNE_FEATURE_SNOWNOISEREMOVE,
            ENeptuneFeature.NEPTUNE_FEATURE_OPTFILTER,
            ENeptuneFeature.NEPTUNE_FEATURE_PAN,
            ENeptuneFeature.NEPTUNE_FEATURE_TILT,
            ENeptuneFeature.NEPTUNE_FEATURE_LCD_BLUE_GAIN,
            ENeptuneFeature.NEPTUNE_FEATURE_LCD_RED_GAIN,
            ENeptuneFeature.NEPTUNE_FEATURE_WHITEBALANCE
        };

        public ComputerVisionUI()
        {
            InitializeComponent();
        }

        private void ComputerVisionUI_Load(object sender, EventArgs e)
        {
            Template_box.AllowDrop = true;
            Image_box.AllowDrop = true;
            Similarity_score.Text = "0.95";
            Overlap.Text = "0.4";
            Min_modify.Text = "-20";
            Max_modify.Text = "20";
            Scale_ratio.Text = "20";
            Method_similarity.Text = "cv2.TM_CCORR_NORMED";
            conf_score.Text = "0.75";
            img_size.Text = "750";
            serverIP.Text = "192.168.176.1";

            Cam_capture.Enabled = false;
            Match_template.Enabled = false;
            add_template.Enabled = false;
            down_scale.Enabled = false;
            up_scale.Enabled = false;
            Elasped_time.ReadOnly = true;
            CVU_status.ReadOnly = true;
            serverStatus.ReadOnly = true;

            InitCameraList();
        }

        private void ComputerVisionUI_FormClosing(object sender, FormClosingEventArgs e)
        {
            CloseCameraHandle();
            NeptuneC.ntcUninit();

            DisconnectServer();
        }

        public class ItemData
        {
            public String m_strLabel = "";
            public Int32 m_nValue = 0;
            public NEPTUNE_CAM_INFO m_stCameraInfo;

            public ItemData(String strLabel, Int32 nValue)
            {
                m_strLabel = strLabel;
                m_nValue = nValue;
            }

            public ItemData(NEPTUNE_CAM_INFO stCameraInfo)
            {
                m_stCameraInfo = stCameraInfo;
                m_strLabel = m_stCameraInfo.strVendor + ": [" + m_stCameraInfo.strSerial + "] " + m_stCameraInfo.strModel;
            }

            public override String ToString()
            {
                return m_strLabel;
            }
        };

        private void CloseCameraHandle()
        {
            if (m_pCameraHandle != IntPtr.Zero)
            {
                NeptuneC.ntcClose(m_pCameraHandle);
                m_pCameraHandle = IntPtr.Zero;
            }
        }

        private void My_Refresh_Click(object sender, EventArgs e)
        {
            InitCameraList();
        }

        private void InitCameraList()
        {
            NeptuneC.ntcInit();
            ItemData CurSelItem = (ItemData)m_cbCameraList.SelectedItem;

            m_cbCameraList.Items.Clear();

            UInt32 uiCount = 0;
            if (NeptuneC.ntcGetCameraCount(ref uiCount) == ENeptuneError.NEPTUNE_ERR_Success)
            {
                if (uiCount > 0)
                {
                    NEPTUNE_CAM_INFO[] pCameraInfo = new NEPTUNE_CAM_INFO[uiCount];
                    ENeptuneError emErr = NeptuneC.ntcGetCameraInfo(pCameraInfo, uiCount);
                    if (emErr == ENeptuneError.NEPTUNE_ERR_Success)
                    {
                        for (UInt32 i = 0; i < uiCount; i++)
                        {
                            Int32 nItem = m_cbCameraList.Items.Add(new ItemData(pCameraInfo[i]));
                            if (CurSelItem != null)
                            {
                                if (((ItemData)m_cbCameraList.Items[nItem]).m_stCameraInfo.strSerial.Equals(CurSelItem.m_stCameraInfo.strSerial) == true)
                                {
                                    m_cbCameraList.SelectedIndex = nItem;
                                }
                            }
                        }
                    }
                }
            }

            if (CurSelItem != null && m_cbCameraList.SelectedIndex == -1)
            {
                CloseCameraHandle();
            }
        }

        private void m_cbCameraList_SelectedIndexChanged(object sender, EventArgs e)
        {
            CloseCameraHandle();

            if (m_cbCameraList.SelectedIndex != -1)
            {
                ENeptuneError emErr = NeptuneC.ntcOpen(((ItemData)m_cbCameraList.SelectedItem).m_stCameraInfo.strCamID, ref m_pCameraHandle, ENeptuneDevAccess.NEPTUNE_DEV_ACCESS_EXCLUSIVE);
                if (emErr == ENeptuneError.NEPTUNE_ERR_Success)
                {
                    //NeptuneC.ntcSetDisplay(m_pCameraHandle, Image_box.Handle);
                    NeptuneC.ntcSetUnplugCallback(m_pCameraHandle, DeviceUnplugCallbackInst, this.Handle);
                    NeptuneC.ntcSetFrameCallback(m_pCameraHandle, FrameCallbackInst, this.Handle);
                }

                _ = NeptuneC.ntcSetAcquisition(m_pCameraHandle, ENeptuneBoolean.NEPTUNE_BOOL_TRUE);

                Int32 nEffectFlags = 0;
                if (NeptuneC.ntcGetEffect(m_pCameraHandle, ref nEffectFlags) == ENeptuneError.NEPTUNE_ERR_Success)
                {
                    nEffectFlags |= (Int32)ENeptuneEffect.NEPTUNE_EFFECT_FLIP;

                    _ = NeptuneC.ntcSetEffect(m_pCameraHandle, nEffectFlags);
                }

                Cam_capture.Enabled = true;
                NEPTUNE_IMAGE_SIZE stImageSize = new NEPTUNE_IMAGE_SIZE();
                stImageSize.nStartX = 0;
                stImageSize.nStartY = 0;
                stImageSize.nSizeX = 3220;
                stImageSize.nSizeY = 3448;
                _ = NeptuneC.ntcSetImageSize(m_pCameraHandle, stImageSize);

                ENeptuneError emErr1 = NeptuneC.ntcSetPixelFormat(m_pCameraHandle, (ENeptunePixelFormat)108);

                NEPTUNE_FEATURE stInfo = new NEPTUNE_FEATURE();
                m_arrFeatureInfo = new NEPTUNE_FEATURE[m_arrMapping.Length];
                stInfo = m_arrFeatureInfo[m_arrMapping.Length - 1];
                stInfo.AutoMode = ENeptuneAutoMode.NEPTUNE_AUTO_OFF;
                NeptuneC.ntcSetFeature(m_pCameraHandle, m_arrMapping[m_arrMapping.Length - 1], stInfo);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Gamma", (int)0);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Gain", (int)0);

                // Red
                stInfo = m_arrFeatureInfo[2];
                stInfo.Value = 469;
                NeptuneC.ntcSetFeature(m_pCameraHandle, m_arrMapping[2], stInfo);

                // Green
                stInfo = m_arrFeatureInfo[3];
                stInfo.Value = 363;
                NeptuneC.ntcSetFeature(m_pCameraHandle, m_arrMapping[3], stInfo);

                // Blue
                stInfo = m_arrFeatureInfo[4];
                stInfo.Value = 874;
                NeptuneC.ntcSetFeature(m_pCameraHandle, m_arrMapping[4], stInfo);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "BlackLevel", (int)108);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Contrast", (int)435);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Hue", (int)255);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Saturation", (int)255);

                NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Sharpness", (int)0);

                //m_arrCheckBox = new CheckBox[m_arrMapping.Length - 1];
                //stInfo = m_arrFeatureInfo[7];
                //stInfo.bOnOff = ENeptuneBoolean.NEPTUNE_BOOL_TRUE;
                //stInfo.Value = 47900;
                //NeptuneC.ntcSetFeature(m_pCameraHandle, m_arrMapping[7], stInfo);

                //stInfo = m_arrFeatureInfo[7];
                //stInfo.Value = 47900;
                //ENeptuneError emErr1 = NeptuneC.ntcSetFeature(m_pCameraHandle, m_arrMapping[7], stInfo);
                //MessageBox.Show($"{emErr1}");
                //NeptuneC.ntcSetNodeInt(m_pCameraHandle, "Shutter", (int)47900);
            }
        }

        private async Task captureFrame()
        {
            if (m_pCameraHandle != IntPtr.Zero)
            {
                if (!Directory.Exists(Path.Combine(defaultDirectory, stream_folder)))
                {
                    Directory.CreateDirectory(Path.Combine(defaultDirectory, stream_folder));
                }

                else
                {
                    DirectoryInfo directory = new DirectoryInfo(Path.Combine(defaultDirectory, stream_folder));

                    // Delete all files within the folder
                    foreach (FileInfo file in directory.GetFiles())
                    {
                        file.Delete();
                    }
                }
                string cameraPath = Path.Combine(defaultDirectory, Path.Combine(stream_folder, "input_image.jpg"));
                _ = await Task.Run(() => NeptuneC.ntcSaveImage(m_pCameraHandle, cameraPath, 50));
            }
        }

        private void Template_box_DragDrop(object sender, DragEventArgs e)
        {
            string[] files = (string[])e.Data.GetData(DataFormats.FileDrop);
            if (files.Length > 0)
            {
                string imagePath = files[0];
                Template_box.ImageLocation = imagePath;
                string fullTemplatePath = imagePath;
                templatePath = get_relativePath(fullTemplatePath, defaultDirectory);

                string templatePathCopy = Path.Combine(Path.GetDirectoryName(templatePath), "copy_" + Path.GetFileName(templatePath));
                File.Copy(templatePath, templatePathCopy, true);

                ShowImage(templatePathCopy, Template_box);

                File.Delete(templatePathCopy);
            }
        }

        private void Template_box_DragEnter(object sender, DragEventArgs e)
        {
            if (e.Data.GetDataPresent(DataFormats.FileDrop))
            {
                e.Effect = DragDropEffects.Copy;
            }
        }

        private void Image_box_DragEnter(object sender, DragEventArgs e)
        {
            if (e.Data.GetDataPresent(DataFormats.FileDrop))
            {
                string[] file = (string[])e.Data.GetData(DataFormats.FileDrop);
                if (file.Length == 1 && (Path.GetExtension(file[0]).ToLower() == ".png" || Path.GetExtension(file[0]).ToLower() == ".jpg" || Path.GetExtension(file[0]).ToLower() == ".jpeg" || Path.GetExtension(file[0]).ToLower() == ".bmp" || Path.GetExtension(file[0]).ToLower() == ".gif"))
                {
                    e.Effect = DragDropEffects.Copy;
                }
            }
        }

        private void Image_box_DragDrop(object sender, DragEventArgs e)
        {
            string[] file = (string[])e.Data.GetData(DataFormats.FileDrop);
            if (file.Length > 0)
            {
                string imagePath = file[0];
                string fullTargetImagePath = imagePath;
                targetImagePath = get_relativePath(fullTargetImagePath, defaultDirectory);

                string targetImagePathCopy = Path.Combine(Path.GetDirectoryName(targetImagePath), "copy_" + Path.GetFileName(targetImagePath));
                File.Copy(targetImagePath, targetImagePathCopy, true);

                ShowImage(targetImagePathCopy, Image_box);

                File.Delete(targetImagePathCopy);
            }
        }

        private void LoadCsvToDataGridView(string csvFilePath, DataGridView dataGridView)
        {
            if (!File.Exists(Path.Combine(defaultDirectory, csvFilePath)))
            {
                throw new ArgumentException("The specified csv file does not exist.", nameof(csvFilePath));
            }

            // Create a new DataTable
            DataTable dataTable = new DataTable();

            // Read the CSV file line by line
            string[] csvLines = File.ReadAllLines(Path.Combine(defaultDirectory, csvFilePath));

            // Add the column headers to the DataTable
            string[] headers = csvLines[0].Split(',');
            foreach (string header in headers)
            {
                dataTable.Columns.Add(header);
            }

            // Add the data rows to the DataTable
            for (int i = 1; i < csvLines.Length; i++)
            {
                string[] fields = csvLines[i].Split(',');
                dataTable.Rows.Add(fields);
            }

            // Bind the DataTable to the DataGridView
            dataGridView.DataSource = dataTable;
        }

        private void ShowImage(string imagePath, PictureBox pictureBox)
        {
            if (!File.Exists(Path.Combine(defaultDirectory, imagePath)))
            {
                throw new ArgumentException("The specified image file does not exist.", nameof(imagePath));
            }

            if (pictureBox.Image != null)
            {
                pictureBox.Image.Dispose();
                pictureBox.Image = null;
            }

            using (Image image = Image.FromFile(Path.Combine(defaultDirectory, imagePath)))
            {
                if (image.PropertyIdList.Contains(0x0112)) // Check if the image has orientation metadata
                {
                    int orientation = (int)image.GetPropertyItem(0x0112).Value[0];
                    switch (orientation)
                    {
                        case 2: // Flip horizontally
                            image.RotateFlip(RotateFlipType.RotateNoneFlipX);
                            break;
                        case 3: // Rotate 180 degrees
                            image.RotateFlip(RotateFlipType.Rotate180FlipNone);
                            break;
                        case 4: // Flip vertically
                            image.RotateFlip(RotateFlipType.RotateNoneFlipY);
                            break;
                        case 5: // Rotate 90 degrees clockwise and flip horizontally
                            image.RotateFlip(RotateFlipType.Rotate90FlipX);
                            break;
                        case 6: // Rotate 90 degrees clockwise
                            image.RotateFlip(RotateFlipType.Rotate90FlipNone);
                            break;
                        case 7: // Rotate 90 degrees clockwise and flip vertically
                            image.RotateFlip(RotateFlipType.Rotate90FlipY);
                            break;
                        case 8: // Rotate 270 degrees clockwise
                            image.RotateFlip(RotateFlipType.Rotate270FlipNone);
                            break;
                        default:
                            break;
                    }
                }

                Invoke(new MethodInvoker(() =>
                {
                    pictureBox.Image = new Bitmap(image);
                    percentageScale(pictureBox);

                    if (pictureBox.Name == "Image_box")
                    {
                        down_scale.Enabled = true;
                        up_scale.Enabled = true;
                        add_template.Enabled = false;
                    }

                    if ((templatePath != null) && (targetImagePath != null))
                    {
                        Match_template.Enabled = true;
                    }
                }));
            }
        }


        private void percentageScale(PictureBox pictureBox)
        {
            if (pictureBox.Name != "Image_box")
            {
                return;
            }

            float originalWidth = pictureBox.Image.Width;
            float originalHeight = pictureBox.Image.Height;
            float aspectRatio = originalWidth / originalHeight;

            float resizedWidth = pictureBox.Width;
            float resizedHeight = pictureBox.Height;

            if (resizedWidth / resizedHeight > aspectRatio)
            {
                resizedWidth = resizedHeight * aspectRatio;
            }
            else
            {
                resizedHeight = resizedWidth / aspectRatio;
            }

            float widthPercentage = resizedWidth / originalWidth * 100;
            float heightPercentage = resizedHeight / originalHeight * 100;

            percentage_scale.Text = $"{(int)Math.Round(widthPercentage)}%";
        }

        private void ResizePictureBox(float percentage)
        {
            float originalWidth = Image_box.Image.Width;
            float originalHeight = Image_box.Image.Height;
            float aspectRatio = originalWidth / originalHeight;

            float resizedWidth = originalWidth * (percentage / 100);
            float resizedHeight = resizedWidth / aspectRatio;

            Image_box.Width = (int)resizedWidth;
            Image_box.Height = (int)resizedHeight;
        }

        private void percentage_scale_TextChanged(object sender, EventArgs e)
        {
            if (float.TryParse(percentage_scale.Text.Replace("%", ""), out float percentage))
            {
                ResizePictureBox(percentage);
            }
        }

        private string get_relativePath(string path, string defaultPaht)
        {
            Uri fullUri = new Uri(path);
            Uri defaultUri = new Uri(defaultPaht);
            string relativePath = Uri.UnescapeDataString(defaultUri.MakeRelativeUri(fullUri).ToString());
            return relativePath;
        }

        private void StartServer()
        {   
            IPAddress ipAddress = IPAddress.Parse(serverIP.Text);
            int port = 48951;
            int port2 = 48952;
            int port3 = 48953;

            // Robot - Winforms - Run CVU algorithm
            serverSocket = new TcpListener(ipAddress, port);
            serverSocket.Start();
            isRunning = true;

            // CVU - Winforms - Get positions of detected objects
            serverSocket2 = new TcpListener(ipAddress, port2);
            serverSocket2.Start();
            isRunning2 = true;

            // Robot - Winforms - Send each position when receiving the request
            serverSocket3 = new TcpListener(ipAddress, port3);
            serverSocket3.Start();
            isRunning3 = true;

            // Start a new thread to accept client connections
            serverThread = new Thread(new ThreadStart(AcceptClients));
            serverThread.Start();

            serverThread2 = new Thread(new ThreadStart(AcceptClients2));
            serverThread2.Start();

            serverThread3 = new Thread(new ThreadStart(AcceptClients3));
            serverThread3.Start();
        }

        private void AcceptClients()
        {
            while (isRunning)
            {
                try
                {
                    TcpClient clientSocket = serverSocket.AcceptTcpClient();
                    Thread clientThread = new Thread(new ParameterizedThreadStart(HandleClient));
                    clientThread.Start(clientSocket);
                }

                catch (SocketException)
                {
                    break;
                }
            }
        }

        private void AcceptClients2()
        {
            while (isRunning2)
            {
                try
                {
                    TcpClient clientSocket = serverSocket2.AcceptTcpClient();
                    Thread clientThread = new Thread(new ParameterizedThreadStart(HandleClient2));
                    clientThread.Start(clientSocket);
                }
                catch (SocketException)
                {
                    break;
                }
            }
        }

        private void AcceptClients3()
        {
            while (isRunning3)
            {
                try
                {
                    TcpClient clientSocket = serverSocket3.AcceptTcpClient();
                    Thread clientThread = new Thread(new ParameterizedThreadStart(HandleClient3));
                    clientThread.Start(clientSocket);
                }
                catch (SocketException)
                {
                    break;
                }
            }
        }

        private void HandleClient(object clientObj)
        {
            TcpClient clientSocket = (TcpClient)clientObj;

            try
            {
                NetworkStream networkStream = clientSocket.GetStream();
                byte[] buffer = new byte[clientSocket.ReceiveBufferSize];

                int bytesRead = networkStream.Read(buffer, 0, clientSocket.ReceiveBufferSize);
                sbyte receivedInteger = (sbyte)buffer[0];

                if (receivedInteger == 100)
                {
                    // Create a new TaskCompletionSource and assign it to matchTemplateCompletionSource
                    matchTemplateCompletionSource = new TaskCompletionSource<bool>();

                    // Invoke the conduct_match_template method on the UI thread
                    string response = null;
                    int numberOfObjects = 0;
                    BeginInvoke(new MethodInvoker(async () =>
                    {
                        stopwatch.Reset();
                        stopwatch.Start();
                        Elasped_time.Text = "...";
                        CVU_status.Text = "Running...";

                        await conduct_capture_frame();

                        response = await conduct_match_template();
                        numberOfObjects = int.Parse(response);

                        // After the conduct_match_template method is complete, set the task completion source result
                        matchTemplateCompletionSource.SetResult(true);
                    }));

                    // Wait for the task completion source to complete
                    matchTemplateCompletionSource.Task.Wait();

                    byte byteNumberOfObjects = (byte)numberOfObjects;
                    networkStream.WriteByte(byteNumberOfObjects);
                    networkStream.Flush();

                    stopwatch.Stop();
                    TimeSpan elapsedTime = stopwatch.Elapsed;
                    double roundedElapsedTime = Math.Round(elapsedTime.TotalSeconds, 2);
                    string elapsedTimeString = roundedElapsedTime.ToString();

                    BeginInvoke(new MethodInvoker(() =>
                    {
                        Elasped_time.Text = $"{elapsedTimeString} s";

                        string resultImagePath = Path.Combine(output_folder, "output.jpg");
                        string csvPath = Path.Combine(output_folder, "result.csv");

                        try
                        {
                            ShowImage(resultImagePath, Image_box);
                            LoadCsvToDataGridView(csvPath, dataGridView1);
                            CVU_status.Text = "Completed";
                        }
                        catch (Exception)
                        {
                            CVU_status.Text = "No detection found";
                        }
                    }));
                }
            }

            catch (Exception e)
            {
                MessageBox.Show($"{e}");
            }
            finally
            {
                clientSocket.Close();
            }
        }

        private void HandleClient2(object clientObj)
        {
            TcpClient clientSocket = (TcpClient)clientObj;

            try
            {
                byte[] numBytes = new byte[1];
                NetworkStream networkStream = clientSocket.GetStream();
                int bytesRead = networkStream.Read(numBytes, 0, 1);

                if (bytesRead == 0)
                {
                    return;
                }
                int numArrays = numBytes[0];

                lock (sharedData)
                {
                    sharedData.Clear();

                    for (int i = 0; i < numArrays; i++)
                    {
                        // Receive the array of 4 float values (x, y, z, r)
                        byte[] arrayBytes = new byte[16];
                        bytesRead = clientSocket.GetStream().Read(arrayBytes, 0, 16);
                        if (bytesRead == 0)
                        {
                            return;
                        }

                        float[] receivedArray = new float[4];
                        for (int j = 0; j < 4; j++)
                        {
                            if (BitConverter.IsLittleEndian)
                                Array.Reverse(arrayBytes, j * 4, 4);
                            receivedArray[j] = BitConverter.ToSingle(arrayBytes, j * 4);
                        }
                        sharedData.Add(receivedArray);
                    }
                }

                byte response = 100;
                networkStream.WriteByte(response);
                networkStream.Flush();

            }
            catch (Exception e)
            {
                MessageBox.Show($"{e}");
            }
            finally
            {
                clientSocket.Close();
            }
        }

        private void HandleClient3(object clientObj)
        {
            TcpClient clientSocket = (TcpClient)clientObj;
            try
            {
                // Access the shared data
                float[][] dataArray;
                lock (sharedData)
                {
                    dataArray = sharedData.ToArray();
                }

                NetworkStream networkStream = clientSocket.GetStream();
                byte[] buffer = new byte[clientSocket.ReceiveBufferSize];

                int bytesRead = networkStream.Read(buffer, 0, clientSocket.ReceiveBufferSize);
                sbyte receivedIndex = (sbyte)buffer[0];

                float[] data = dataArray[receivedIndex];

                byte[] arrayBytes = new byte[16];
                for (int i = 0; i < 4; i++)
                {
                    byte[] floatBytes = BitConverter.GetBytes(data[i]);
                    if (BitConverter.IsLittleEndian)
                        Array.Reverse(floatBytes);
                    Buffer.BlockCopy(floatBytes, 0, arrayBytes, i * 4, 4);
                }

                networkStream.Write(arrayBytes, 0, arrayBytes.Length);
            }
            catch (Exception e)
            {
                MessageBox.Show($"{e}");
            }
            finally
            {
                clientSocket.Close();
            }
        }


        private void DisconnectServer()
        {
            if (serverStatus.Text == "Connected")
            {
                // Set the flag to stop the server thread
                isRunning = false;
                isRunning2 = false;
                isRunning3 = false;

                // Close the server socket to stop AcceptTcpClient() blocking
                serverSocket.Stop();
                serverSocket2.Stop();
                serverSocket3.Stop();

                // Wait for the server thread to exit gracefully
                serverThread.Join();
                serverThread2.Join();
                serverThread3.Join();
            }
        }

        private void Socket_connect_Click(object sender, EventArgs e)
        {
            StartServer();
            serverStatus.Text = "Connected";
            Socket_connect.Enabled = false;
            Socket_disconnect.Enabled = true;
        }

        private void Socket_disconnect_Click(object sender, EventArgs e)
        {
            DisconnectServer();
            serverStatus.Text = "Disconnected";
            Socket_connect.Enabled = true;
            Socket_disconnect.Enabled = false;
        }

        async Task<string> SendRequest(string url, Dictionary<string, string> formFields)
        {
            using (var client = new HttpClient())
            {
                using (var formData = new MultipartFormDataContent())
                {
                    foreach (var field in formFields)
                    {
                        formData.Add(new StringContent(field.Value), field.Key);
                    }

                    var response = await client.PostAsync(url, formData);

                    response.EnsureSuccessStatusCode();

                    return await response.Content.ReadAsStringAsync();
                }
            }
        }

        private async Task<string> conduct_match_template()
        {
            // Clear the DataGridView before assigning the new data source
            dataGridView1.DataSource = null;
            dataGridView1.Rows.Clear();
            dataGridView1.Columns.Clear();

            var formFields = new Dictionary<string, string>
            {
                {"api_folder", defaultDirectory},
                {"img_path", targetImagePath},
                {"template_path", templatePath},
                {"threshold", Similarity_score.Text},
                {"overlap", Overlap.Text},
                {"method", Method_similarity.Text},
                {"min_modify", Min_modify.Text},
                {"max_modify", Max_modify.Text},
                {"conf_score", conf_score.Text},
                {"img_size", img_size.Text},
                {"server_ip", serverIP.Text},
                {"output_folder", output_folder}
            };

            var response = await SendRequest("http://127.0.0.1:5000/my_cvu_api", formFields);

            return response;
        }

        private async void Match_template_Click(object sender, EventArgs e)
        {
            stopwatch.Reset();
            stopwatch.Start();
            Elasped_time.Text = "...";
            CVU_status.Text = "Running...";

            _ = await conduct_match_template();

            stopwatch.Stop();
            TimeSpan elapsedTime = stopwatch.Elapsed;
            double roundedElapsedTime = Math.Round(elapsedTime.TotalSeconds, 2);
            string elapsedTimeString = roundedElapsedTime.ToString();

            Elasped_time.Text = $"{elapsedTimeString} s";

            string resultImagePath = Path.Combine(output_folder, "output.jpg");
            string csvPath = Path.Combine(output_folder, "result.csv");

            try
            {
                ShowImage(resultImagePath, Image_box);
                LoadCsvToDataGridView(csvPath, dataGridView1);
                CVU_status.Text = "Completed";
            }
            catch (Exception)
            {
                CVU_status.Text = "No detection found";
            }
        }

        private async Task conduct_capture_frame()
        {
            string cameraPath = Path.Combine(stream_folder, "input_image.jpg");

            try
            {
                await captureFrame();
            }
            catch (Exception e)
            {
                MessageBox.Show($"{e}");
            }

            targetImagePath = cameraPath;
        }

        private async void Cam_capture_Click(object sender, EventArgs e)
        {
            await conduct_capture_frame();

            string cameraPath = Path.Combine(stream_folder, "input_image.jpg");
            Match_template.Enabled = true;
            try
            {
                ShowImage(cameraPath, Image_box);
            }
            catch (Exception)
            {
                MessageBox.Show("No connection with camera");
            }
        }

        private void Image_box_MouseDown(object sender, MouseEventArgs e)
        {
            if (e.Button == MouseButtons.Left)
            {
                isSelecting = true;
                startPoint = e.Location;
            }
        }

        private void Image_box_MouseUp(object sender, MouseEventArgs e)
        {
            if (e.Button == MouseButtons.Left)
            {
                isSelecting = false;
                isReshape = false;

                // update selectedRect with the selected region's rectangle
                selectedRect = new Rectangle(Math.Min(rect.Left, rect.Right),
                                             Math.Min(rect.Top, rect.Bottom),
                                             Math.Abs(rect.Width),
                                             Math.Abs(rect.Height));

                add_template.Enabled = true;
            }
        }

        private void Image_box_MouseMove(object sender, MouseEventArgs e)
        {
            if (isSelecting)
            {
                int x = Math.Min(startPoint.X, e.Location.X);
                int y = Math.Min(startPoint.Y, e.Location.Y);
                int width = Math.Abs(startPoint.X - e.Location.X);
                int height = Math.Abs(startPoint.Y - e.Location.Y);

                rect = new Rectangle(x, y, width, height);
                Image_box.Invalidate();
            }
        }

        private void Image_box_Paint(object sender, PaintEventArgs e)
        {
            if ((isSelecting || isReshape) && rect.Width > 0 && rect.Height > 0)
            {
                using (Pen pen = new Pen(Color.Green, 2))
                {
                    e.Graphics.DrawRectangle(pen, rect);

                    int rectX = rect.X;
                    int rectY = rect.Y;
                    int rectWidth = rect.Width;
                    int rectHeight = rect.Height;

                    // Draw small rectangles at each corner of the rect
                    e.Graphics.FillRectangle(Brushes.Green, rectX - cornerSize + 1, rectY - cornerSize + 1, cornerSize * 2 - 2, cornerSize * 2 - 2);
                    e.Graphics.FillRectangle(Brushes.Green, rectX + rectWidth - cornerSize + 1, rectY - cornerSize + 1, cornerSize * 2 - 2, cornerSize * 2 - 2);
                    e.Graphics.FillRectangle(Brushes.Green, rectX - cornerSize + 1, rectY + rectHeight - cornerSize + 1, cornerSize * 2 - 2, cornerSize * 2 - 2);
                    e.Graphics.FillRectangle(Brushes.Green, rectX + rectWidth - cornerSize + 1, rectY + rectHeight - cornerSize + 1, cornerSize * 2 - 2, cornerSize * 2 - 2);
                }
            }
        }

        private void add_template_Click(object sender, EventArgs e)
        {
            string streamTemplatePath = Path.Combine(template_folder, "template.jpg");

            if (selectedRect.Width > 0 && selectedRect.Height > 0)
            {
                Bitmap bitmap = new Bitmap(Image_box.ClientSize.Width, Image_box.ClientSize.Height);
                Image_box.DrawToBitmap(bitmap, Image_box.ClientRectangle);

                Bitmap croppedBitmap = bitmap.Clone(selectedRect, bitmap.PixelFormat);

                if (!Directory.Exists(template_folder))
                {
                    Directory.CreateDirectory(template_folder);
                }

                croppedBitmap.Save(streamTemplatePath, ImageFormat.Jpeg);
            }

            try
            {
                ShowImage(streamTemplatePath, Template_box);
            }
            catch (Exception)
            {
                MessageBox.Show("Please select a region of interest first");
            }

            templatePath = streamTemplatePath;
        }

        private void updateScrollBarPositions()
        {
            // Calculate the difference between the old and new size of the picture box
            int deltaX = Image_box.Width - hScrollBar1.Maximum;
            int deltaY = Image_box.Height - vScrollBar1.Maximum;

            // Check if the picture box is smaller than the panel and adjust the scroll bar values accordingly
            if (deltaX < 0)
            {
                hScrollBar1.Value = 0;
                deltaX = 0;
            }
            if (deltaY < 0)
            {
                vScrollBar1.Value = 0;
                deltaY = 0;
            }

            // Calculate the new position of the scroll bars based on the difference
            int newHValue = Math.Max(-Image_box.Location.X, 0);
            int newVValue = Math.Max(-Image_box.Location.Y, 0);

            // Update the scrollbars' maximum values to reflect the new size of the picturebox
            hScrollBar1.Maximum = Math.Max(Image_box.Width - panel13.Width, 0);
            vScrollBar1.Maximum = Math.Max(Image_box.Height - panel13.Height, 0);

            // Make sure the scrollbars' values are still within their maximum range
            hScrollBar1.Value = Math.Min(newHValue, hScrollBar1.Maximum);
            vScrollBar1.Value = Math.Min(newVValue, vScrollBar1.Maximum);

            // Update the position of the picturebox based on the scrollbar values
            Image_box.Location = new Point(Math.Max(-hScrollBar1.Value, 6), Math.Max(-vScrollBar1.Value, 21));
        }

        private void vScrollBar1_Scroll(object sender, ScrollEventArgs e)
        {
            Image_box.Top = -e.NewValue;
        }

        private void hScrollBar1_Scroll(object sender, ScrollEventArgs e)
        {
            Image_box.Left = -e.NewValue;
        }

        private void Image_box_Resize(object sender, EventArgs e)
        {
            Image_box.SizeMode = PictureBoxSizeMode.Zoom;
            updateScrollBarPositions();
        }

        private void hScrollBar1_ValueChanged(object sender, EventArgs e)
        {
            Image_box.Left = -hScrollBar1.Value;
        }

        private void vScrollBar1_ValueChanged(object sender, EventArgs e)
        {
            Image_box.Top = -vScrollBar1.Value;
        }

        private void down_scale_MouseDown(object sender, MouseEventArgs e)
        {

            isResizing = true;
            int scaleFactor = Convert.ToInt32(Scale_ratio.Text);
            while (isResizing)
            {
                float aspectRatio = (float)Image_box.Image.Width / (float)Image_box.Image.Height;
                int newWidth = Image_box.Width - (int)(scaleFactor * 1);
                int newHeight = (int)(newWidth / aspectRatio);

                Point oldCenter = new Point(Image_box.Location.X + Image_box.Width / 2,
                                             Image_box.Location.Y + Image_box.Height / 2);

                Image_box.Size = new Size(newWidth, newHeight);

                Point newCenter = new Point(oldCenter.X + (Image_box.Width - newWidth) / 2,
                                             oldCenter.Y + (Image_box.Height - newHeight) / 2);

                Image_box.Location = new Point(newCenter.X - Image_box.Width / 2,
                                                 newCenter.Y - Image_box.Height / 2);

                percentageScale(Image_box);
                updateScrollBarPositions();
                Application.DoEvents();
            }
        }

        private void down_scale_MouseUp(object sender, MouseEventArgs e)
        {
            isResizing = false;
        }

        private void up_scale_MouseDown(object sender, MouseEventArgs e)
        {
            isResizing = true;
            int scaleFactor = Convert.ToInt32(Scale_ratio.Text);
            while (isResizing)
            {
                float aspectRatio = (float)Image_box.Image.Width / (float)Image_box.Image.Height;
                int newWidth = Image_box.Width + (int)(scaleFactor * 1);
                int newHeight = (int)(newWidth / aspectRatio);

                Point oldCenter = new Point(Image_box.Location.X + Image_box.Width / 2,
                                             Image_box.Location.Y + Image_box.Height / 2);

                Image_box.Size = new Size(newWidth, newHeight);

                Point newCenter = new Point(oldCenter.X - (newWidth - Image_box.Width) / 2,
                                             oldCenter.Y - (newHeight - Image_box.Height) / 2);

                Image_box.Location = new Point(newCenter.X - Image_box.Width / 2,
                                                 newCenter.Y - Image_box.Height / 2);

                percentageScale(Image_box);
                updateScrollBarPositions();
                Application.DoEvents();
            }
        }

        private void up_scale_MouseUp(object sender, MouseEventArgs e)
        {
            isResizing = false;
        }

        private void Similarity_score_TextChanged(object sender, EventArgs e)
        {

        }

        private void Overlap_TextChanged(object sender, EventArgs e)
        {

        }

        private void Template_box_Click(object sender, EventArgs e)
        {

        }
    }
}

/////////////////////////////////////////////////////

REM "GRASPING ROBOT PROGRAM"
DIM NUMBER_PALLET AS INTEGER
DIM TOTAL_NUMBER_PALLET AS INTEGER
NUMBER_PALLET=1
SETM O56,0
USE 27
REM "HOME POSITION"
MOVEX A=1, M1X, P, (477.02, 24.34, 535.0, -54.0, -22.33, 25.11), R=V100%, H=31, MS
REM "DOI TIN HIEU KHI CO PALLET"
WAITI I50
REM "DOI_TIN_HIEU_KHI_CO_THONG_SO_VI_TRI_CON_HANG"
CALLMCR 12, 100 
IF V5%>9
V5%=9
ENDIF
FOR V1! = 1 TO V5% STEP 1
REM "VI_TRI_GAP_VAT"
MOVEX A=1, M1X, P, (449.20, 13.51, 340.67, -88.26, 0.00, 1.21), R=40, H=31, MS
REM "MO_TAY_GAP"
SETM O54,1
MOVEX A=1, M1X, P, (511.23, 24.34, 534.94, -54.00, -22.33, 25.10), R=40, MS
MOVEX A=1, M1X, P, (202.23, 337.96, 603.44, -4.10, -20.74, 6.79), R=40, MS
MOVEX A=1, M1X, P, (-122.48, 480.19, 507.24, 48.20, -20.91, 23.10), R=40, H=31, MS
REM "VI_TRI_THA_VAT"
USE 11
MOVEX A=1, M1X,P, P[NUMBER_PALLET], R=40, H=31, MS
REM "DONG_TAY_GAP"
SETM O54,0
NUMBER_PALLET=NUMBER_PALLET+1
IF NUMBER_PALLET>9
GOTO 39
ENDIF
MOVEX A=1, M1X, P, (212.14, 450.14, 140.04, -6.30, 1, 4.84), R=40, H=31, MS
NEXT
IF NUMBER_PALLET<9
GOTO 6
ENDIF
DELAY 0.1
USE 27
MOVEX A=1, M1X, P,(212.14, 450.14, 140.04, -6.3, 1.0, 4.84), R=V100%, H=31,MS
SETM O56,1
END 
 
 

 ///////////////usertask
'CALL SOCKET COMMUNICATE
DIM COUNT_4_BYTE AS INTEGER
SET O50
V5% = 1
'CREATING SOCKET
SOCKCREATE 1,0

'CONNECTING TO SERVER SOCKET
SOCKCONNECT 1,112, 48951, 0

SETBYTE 1,100,0
'TRANSMITING DATA
SOCKSEND 1,1,1,0,V5%

SOCKRECV 1,1,1,0,V6%
'CLOSE SOCKET
SOCKCLOSE 1
EXIT



REM "GRASPING ROBOT PROGRAM"
DIM NUMBER_PALLET AS INTEGER
DIM TOTAL_NUMBER_PALLET AS INTEGER
NUMBER_PALLET=1
V152% = 0
SETM O56,0
USE 27
REM "HOME POSITION"
MOVEX A=1, M1X, P, (477.02, 24.34, 535.0, -54.0, -22.33, 25.11), R=V100%, H=31, MS
REM "DOI TIN HIEU KHI CO PALLET"
WAITI I50
REM "DOI_TIN_HIEU_KHI_CO_THONG_SO_VI_TRI_CON_HANG"
CALLMCR 12, 100 
MOVEX A=1, M1X, P, (V1%, 24.34, 535.0, -54.0, -22.33, 25.11), R=40, H=31, MS

SETM O45,0
IF V5%>9
V5%=9
ENDIF
FOR V1! = 1 TO V5% STEP 1
REM "VI_TRI_GAP_VAT"
MOVEX A=1, M1X, P, (449.20, 13.51, 340.67, -88.26, 0.00, 1.21), R=40, H=31, MS
REM "MO_TAY_GAP"
SETM O54,1
MOVEX A=1, M1X, P, (511.23, 24.34, 534.94, -54.00, -22.33, 25.10), R=40, MS
MOVEX A=1, M1X, P, (202.23, 337.96, 603.44, -4.10, -20.74, 6.79), R=40, MS
MOVEX A=1, M1X, P, (-122.48, 480.19, 507.24, 48.20, -20.91, 23.10), R=40, H=31, MS
REM "VI_TRI_THA_VAT"
USE 11
MOVEX A=1, M1X,P, P[NUMBER_PALLET], R=40, H=31, MS
REM "DONG_TAY_GAP"
SETM O54,0
NUMBER_PALLET=NUMBER_PALLET+1
IF NUMBER_PALLET>9
GOTO 39
ENDIF
MOVEX A=1, M1X, P, (212.14, 450.14, 140.04, -6.30, 1, 4.84), R=40, H=31, MS
NEXT
IF NUMBER_PALLET<9
GOTO 6
ENDIF
DELAY 0.1
USE 27
MOVEX A=1, M1X, P,(212.14, 450.14, 140.04, -6.3, 1.0, 4.84), R=V100%, H=31,MS
SETM O56,1
END 
 
 

 'CALL SOCKET COMMUNICATE
SET O50

'CREATING SOCKET
SOCKCREATE 3,0

'CONNECTING TO SERVER SOCKET
SOCKCONNECT 3,1, 48951, 10

SETBYTE 2,56,0
'TRANSMITING DATA
SOCKSEND 3,2,1,0,V5%

SOCKRECV 3, 2, 2, 0, V150%

GETINT 1, V151%, 0
V1% = V151%


'CLOSE SOCKET
SOCKCLOSE 1
EXIT

//////////////////////test socket:
python server:
# server.py
import socket
import numpy as np
import struct


# Định nghĩa host và port mà server sẽ chạy và lắng nghe
host = '172.25.144.1'
port = 48951



s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.bind((host, port))
s.listen(1)  # Chỉ chấp nhận 1 kết nối đồng thời
print("Server listening on port", port)
data_arr = [225.56,255.3,301.4,186.2]




try:
    while True:
        c, addr = s.accept()
        print("Connected from", str(addr))

        # # Server sử dụng kết nối để gửi dữ liệu tới client dưới dạng binary
        # c.send(b"Hello, how are you")

        # Nhận 1 byte dữ liệu từ client
        data = c.recv(1)
        # decode_data = struct.unpack('!f', data)[0]
        decode_data = int.from_bytes(data, byteorder='little')
        print(f"Received data: {decode_data}")
        if decode_data == 56:
            for element in data_arr:

                
                print("hjkhjk: ",element)
                byte_value = struct.pack('!1f', element)
                c.sendall(byte_value)
                print(f"Dữ liệu gui từ server: {byte_value}")

        # if decode_data == "8":
        #     for element in data_arr:
        #         byte_value = struct.pack('!i', element)
        #         c.send(byte_value)
        #         print(f"Dữ liệu gửi từ server: {element}")

    


        # c.close()

except Exception as e:
    print("Error:", str(e))

finally:
    print("loi")
    s.close()

main:

REM "TEST CONNECT SOCKET"
DIM CLOSE AS INTEGER
REM "HOME POSITION"
MOVEX A=1, M1X, P, (477.02, 24.34, 535.0, -54.0, -22.33, 25.11), R=40, H=31, MS
V50% = 1
V27% = 0
V3! = 0
V1%=0
V4%=0
FORKMCR 3, 100
FOR V2! = 1 TO 4 STEP 1
V4%=1
DELAY 5
REM "RUN"
MOVEX A=1, M1X, P, (V3!,  24.34, 535.0, -54.0, -22.33, 25.11), R=40, H=31, MS
SETM O10,0
V27% = V27% + 1
NEXT


V50% = 0
V1%=1
SETM O11,0
END

usertask:
'sample
INCLUDE "SAMPLE"
'CALL SOCKET COMMUNICATE
'CREATING SOCKET
SOCKCREATE 3,0
'CONNECTING TO SERVER SOCKET
SOCKCONNECT 3,1, 48951, 10
V50% = 1
DIM COUNT AS INTEGER
WAITI I55

SETBYTE 2,V27!,0
'TRANSMITING DATA
SOCKSEND 3,2,1,5,V5%
SOCKRECV 3, 1, 16, 5, V150%

WHILE (V1%=0)

IF (V4%=1)
COUNT = V27%*4 
SET O10
V27! = 56


GETREAL 1, V15!, COUNT
V3! = V15!
V4%=0
ENDIF
ENDW

SET O11
'CLOSE SOCKET
SOCKCLOSE 1
EXIT

/////////////////////////////////////////
REM "TEST CONNECT SOCKET"
DIM CLOSE AS INTEGER
REM "NUMBER OF OBJECT DETECT
V20!=0
REM "HOME POSITION"
MOVEX A=1, M1X, P, (477.02, 24.34, 535.0, -54.0, -22.33, 25.11), R=40, H=31, MS
V15!=0
V27% = 0
V3! = 0
V4! = 0
V5! = 0
V6! = 0
V1%=0
V5%=0
V4%=0
FORKMCR 3, 100
V4%=1
FOR V30! = 1 TO V20! STEP 1
FOR V2! = 1 TO 4 STEP 1
V5%=1
DELAY 1
REM "RUN"
MOVEX A=1, M1X, P, (477.02, 24.34, 535.0, -54.0, -22.33, 25.11), R=40, H=31, MS
SETM O10,0

NEXT
V20!=0
NEXT
IF I50 = 1
V50% = 1
V27% = 0
V3! = 0
V4%=1
DELAY 5
GOTO 14
ENDIF
V1%=1
SETM O11,0
END

///////////////////////
'sample
INCLUDE "SAMPLE"
'CALL SOCKET COMMUNICATE
'CREATING SOCKET
SOCKCREATE 3,0
DIM COUNT AS INTEGER
WAITI I55
IF (V4%=1)
SET O10
'CONNECTING TO SERVER SOCKET
SOCKCONNECT 3,1, 48951, 10
V27! = 1
SETBYTE 2,V27!,0
'TRANSMITING DATA
SOCKSEND 3,2,1,5,V5%
SOCKRECV 3, 2, 36, 15, V150%
'lay do dai
GETREAL 2, V15!, 0
V20!=V15!
V27% = V27% + 1
V4%=0
ENDIF
IF(V5%=1)
GETREAL 2, V15!, V27%*4
V3! = V15!
V27%=V27%+1
GETREAL 2, V15!, V27%*4
V4! = V15!
V27%=V27%+1
GETREAL 2, V15!, V27%*4
V5! = V15!
V27%=V27%+1
GETREAL 2, V15!, V27%*4
V6! = V15!
V27%=V27%+1
V5%=0
ENDIF
IF (V1%=0)
GOTO 7
ENDIF

SET O11
'CLOSE SOCKET
SOCKCLOSE 1
EXIT
/////////////////////////////////////////////
# server.py
import socket
import numpy as np
import struct
import requests
import time

# Định nghĩa host và port mà server sẽ chạy và lắng nghe
host = '172.25.144.1'
port = 48951

headers = {"Content-Type": "application/json", "Authorization": "Bearer your_token"}

s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.bind((host, port))
s.listen(1)  # Chỉ chấp nhận 1 kết nối đồng thời
print("Server listening on port", port)
# data_arr=[]




try:
    while True:
        c, addr = s.accept()
        print("Connected from", str(addr))

        # # Server sử dụng kết nối để gửi dữ liệu tới client dưới dạng binary
        # c.send(b"Hello, how are you")
        while True:
            # Nhận 1 byte dữ liệu từ client
            data = c.recv(1)
            # decode_data = struct.unpack('!f', data)[0]
            decode_data = int.from_bytes(data, byteorder='little')
            print(f"Received data: {decode_data}")
            if decode_data == 1:
                # for i in range(len(data_arr)):
                #     data_arr[i] = data_arr[i] + 10
                # for element in data_arr:
                    
                #     print("hjkhjk: ",element)
                #     byte_value = struct.pack('!1f', element)
                #     c.sendall(byte_value)
                #     print(f"Dữ liệu gui từ server: {byte_value}")
                #     # ////////////////////////////////
                api_url = "http://127.0.0.1:5000/cvu_process"
                form_data={
                    "imgLink" : r"C:\Users\TKD01A-1\Documents\yolov8_test\nhandienvat-main\nhandienvat-main\test\dataset_specialItems\train\images\20230922_155434_jpg.rf.ac089e9cd5afade05e7ba5d035c4d823.jpg",
                    "templateLink" : r"C:\Users\TKD01A-1\Documents\yolov8_test\nhandienvat-main\nhandienvat-main\test\template.jpg",
                    "modelLink" : r"C:\Users\TKD01A-1\Documents\yolov8_test\nhandienvat-main\nhandienvat-main\test\runs\segment\train\weights\last.pt",
                    "pathSaveOutputImg" : "",
                    "csvLink" : "",
                    "outputImgLink" : "",
                    "min_modify" : "-20",
                    "max_modify" : "20",
                    "configScore" : "0.9",
                    "img_size" : "640",
                    "method" : "cv2.TM_CCORR_NORMED",
                    "server_ip" : ""
                }
                response = requests.post(api_url, data=form_data)
                if response.status_code == 200:
                    print("respons and type: ", response.json(), type(response))
                    data_arr = response.json()
                    byte_value_length = struct.pack('!1f', len(data_arr))
                    c.sendall(byte_value_length)
                    print(f"Dữ liệu gui từ server: {byte_value_length}")
                    for object in data_arr:
                        for element in object:
                            print("hjkhjk: ",element)
                            byte_value = struct.pack('!f', element)
                            c.sendall(byte_value)
                            print(f"Dữ liệu gui từ server: {byte_value}")
                    print("done!!")
                    break
            # c.close()

except Exception as e:
    print("Error:", str(e))

finally:
    print("loi")
    s.close()

////////////////////////////////////////////////////////
cvu_multi.py

from flask import Flask, request
import numpy as np
from components import *
from ultis import *
from copy import deepcopy
from API_flask import create_app
import csv
import time
import threading
# import keyboard

logger = logging.getLogger(__name__)

app = create_app()

@app.route('/cvu_process', methods=['GET','POST'])
def cvu_process():

  start_time = time.time()
# /////////Input////////////////////
  if request.method == "POST":
      #///Form data
      imgLink = request.form.get('imgLink')
      templateLink = request.form.get('templateLink')
      modelLink = request.form.get('modelLink')
      pathSaveOutputImg = request.form.get('pathSaveOutputImg')

      try:
            csvLink = request.form.get('csvLink')
            outputImgLink = request.form.get('outputImgLink')
            min_modify = int(request.form.get('min_modify'))
            max_modify = int(request.form.get('max_modify'))
            configScore = float(request.form.get('configScore'))
            img_size = int(request.form.get('img_size'))
            method = request.form.get('method')
            server_ip = request.form.get('server_ip')

      except Exception as e:
            logger.error(f'{e}\n')
            return f'{e}\n'

      #/////////Begin process/////////////////
      imgLink = imgLink.replace('\\', '/')
      templateLink = templateLink.replace('\\', '/')
      img = cv2.imread(imgLink)
      template = cv2.imread(templateLink)
      template = cv2.resize(template, (255,165))
      gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
      template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)
      copy_of_template_gray = deepcopy(template_gray)
      minus_modify_angle = np.arange(0, min_modify, -2) 
      plus_modify_angle = np.arange(0, max_modify, 2) 
      low_clip, high_clip=5.0, 97.0
      copy_of_template_gray = contrast_stretching(copy_of_template_gray,  low_clip,high_clip)
      _, copy_of_template_gray = cv2.threshold(copy_of_template_gray, 100, 255, cv2.THRESH_BINARY_INV)
      # cv2.imwrite("thresTemp.jpg",copy_of_template_gray)
      intensity_of_template_gray = np.sum(copy_of_template_gray == 0)
      findCenter_type = 0
      good_points = []
      try:
            object_item = proposal_box_yolo(imgLink,modelLink,img_size,configScore)#object_item sẽ gồm list thông tin góc và tọa độ của đường bao
           
            # print("number class: ",object_item[0][2])
            if object_item == None:
                 return good_points
            good_points.append([object_item[0][2]])
            for angle,bboxes,_ in object_item:
                #   print("------------------------------------------------------------")
                  result_queue = []
                  minus_sub_angles, plus_sub_angles = angle + minus_modify_angle, angle + plus_modify_angle
                  
                  # threshold = 0.95
                  point = match_pattern(gray_img, template_gray, bboxes, angle, eval(method)) 
                  if point is None:
                        continue
                  p1 = threading.Thread(target=find_center2, args=(gray_img,bboxes,low_clip,high_clip, intensity_of_template_gray, findCenter_type,result_queue,))
                  p2 = threading.Thread(target=compare_angle, args=(point,minus_sub_angles,plus_sub_angles, gray_img, template_gray, bboxes, angle, eval(method),result_queue,))
                  p1.start(), p2.start()                       
                  p1.join(), p2.join()
                  
                  # if len(result_queue) == 2:
                  bestAngle, bestPoint = result_queue[1]
                  center_0,center_1, possible_grasp_ratio =  result_queue[0]
                  if center_0 == None and center_1 == None:
                        continue          
                  if possible_grasp_ratio < 90:
                        print("score<95!")
                        continue
                  good_points.append([center_0,center_1,bestAngle,possible_grasp_ratio])
                  # Viết chữ lên hình ảnh
                  # cv2.putText(img, f"{bestAngle}", (int(center_0),int(center_1)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
                  # x2, y2 = int(center_0 + 150* np.cos(np.radians(bestAngle)) ),int(center_1 + 150* np.sin(np.radians(bestAngle)) )
                  # x3, y3 = int(center_0 + 100* np.cos(np.radians(bestAngle+90)) ),int(center_1 + 100* np.sin(np.radians(bestAngle+90)) )
                  # cv2.line(img,(center_0,center_1),(x2,y2),(255,255,0),2)
                  # cv2.line(img,(center_0,center_1),(x3,y3),(255,0,255),2)
                  # cv2.imwrite("amTam.jpg",img)
                  # print("total: ",center_0,center_1,bestAngle,possible_grasp_ratio)
            print("good point arr: ",good_points)

            print("process time: ",time.time() - start_time)
            return good_points
      except Exception as e:
           print("System error: ", e)
           return good_points

#   if request.method == "GET":
#        return f'<div><h1>Get result</h1></div>'
       

if __name__ == "__main__":
     app.run(debug=True)

//////////////////////////////////////////////////////////////////////////

proposal_bb.py

import numpy as np
from ultis.apply_min_area import apply_min_area
from ultralytics import YOLO

class YOLOSegmentation:
    def __init__(self,model):
        self.model =YOLO(model)  # load a custom model
        

    def predict(self,img,img_size,configScore):

        #Thực hiện nhận diện
        pred_img = self.model(img, save=False, conf=configScore, imgsz=img_size)
        #lấy danh sách các bounding box
        bboxes = np.array(pred_img[0].boxes.xyxy, dtype="int")
        # lấy danh sách mặt nạ masks của mỗi đối tượng đcược nhận diện
        masks = np.array(pred_img[0].masks.xy,dtype=object)
        # lấy danh sách các loại đối tượng đã được nhận diện
        class_ids = np.array(pred_img[0].boxes.cls,dtype="int")
        # print(class_ids)
        # lấy danh sách độ chính xác của các đối tượng đã được nhận diện
        score = np.array(pred_img[0].boxes.conf,dtype="float").round(2)
        return bboxes, masks, class_ids, score
    
    @staticmethod
    def filter_boxes(bboxes,masks,class_ids,score):
        # chỉ lấy các đối tượng với chỉ số là 3(mat_dung)
        obj_detect = class_ids == 0
        # object thông tin bounding box, masks, score của các dối tượng đúng
        object_true = bboxes[obj_detect,:], masks[obj_detect], score[obj_detect]
        # print(object_true)
        # object thông tin bounding box, masks, score của các dối tượng sai
        object_fail = bboxes[~obj_detect,:],masks[~obj_detect],score[~obj_detect]
        # print("detect: ",object_true)
        return object_true,object_fail

    @staticmethod
    def create_angle(mask_true):  
         #truy cập vào từng điểm của mask và tính toán
         angle = list(map(lambda x: apply_min_area(x), mask_true))
         return angle
    @staticmethod
    def convert_xywh(boxes):
    #    tạo thông tin bounding box từ dang xyxy chuyển sang dạng xywh(xy ở đây là điểm x_min y_min chứ ko phải là center)
        boxes[:, 2], boxes[:, 3] = boxes[:, 2] - boxes[:, 0], boxes[:, 3] - boxes[:, 1]
        return boxes
    
        

def proposal_box_yolo(img,model,image_size,configScore):
    
    ys = YOLOSegmentation(model)
    try:
        bboxes,masks,class_ids, score = ys.predict(img,image_size, configScore)
        # print("just say hello!")
        obj,_ = ys.filter_boxes(bboxes,masks,class_ids,score)
        # print("dodai conhang: ", len(obj[0]))
        # tính toán góc xoay dựa vào các điểm masks(obj[1] là list các điểm masks đúng)
        angle_test = ys.create_angle(obj[1])
        # print(" conhang: ", angle_test)
        # print("angle: ",angle_test)
        xywh_boxes = ys.convert_xywh(obj[0])
        number_items = np.full(len(xywh_boxes),len(obj[0]),dtype=float)
        # print("number_items",number_items)
        return list(zip(angle_test, xywh_boxes,number_items))
    except Exception as e:
        print("Yolo detection error or no detection: ",e)
        
    

//////////////////////////////
apply_min_area.py


import cv2
import numpy as np

def apply_min_area(contour):
 contour = np.array(contour, dtype=np.int32)
 rotated_rect= cv2.minAreaRect(contour)
 #lấy góc mặc định
 #lấy tọa độ các đỉnh của rotated_rect
 rect_points = cv2.boxPoints(rotated_rect).astype(int)

 edge1 = np.array(rect_points[1]) - np.array(rect_points[0])
 edge2 = np.array(rect_points[2]) - np.array(rect_points[1])
 reference = np.array([1, 0])
#  với ta có adge1 và edge2 là toa độ lần lượt của vecto 01 và vecto 12
#  và cùng với cú pháp numpy np.linalg.norm(edge) ta sẽ tính được độ dài của 2 vecto đó, độ dài nào lớn hơn(vì đây là hình chữ nhật và ta lấy cạnh dài để căn góc) thì 
#  được dùng làm đoạn thẳng kết hợp với điểm tham chiếu để tính toán góc bằng công thức cosin
#  print("dodai: ",np.linalg.norm(edge1),np.linalg.norm(edge2))
#  print("point: ", rect_points)
 if np.linalg.norm(edge1) > np.linalg.norm(edge2):    

         used_edge = edge1
         #  np.arccos dùng để tính phép tính arccos trong lượng giác, np.dot dùng để tính tích vô hướng
         pre_angle = (180.0 / np.pi) * (np.arccos(np.dot(reference, used_edge) / (np.linalg.norm(reference) * np.linalg.norm(used_edge))))
         angle = 360 - pre_angle
        #  print("angle AS: ",angle)    
 else:
         
         used_edge = edge2
         angle = (180.0 / np.pi)*(np.arccos(np.dot(reference, used_edge) / (np.linalg.norm(reference) * np.linalg.norm(used_edge))))
        #  print("angle AC: ",angle)
 if angle <= 0:
        angle = 360 + angle
 return angle


////////////////////////////////////////////////

compare_angle.py

from components.match_pattern import match_pattern

def compare_angle(point,minus_sub_angles,plus_sub_angles, gray_img, template_gray, bboxes, angle, method,result_queue ):
    minus_pointer, plus_pointer = 0,0
    exactly_minus, exactly_plus = 0, 0
    high_point_minus, high_point_plus = point, point
    bestAngle,bestPoint = 0,0

    while minus_pointer < len(minus_sub_angles)  or plus_pointer < len(plus_sub_angles):
        point_minus = match_pattern(gray_img, template_gray, bboxes, minus_sub_angles[minus_pointer], method)
        point_plus = match_pattern(gray_img, template_gray, bboxes, plus_sub_angles[plus_pointer], method)
        if point_minus >= high_point_minus:
            if minus_sub_angles[minus_pointer] >= 360:
                exactly_minus = minus_sub_angles[minus_pointer] - 360
            else:
                exactly_minus = minus_sub_angles[minus_pointer]
            if point_minus > bestPoint:
                bestAngle = exactly_minus
                bestPoint = point_minus
            high_point_minus = point_minus
        
        if point_plus >= high_point_plus:
            if plus_sub_angles[plus_pointer] >= 360:
                exactly_plus = plus_sub_angles[plus_pointer] - 360
            else:
                exactly_plus = plus_sub_angles[minus_pointer]
            if point_plus > bestPoint:
                bestAngle = exactly_plus
                bestPoint = point_plus
            high_point_plus = point_plus
        
        minus_pointer = minus_pointer + 1
        plus_pointer = plus_pointer + 1
        # print("---------------------------------------")
    if bestPoint < point:
        bestAngle = angle
    result_queue.append((bestAngle, bestPoint))
    return bestAngle, bestPoint

#kiểm tra từng phần tử minus_sub_angles,plus_sub_angles, lấy angle mà có số điểm cao nhất 


///////////////////////////////////////////////////////////////////
match_pattern.py

import cv2
from ultis.rotate_object import rotate_object 
import logging


logger = logging.getLogger(__name__)
# mở rộng khung ảnh một khi tọa độ bbox ở sát khung ảnh, cản trở việc roi đối tượng
def padded_image(img_gray, bboxes, esilon_w,epsilon_h):
   # print("b0,b1", bboxes[0],  bboxes[1])
   x_start,y_start  = bboxes[0] - esilon_w, bboxes[1] - epsilon_h
   x_end, y_end = bboxes[0] + bboxes[2] + esilon_w, bboxes[1] + bboxes[3] + epsilon_h
   padded_left, padded_top = min(x_start,0), min(y_start,0)
   padded_right, padded_bottom = min(img_gray.shape[1] - x_end, 0), min(img_gray.shape[0] - y_end,0)
    
   # print("paded: ",padded_left, padded_top, padded_right, padded_bottom)
   img_padded = cv2.copyMakeBorder(img_gray,abs(padded_top),abs(padded_bottom), abs(padded_left), abs(padded_right),cv2.BORDER_CONSTANT, value=0 )
   # cv2.imwrite("img_paded.jpg",img_padded)
   return img_padded, x_start, x_end, y_start, y_end


def match_pattern(img_gray, template_gray, boxes, sub_angle, method ):
   rotated_template,mask, w_temp,h_temp = rotate_object(template_gray,sub_angle)
   epsilon_w, epsilon_h = abs(boxes[2]-w_temp), abs(boxes[3]-h_temp)
   # print("epsilon_w, epsilon_h", epsilon_w, epsilon_h,boxes[2],boxes[3])
   img_padded, x_start, x_end, y_start, y_end = padded_image(img_gray,boxes, epsilon_w, epsilon_h)
   img_roi = img_padded[abs(y_start) : abs(y_end)  ,abs(x_start) : abs(x_end)]
   # cv2.imwrite( "img_roi.jpg",img_roi)
   matched_points = cv2.matchTemplate(img_roi, rotated_template, method, None, mask)
   _, max_val, _, _ = cv2.minMaxLoc(matched_points)
   # print("point mark: ", max_val)
   return max_val




# matched_points là một mảng hai chiều chứa các điểm tương tự hoặc giá trị liên quan đến mức độ khớp giữa mẫu và ảnh. Mỗi phần tử trong mảng tương ứng với mức độ khớp tại một vị trí cụ thể trong ảnh.

# Hàm cv2.minMaxLoc được sử dụng để tìm giá trị nhỏ nhất và lớn nhất trong mảng matched_points và cũng trả về vị trí (tọa độ x, y) của giá trị lớn nhất.

# Khi bạn gán kết quả từ cv2.minMaxLoc cho các biến _, max_val, _, và max_loc, bạn có các giá trị sau:

# _ (underscore đầu tiên) là giá trị nhỏ nhất trong mảng matched_points, nhưng bạn không sử dụng nó trong ví dụ này, vì bạn chỉ quan tâm đến giá trị lớn nhất.
# max_val là giá trị lớn nhất trong mảng matched_points. Đây có thể được coi là mức độ khớp tối đa giữa mẫu và vùng quan tâm của ảnh.
# _ (underscore thứ hai) là vị trí (tọa độ x, y) của giá trị nhỏ nhất trong mảng, bạn cũng không sử dụng nó.
# max_loc là vị trí (tọa độ x, y) của giá trị lớn nhất trong mảng matched_points. Nó cho biết vị trí tương ứng trên ảnh gốc (img_gray) mà mẫu có mức độ khớp tối đa.
# Sau dòng mã này, bạn có thể sử dụng giá trị max_val để xem mức độ khớp tối đa và max_loc để biết vị trí tương ứng trên ảnh.

///////////////////////////////////////////////////////////////////////

finde_center_2.py


import cv2
import numpy as np
from ultis.processing_image import contrast_stretching
import time


def find_center2(gray_img,bboxes, low_clip,high_clip,intensity_of_template_gray,findCenter_type,result_queue):
    # print("time excute: ", time.time())
    x1,y1,w,h = bboxes[0], bboxes[1], bboxes[2], bboxes[3]
    # c1,c2 = x1 + w/2, y1 + h/2
    center_obj = (0,0)
    if(x1<10 or y1 <10):
     gray_img = cv2.copyMakeBorder(gray_img,20,20, 20, 20,cv2.BORDER_CONSTANT, value=0 )
    imgRoi = gray_img[y1 - 3:y1+h +3,x1 - 3:x1+w +3]
    # cv2.imwrite("roi_findCenter.jpg",imgRoi)
    
    padded_roi_gray = gray_img[y1-20:y1+h + 20,x1-20:x1+w + 20]
    
    thresholdImg = contrast_stretching(imgRoi,low_clip,high_clip)
    
    padded_thresholdImg = contrast_stretching(padded_roi_gray,  low_clip,high_clip)
    _,thresholdImg = cv2.threshold(thresholdImg,100,255, cv2.THRESH_BINARY_INV)
    # cv2.imwrite("thes1.jpg",padded_thresholdImg)
    _,padded_roi_gray = cv2.threshold(padded_thresholdImg,100,255, cv2.THRESH_BINARY_INV)
    intensity_of_roi_gray = np.sum(padded_roi_gray == 0)
    # print("intensity_of_roi_gray: ",intensity_of_roi_gray)
    # print("intensity_of_temp_gray: ",intensity_of_template_gray)
    possible_grasp_ratio = (intensity_of_template_gray / intensity_of_roi_gray) * 100
    # print("possible_grasp_ratio: ",possible_grasp_ratio)
    if findCenter_type == 0:
        try:
            contours,_ = cv2.findContours(thresholdImg,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
            # print("len: ",len(contours))
            check_true = False
            for element in contours:
                s = cv2.contourArea(element)
                # print('S contour: ',s)
                if s < 1500 or s > 3000:
                    continue
                check_true = True
                (x_axis, y_axis), radius = cv2.minEnclosingCircle(element)
                center = (int(x_axis + x1),int(y_axis + y1)) 
                radius = int(radius) 
                # print("kq: ",(int(abs(center[0] - c1)),int( abs(center[1] - c2))))
                # result = (int(c1 + abs(center[0] - c1)/2),int(c2 + abs(center[1] - c2)/2))
                cv2.circle(gray_img,center,radius,(0,255,0),1) 
                cv2.circle(gray_img,center,1,(255,255,0),3) 
                # cv2.imwrite("thes.jpg",gray_img)
                center_obj = (center[0],center[1])
            if check_true == False:
                center_obj = (None, None)

        except Exception as e:
                # print("S < 1500 or s > 3000")
                center_obj = (None, None)

    result_queue.append((center[0],center[1],possible_grasp_ratio))
    
    return center_obj, possible_grasp_ratio





